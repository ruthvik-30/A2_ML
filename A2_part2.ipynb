{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part - 2:\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               f3      target\n",
      "count  766.000000  766.000000\n",
      "mean    69.118799    0.349869\n",
      "std     19.376901    0.477240\n",
      "min      0.000000    0.000000\n",
      "25%     62.500000    0.000000\n",
      "50%     72.000000    0.000000\n",
      "75%     80.000000    1.000000\n",
      "max    122.000000    1.000000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of             f1     f2  f3         f4          f5        f6        f7  target\n",
       "0     6.000000  148.0  72  35.000000    0.000000  33.60000  0.627000       1\n",
       "1     1.000000   85.0  66  29.000000    0.000000  26.60000  0.351000       0\n",
       "2     8.000000  183.0  64   0.000000    0.000000  23.30000  0.672000       1\n",
       "3     1.000000   89.0  66  23.000000   94.000000  28.10000  0.167000       0\n",
       "4     0.000000  137.0  40  35.000000  168.000000  43.10000  2.288000       1\n",
       "..         ...    ...  ..        ...         ...       ...       ...     ...\n",
       "761   9.000000   89.0  62   0.000000    0.000000  22.50000  0.472128       0\n",
       "762  10.000000  101.0  76  48.000000  180.000000  31.99817  0.171000       0\n",
       "763   2.000000  122.0  70  27.000000   80.091503  36.80000  0.340000       0\n",
       "764   3.849673  121.0  72  23.000000  112.000000  26.20000  0.245000       0\n",
       "765   1.000000  126.0  60  20.542484    0.000000  30.10000  0.349000       1\n",
       "\n",
       "[766 rows x 8 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that there are alphabets in numeric columns so we replace them with mean of that respective column\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "df.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                f1        f2        f3            f4        f5            f6  \\\n",
       "0    6.382139e-01  0.849057  0.148790  9.070150e-01 -0.695096  2.030729e-01   \n",
       "1   -8.457789e-01 -1.125480 -0.161060  5.305956e-01 -0.695096 -6.843559e-01   \n",
       "2    1.231811e+00  1.946022 -0.264343 -1.288765e+00 -0.695096 -1.102715e+00   \n",
       "3   -8.457789e-01 -1.000113 -0.161060  1.541762e-01  0.120709 -4.941926e-01   \n",
       "4   -1.142577e+00  0.504297 -1.503740  9.070150e-01  0.762938  1.407440e+00   \n",
       "..            ...       ...       ...           ...       ...           ...   \n",
       "761  1.528610e+00 -1.000113 -0.367626 -1.288765e+00 -0.695096 -1.204136e+00   \n",
       "762  1.825408e+00 -0.624010  0.355356  1.722590e+00  0.867083  4.503972e-16   \n",
       "763 -5.489804e-01  0.034169  0.045507  4.051224e-01  0.000000  6.087546e-01   \n",
       "764  1.318050e-16  0.002827  0.148790  1.541762e-01  0.276927 -7.350661e-01   \n",
       "765 -8.457789e-01  0.159536 -0.470909  2.228851e-16 -0.695096 -2.406415e-01   \n",
       "\n",
       "               f7  target  \n",
       "0    4.677335e-01       1  \n",
       "1   -3.658228e-01       0  \n",
       "2    6.036394e-01       1  \n",
       "3   -9.215270e-01       0  \n",
       "4    5.484172e+00       1  \n",
       "..            ...     ...  \n",
       "761  1.676510e-16       0  \n",
       "762 -9.094465e-01       0  \n",
       "763 -3.990443e-01       0  \n",
       "764 -6.859567e-01       0  \n",
       "765 -3.718631e-01       1  \n",
       "\n",
       "[766 rows x 8 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df1 = df.drop('target', axis=1)\n",
    "column = df1.columns\n",
    "df1[column] = scaler.fit_transform(df1[column])\n",
    "df1['target'] = df['target']\n",
    "df1.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  torch.Size([697, 7])\n",
      "Y_train size:  torch.Size([697])\n",
      "X_val size:  torch.Size([149, 7])\n",
      "Y_val size:  torch.Size([149])\n",
      "X_test size:  torch.Size([150, 7])\n",
      "Y_test size:  torch.Size([150])\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('target', axis=1)  # Replace 'target_column' with the actual target column name\n",
    "Y = df['target']\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X, Y = ros.fit_resample(X, Y)\n",
    "# Split the data into training and testing sets (80% train, 20% test by default)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.175, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "print(\"X_train size: \", X_train_tensor.shape)\n",
    "print(\"Y_train size: \", Y_train_tensor.shape)\n",
    "print(\"X_val size: \", X_val_tensor.shape)\n",
    "print(\"Y_val size: \", Y_val_tensor.shape)\n",
    "print(\"X_test size: \", X_test_tensor.shape)\n",
    "print(\"Y_test size: \", Y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning hyperparameters:\n",
    "=======================\n",
    "1. Dropout value:\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_dropout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(NN_dropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6610, Validation Loss: 0.6292, Training Accuracy: 0.6327, Validation Accuracy: 0.6779\n",
      "Epoch 20/300, Training Loss: 0.6358, Validation Loss: 0.6031, Training Accuracy: 0.6643, Validation Accuracy: 0.7047\n",
      "Epoch 30/300, Training Loss: 0.6222, Validation Loss: 0.5934, Training Accuracy: 0.6772, Validation Accuracy: 0.7181\n",
      "Epoch 40/300, Training Loss: 0.6214, Validation Loss: 0.5971, Training Accuracy: 0.6786, Validation Accuracy: 0.7181\n",
      "Epoch 50/300, Training Loss: 0.6338, Validation Loss: 0.6155, Training Accuracy: 0.6729, Validation Accuracy: 0.6309\n",
      "Epoch 60/300, Training Loss: 0.6109, Validation Loss: 0.5962, Training Accuracy: 0.6829, Validation Accuracy: 0.6644\n",
      "Epoch 70/300, Training Loss: 0.6008, Validation Loss: 0.5880, Training Accuracy: 0.6915, Validation Accuracy: 0.6711\n",
      "Epoch 80/300, Training Loss: 0.6279, Validation Loss: 0.5877, Training Accuracy: 0.6614, Validation Accuracy: 0.7114\n",
      "Epoch 90/300, Training Loss: 0.6075, Validation Loss: 0.6006, Training Accuracy: 0.6729, Validation Accuracy: 0.6309\n",
      "Epoch 100/300, Training Loss: 0.5712, Validation Loss: 0.5631, Training Accuracy: 0.7088, Validation Accuracy: 0.7315\n",
      "Epoch 110/300, Training Loss: 0.5869, Validation Loss: 0.5676, Training Accuracy: 0.7188, Validation Accuracy: 0.6644\n",
      "Epoch 120/300, Training Loss: 0.5767, Validation Loss: 0.5669, Training Accuracy: 0.7274, Validation Accuracy: 0.7315\n",
      "Epoch 130/300, Training Loss: 0.5775, Validation Loss: 0.5428, Training Accuracy: 0.6973, Validation Accuracy: 0.7450\n",
      "Epoch 140/300, Training Loss: 0.5622, Validation Loss: 0.5390, Training Accuracy: 0.6973, Validation Accuracy: 0.7383\n",
      "Epoch 150/300, Training Loss: 0.5702, Validation Loss: 0.5362, Training Accuracy: 0.7131, Validation Accuracy: 0.7114\n",
      "Epoch 160/300, Training Loss: 0.5670, Validation Loss: 0.5440, Training Accuracy: 0.7044, Validation Accuracy: 0.7315\n",
      "Epoch 170/300, Training Loss: 0.5517, Validation Loss: 0.5480, Training Accuracy: 0.7245, Validation Accuracy: 0.7181\n",
      "Epoch 180/300, Training Loss: 0.5500, Validation Loss: 0.5519, Training Accuracy: 0.7159, Validation Accuracy: 0.7114\n",
      "Epoch 190/300, Training Loss: 0.5489, Validation Loss: 0.5513, Training Accuracy: 0.7331, Validation Accuracy: 0.7047\n",
      "Epoch 200/300, Training Loss: 0.5683, Validation Loss: 0.5398, Training Accuracy: 0.7102, Validation Accuracy: 0.7248\n",
      "Epoch 210/300, Training Loss: 0.5318, Validation Loss: 0.5262, Training Accuracy: 0.7331, Validation Accuracy: 0.7584\n",
      "Epoch 220/300, Training Loss: 0.5442, Validation Loss: 0.5359, Training Accuracy: 0.7274, Validation Accuracy: 0.7383\n",
      "Epoch 230/300, Training Loss: 0.5361, Validation Loss: 0.5286, Training Accuracy: 0.7475, Validation Accuracy: 0.7248\n",
      "Epoch 240/300, Training Loss: 0.5508, Validation Loss: 0.5308, Training Accuracy: 0.7446, Validation Accuracy: 0.7315\n",
      "Epoch 250/300, Training Loss: 0.5452, Validation Loss: 0.5216, Training Accuracy: 0.7360, Validation Accuracy: 0.7315\n",
      "Epoch 260/300, Training Loss: 0.5343, Validation Loss: 0.5515, Training Accuracy: 0.7217, Validation Accuracy: 0.7047\n",
      "Epoch 270/300, Training Loss: 0.5420, Validation Loss: 0.5519, Training Accuracy: 0.7374, Validation Accuracy: 0.7181\n",
      "Epoch 280/300, Training Loss: 0.5289, Validation Loss: 0.5372, Training Accuracy: 0.7446, Validation Accuracy: 0.7114\n",
      "Epoch 290/300, Training Loss: 0.5378, Validation Loss: 0.5351, Training Accuracy: 0.7303, Validation Accuracy: 0.7315\n",
      "Epoch 300/300, Training Loss: 0.5308, Validation Loss: 0.5462, Training Accuracy: 0.7489, Validation Accuracy: 0.7248\n",
      "Test Loss: 0.5221, Test Accuracy: 0.7733\n",
      "Dropout: 0.3\n",
      "Test Accuracy: 0.7733\n",
      "Test Precision: 0.8364\n",
      "Test Recall: 0.6479\n",
      "Test F1 Score: 0.7302\n",
      "Epoch 10/300, Training Loss: 0.6798, Validation Loss: 0.6647, Training Accuracy: 0.5882, Validation Accuracy: 0.6577\n",
      "Epoch 20/300, Training Loss: 0.6622, Validation Loss: 0.6382, Training Accuracy: 0.6298, Validation Accuracy: 0.7114\n",
      "Epoch 30/300, Training Loss: 0.6647, Validation Loss: 0.6614, Training Accuracy: 0.5968, Validation Accuracy: 0.5772\n",
      "Epoch 40/300, Training Loss: 0.6723, Validation Loss: 0.6381, Training Accuracy: 0.6227, Validation Accuracy: 0.6846\n",
      "Epoch 50/300, Training Loss: 0.6508, Validation Loss: 0.5986, Training Accuracy: 0.6485, Validation Accuracy: 0.7181\n",
      "Epoch 60/300, Training Loss: 0.6621, Validation Loss: 0.6387, Training Accuracy: 0.6055, Validation Accuracy: 0.6376\n",
      "Epoch 70/300, Training Loss: 0.6353, Validation Loss: 0.6071, Training Accuracy: 0.6442, Validation Accuracy: 0.6577\n",
      "Epoch 80/300, Training Loss: 0.6338, Validation Loss: 0.6042, Training Accuracy: 0.6585, Validation Accuracy: 0.6846\n",
      "Epoch 90/300, Training Loss: 0.6219, Validation Loss: 0.5950, Training Accuracy: 0.6786, Validation Accuracy: 0.6846\n",
      "Epoch 100/300, Training Loss: 0.6234, Validation Loss: 0.6106, Training Accuracy: 0.6528, Validation Accuracy: 0.7181\n",
      "Epoch 110/300, Training Loss: 0.6256, Validation Loss: 0.5830, Training Accuracy: 0.6657, Validation Accuracy: 0.6913\n",
      "Epoch 120/300, Training Loss: 0.6081, Validation Loss: 0.5740, Training Accuracy: 0.6786, Validation Accuracy: 0.7047\n",
      "Epoch 130/300, Training Loss: 0.6400, Validation Loss: 0.5877, Training Accuracy: 0.6643, Validation Accuracy: 0.7047\n",
      "Epoch 140/300, Training Loss: 0.6180, Validation Loss: 0.5903, Training Accuracy: 0.6858, Validation Accuracy: 0.7047\n",
      "Epoch 150/300, Training Loss: 0.6125, Validation Loss: 0.5879, Training Accuracy: 0.6700, Validation Accuracy: 0.7181\n",
      "Epoch 160/300, Training Loss: 0.6207, Validation Loss: 0.5777, Training Accuracy: 0.6671, Validation Accuracy: 0.7248\n",
      "Epoch 170/300, Training Loss: 0.5933, Validation Loss: 0.5721, Training Accuracy: 0.6829, Validation Accuracy: 0.7383\n",
      "Epoch 180/300, Training Loss: 0.5815, Validation Loss: 0.5600, Training Accuracy: 0.7016, Validation Accuracy: 0.7383\n",
      "Epoch 190/300, Training Loss: 0.5770, Validation Loss: 0.5602, Training Accuracy: 0.6987, Validation Accuracy: 0.7181\n",
      "Epoch 200/300, Training Loss: 0.5845, Validation Loss: 0.5521, Training Accuracy: 0.6930, Validation Accuracy: 0.7248\n",
      "Epoch 210/300, Training Loss: 0.6334, Validation Loss: 0.5665, Training Accuracy: 0.6844, Validation Accuracy: 0.7383\n",
      "Epoch 220/300, Training Loss: 0.6155, Validation Loss: 0.6001, Training Accuracy: 0.6714, Validation Accuracy: 0.7383\n",
      "Epoch 230/300, Training Loss: 0.5757, Validation Loss: 0.5456, Training Accuracy: 0.7030, Validation Accuracy: 0.6980\n",
      "Epoch 240/300, Training Loss: 0.5905, Validation Loss: 0.5607, Training Accuracy: 0.6858, Validation Accuracy: 0.7114\n",
      "Epoch 250/300, Training Loss: 0.6008, Validation Loss: 0.6751, Training Accuracy: 0.6514, Validation Accuracy: 0.6107\n",
      "Epoch 260/300, Training Loss: 0.5974, Validation Loss: 0.5397, Training Accuracy: 0.6930, Validation Accuracy: 0.7047\n",
      "Epoch 270/300, Training Loss: 0.5970, Validation Loss: 0.5474, Training Accuracy: 0.6973, Validation Accuracy: 0.7181\n",
      "Epoch 280/300, Training Loss: 0.5951, Validation Loss: 0.5671, Training Accuracy: 0.6915, Validation Accuracy: 0.7047\n",
      "Epoch 290/300, Training Loss: 0.5762, Validation Loss: 0.5505, Training Accuracy: 0.7073, Validation Accuracy: 0.7181\n",
      "Epoch 300/300, Training Loss: 0.6102, Validation Loss: 0.5752, Training Accuracy: 0.6801, Validation Accuracy: 0.6980\n",
      "Test Loss: 0.5741, Test Accuracy: 0.7600\n",
      "Dropout: 0.5\n",
      "Test Accuracy: 0.7600\n",
      "Test Precision: 0.7692\n",
      "Test Recall: 0.7042\n",
      "Test F1 Score: 0.7353\n",
      "Epoch 10/300, Training Loss: 0.6922, Validation Loss: 0.6899, Training Accuracy: 0.5509, Validation Accuracy: 0.5235\n",
      "Epoch 20/300, Training Loss: 0.6974, Validation Loss: 0.6847, Training Accuracy: 0.5696, Validation Accuracy: 0.5705\n",
      "Epoch 30/300, Training Loss: 0.6685, Validation Loss: 0.6861, Training Accuracy: 0.5968, Validation Accuracy: 0.5235\n",
      "Epoch 40/300, Training Loss: 0.6716, Validation Loss: 0.6833, Training Accuracy: 0.5954, Validation Accuracy: 0.5705\n",
      "Epoch 50/300, Training Loss: 0.6945, Validation Loss: 0.6900, Training Accuracy: 0.5710, Validation Accuracy: 0.5235\n",
      "Epoch 60/300, Training Loss: 0.6870, Validation Loss: 0.6832, Training Accuracy: 0.5710, Validation Accuracy: 0.5302\n",
      "Epoch 70/300, Training Loss: 0.6730, Validation Loss: 0.6737, Training Accuracy: 0.5954, Validation Accuracy: 0.5839\n",
      "Epoch 80/300, Training Loss: 0.6700, Validation Loss: 0.6846, Training Accuracy: 0.5782, Validation Accuracy: 0.5369\n",
      "Epoch 90/300, Training Loss: 0.6880, Validation Loss: 0.6833, Training Accuracy: 0.5581, Validation Accuracy: 0.5369\n",
      "Epoch 100/300, Training Loss: 0.6840, Validation Loss: 0.6756, Training Accuracy: 0.5667, Validation Accuracy: 0.5705\n",
      "Epoch 110/300, Training Loss: 0.6683, Validation Loss: 0.6749, Training Accuracy: 0.5940, Validation Accuracy: 0.5705\n",
      "Epoch 120/300, Training Loss: 0.6766, Validation Loss: 0.6746, Training Accuracy: 0.5768, Validation Accuracy: 0.5705\n",
      "Epoch 130/300, Training Loss: 0.6735, Validation Loss: 0.6714, Training Accuracy: 0.5997, Validation Accuracy: 0.5638\n",
      "Epoch 140/300, Training Loss: 0.6823, Validation Loss: 0.6643, Training Accuracy: 0.6069, Validation Accuracy: 0.6376\n",
      "Epoch 150/300, Training Loss: 0.6708, Validation Loss: 0.6671, Training Accuracy: 0.5882, Validation Accuracy: 0.6174\n",
      "Epoch 160/300, Training Loss: 0.6691, Validation Loss: 0.6655, Training Accuracy: 0.5897, Validation Accuracy: 0.5906\n",
      "Epoch 170/300, Training Loss: 0.6561, Validation Loss: 0.6587, Training Accuracy: 0.6227, Validation Accuracy: 0.6309\n",
      "Epoch 180/300, Training Loss: 0.6811, Validation Loss: 0.6647, Training Accuracy: 0.6098, Validation Accuracy: 0.5973\n",
      "Epoch 190/300, Training Loss: 0.6438, Validation Loss: 0.6509, Training Accuracy: 0.6313, Validation Accuracy: 0.6443\n",
      "Epoch 200/300, Training Loss: 0.6519, Validation Loss: 0.6580, Training Accuracy: 0.6112, Validation Accuracy: 0.6174\n",
      "Epoch 210/300, Training Loss: 0.6828, Validation Loss: 0.6605, Training Accuracy: 0.6413, Validation Accuracy: 0.6040\n",
      "Epoch 220/300, Training Loss: 0.6742, Validation Loss: 0.6526, Training Accuracy: 0.5940, Validation Accuracy: 0.6242\n",
      "Epoch 230/300, Training Loss: 0.6643, Validation Loss: 0.6435, Training Accuracy: 0.6169, Validation Accuracy: 0.6376\n",
      "Epoch 240/300, Training Loss: 0.6518, Validation Loss: 0.6277, Training Accuracy: 0.6255, Validation Accuracy: 0.6711\n",
      "Epoch 250/300, Training Loss: 0.6436, Validation Loss: 0.6232, Training Accuracy: 0.6456, Validation Accuracy: 0.6711\n",
      "Epoch 260/300, Training Loss: 0.6519, Validation Loss: 0.6421, Training Accuracy: 0.6227, Validation Accuracy: 0.6577\n",
      "Epoch 270/300, Training Loss: 0.6358, Validation Loss: 0.6319, Training Accuracy: 0.6399, Validation Accuracy: 0.6577\n",
      "Epoch 280/300, Training Loss: 0.6499, Validation Loss: 0.6236, Training Accuracy: 0.6298, Validation Accuracy: 0.6577\n",
      "Epoch 290/300, Training Loss: 0.6430, Validation Loss: 0.6114, Training Accuracy: 0.6298, Validation Accuracy: 0.6711\n",
      "Epoch 300/300, Training Loss: 0.6554, Validation Loss: 0.6373, Training Accuracy: 0.6428, Validation Accuracy: 0.6443\n",
      "Test Loss: 0.6273, Test Accuracy: 0.6800\n",
      "Dropout: 0.7\n",
      "Test Accuracy: 0.6800\n",
      "Test Precision: 0.8966\n",
      "Test Recall: 0.3662\n",
      "Test F1 Score: 0.5200\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "dropout_rate = [0.3, 0.5, 0.7]\n",
    "for dropout in dropout_rate:\n",
    "    model = NN_dropout(input_size, hidden_size, output_size, dropout)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "\n",
    "    num_epochs = 300\n",
    "    batch_size = 64\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "                val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "                correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluation on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_losses = [test_loss] * num_epochs\n",
    "    test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"Dropout: {dropout}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Batch size:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_batchsize_lr(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN_batchsize_lr, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6837, Validation Loss: 0.6709, Training Accuracy: 0.5825, Validation Accuracy: 0.6107\n",
      "Epoch 20/300, Training Loss: 0.6645, Validation Loss: 0.6479, Training Accuracy: 0.5983, Validation Accuracy: 0.6040\n",
      "Epoch 30/300, Training Loss: 0.6622, Validation Loss: 0.6512, Training Accuracy: 0.6055, Validation Accuracy: 0.6040\n",
      "Epoch 40/300, Training Loss: 0.6578, Validation Loss: 0.6553, Training Accuracy: 0.6313, Validation Accuracy: 0.5906\n",
      "Epoch 50/300, Training Loss: 0.6414, Validation Loss: 0.6252, Training Accuracy: 0.6284, Validation Accuracy: 0.6443\n",
      "Epoch 60/300, Training Loss: 0.6524, Validation Loss: 0.6383, Training Accuracy: 0.6255, Validation Accuracy: 0.6040\n",
      "Epoch 70/300, Training Loss: 0.6464, Validation Loss: 0.6493, Training Accuracy: 0.6298, Validation Accuracy: 0.5772\n",
      "Epoch 80/300, Training Loss: 0.6356, Validation Loss: 0.6166, Training Accuracy: 0.6643, Validation Accuracy: 0.7181\n",
      "Epoch 90/300, Training Loss: 0.6172, Validation Loss: 0.5776, Training Accuracy: 0.6671, Validation Accuracy: 0.6913\n",
      "Epoch 100/300, Training Loss: 0.6481, Validation Loss: 0.5954, Training Accuracy: 0.6571, Validation Accuracy: 0.6711\n",
      "Epoch 110/300, Training Loss: 0.6338, Validation Loss: 0.5934, Training Accuracy: 0.6643, Validation Accuracy: 0.6846\n",
      "Epoch 120/300, Training Loss: 0.6297, Validation Loss: 0.5894, Training Accuracy: 0.6456, Validation Accuracy: 0.6779\n",
      "Epoch 130/300, Training Loss: 0.6189, Validation Loss: 0.5734, Training Accuracy: 0.6471, Validation Accuracy: 0.7047\n",
      "Epoch 140/300, Training Loss: 0.5928, Validation Loss: 0.5645, Training Accuracy: 0.6887, Validation Accuracy: 0.7383\n",
      "Epoch 150/300, Training Loss: 0.6951, Validation Loss: 0.6528, Training Accuracy: 0.6169, Validation Accuracy: 0.5973\n",
      "Epoch 160/300, Training Loss: 0.6327, Validation Loss: 0.5765, Training Accuracy: 0.6643, Validation Accuracy: 0.7517\n",
      "Epoch 170/300, Training Loss: 0.6412, Validation Loss: 0.5622, Training Accuracy: 0.6729, Validation Accuracy: 0.7114\n",
      "Epoch 180/300, Training Loss: 0.6253, Validation Loss: 0.5743, Training Accuracy: 0.6643, Validation Accuracy: 0.6779\n",
      "Epoch 190/300, Training Loss: 0.6100, Validation Loss: 0.5542, Training Accuracy: 0.6614, Validation Accuracy: 0.7315\n",
      "Epoch 200/300, Training Loss: 0.6326, Validation Loss: 0.5868, Training Accuracy: 0.6628, Validation Accuracy: 0.7047\n",
      "Epoch 210/300, Training Loss: 0.5900, Validation Loss: 0.5530, Training Accuracy: 0.6901, Validation Accuracy: 0.7248\n",
      "Epoch 220/300, Training Loss: 0.6175, Validation Loss: 0.5521, Training Accuracy: 0.6743, Validation Accuracy: 0.7718\n",
      "Epoch 230/300, Training Loss: 0.6118, Validation Loss: 0.5582, Training Accuracy: 0.6758, Validation Accuracy: 0.7315\n",
      "Epoch 240/300, Training Loss: 0.6043, Validation Loss: 0.5844, Training Accuracy: 0.6844, Validation Accuracy: 0.6644\n",
      "Epoch 250/300, Training Loss: 0.6516, Validation Loss: 0.6293, Training Accuracy: 0.6370, Validation Accuracy: 0.6174\n",
      "Epoch 260/300, Training Loss: 0.6038, Validation Loss: 0.5631, Training Accuracy: 0.6887, Validation Accuracy: 0.6846\n",
      "Epoch 270/300, Training Loss: 0.6221, Validation Loss: 0.5773, Training Accuracy: 0.6585, Validation Accuracy: 0.6779\n",
      "Epoch 280/300, Training Loss: 0.5927, Validation Loss: 0.5562, Training Accuracy: 0.6973, Validation Accuracy: 0.7047\n",
      "Epoch 290/300, Training Loss: 0.5929, Validation Loss: 0.5391, Training Accuracy: 0.6987, Validation Accuracy: 0.7383\n",
      "Epoch 300/300, Training Loss: 0.6283, Validation Loss: 0.5589, Training Accuracy: 0.6901, Validation Accuracy: 0.7248\n",
      "Test Loss: 0.5568, Test Accuracy: 0.7533\n",
      "Batch size: 32\n",
      "Test Accuracy: 0.7533\n",
      "Test Precision: 0.7656\n",
      "Test Recall: 0.6901\n",
      "Test F1 Score: 0.7259\n",
      "Epoch 10/300, Training Loss: 0.5944, Validation Loss: 0.5566, Training Accuracy: 0.7030, Validation Accuracy: 0.7047\n",
      "Epoch 20/300, Training Loss: 0.5921, Validation Loss: 0.5352, Training Accuracy: 0.7073, Validation Accuracy: 0.7315\n",
      "Epoch 30/300, Training Loss: 0.5812, Validation Loss: 0.5370, Training Accuracy: 0.7030, Validation Accuracy: 0.7383\n",
      "Epoch 40/300, Training Loss: 0.5707, Validation Loss: 0.5338, Training Accuracy: 0.7202, Validation Accuracy: 0.7383\n",
      "Epoch 50/300, Training Loss: 0.5889, Validation Loss: 0.5554, Training Accuracy: 0.7145, Validation Accuracy: 0.7114\n",
      "Epoch 60/300, Training Loss: 0.5964, Validation Loss: 0.5426, Training Accuracy: 0.6958, Validation Accuracy: 0.7450\n",
      "Epoch 70/300, Training Loss: 0.5856, Validation Loss: 0.5621, Training Accuracy: 0.7260, Validation Accuracy: 0.7315\n",
      "Epoch 80/300, Training Loss: 0.5825, Validation Loss: 0.5243, Training Accuracy: 0.7145, Validation Accuracy: 0.7248\n",
      "Epoch 90/300, Training Loss: 0.5985, Validation Loss: 0.5385, Training Accuracy: 0.7088, Validation Accuracy: 0.7248\n",
      "Epoch 100/300, Training Loss: 0.5798, Validation Loss: 0.5508, Training Accuracy: 0.7131, Validation Accuracy: 0.7047\n",
      "Epoch 110/300, Training Loss: 0.5874, Validation Loss: 0.5502, Training Accuracy: 0.7059, Validation Accuracy: 0.7383\n",
      "Epoch 120/300, Training Loss: 0.5731, Validation Loss: 0.5278, Training Accuracy: 0.7274, Validation Accuracy: 0.7383\n",
      "Epoch 130/300, Training Loss: 0.5700, Validation Loss: 0.5294, Training Accuracy: 0.7145, Validation Accuracy: 0.7383\n",
      "Epoch 140/300, Training Loss: 0.5898, Validation Loss: 0.5595, Training Accuracy: 0.7016, Validation Accuracy: 0.6913\n",
      "Epoch 150/300, Training Loss: 0.5939, Validation Loss: 0.5483, Training Accuracy: 0.6973, Validation Accuracy: 0.7181\n",
      "Epoch 160/300, Training Loss: 0.5731, Validation Loss: 0.5245, Training Accuracy: 0.7030, Validation Accuracy: 0.7383\n",
      "Epoch 170/300, Training Loss: 0.5694, Validation Loss: 0.5266, Training Accuracy: 0.7116, Validation Accuracy: 0.7450\n",
      "Epoch 180/300, Training Loss: 0.5666, Validation Loss: 0.5266, Training Accuracy: 0.7116, Validation Accuracy: 0.7450\n",
      "Epoch 190/300, Training Loss: 0.5777, Validation Loss: 0.5542, Training Accuracy: 0.7102, Validation Accuracy: 0.7047\n",
      "Epoch 200/300, Training Loss: 0.6151, Validation Loss: 0.5380, Training Accuracy: 0.7102, Validation Accuracy: 0.7315\n",
      "Epoch 210/300, Training Loss: 0.6098, Validation Loss: 0.5398, Training Accuracy: 0.7088, Validation Accuracy: 0.7315\n",
      "Epoch 220/300, Training Loss: 0.5958, Validation Loss: 0.5357, Training Accuracy: 0.7116, Validation Accuracy: 0.7383\n",
      "Epoch 230/300, Training Loss: 0.5542, Validation Loss: 0.5445, Training Accuracy: 0.7331, Validation Accuracy: 0.7383\n",
      "Epoch 240/300, Training Loss: 0.5941, Validation Loss: 0.5463, Training Accuracy: 0.7016, Validation Accuracy: 0.7248\n",
      "Epoch 250/300, Training Loss: 0.6130, Validation Loss: 0.5509, Training Accuracy: 0.7159, Validation Accuracy: 0.7181\n",
      "Epoch 260/300, Training Loss: 0.5979, Validation Loss: 0.5437, Training Accuracy: 0.6944, Validation Accuracy: 0.7114\n",
      "Epoch 270/300, Training Loss: 0.5892, Validation Loss: 0.5350, Training Accuracy: 0.7001, Validation Accuracy: 0.7248\n",
      "Epoch 280/300, Training Loss: 0.5787, Validation Loss: 0.5423, Training Accuracy: 0.7188, Validation Accuracy: 0.7248\n",
      "Epoch 290/300, Training Loss: 0.5887, Validation Loss: 0.5342, Training Accuracy: 0.6958, Validation Accuracy: 0.7315\n",
      "Epoch 300/300, Training Loss: 0.5757, Validation Loss: 0.5329, Training Accuracy: 0.7145, Validation Accuracy: 0.7181\n",
      "Test Loss: 0.5428, Test Accuracy: 0.7600\n",
      "Batch size: 64\n",
      "Test Accuracy: 0.7600\n",
      "Test Precision: 0.8302\n",
      "Test Recall: 0.6197\n",
      "Test F1 Score: 0.7097\n",
      "Epoch 10/300, Training Loss: 0.5757, Validation Loss: 0.5369, Training Accuracy: 0.6958, Validation Accuracy: 0.7248\n",
      "Epoch 20/300, Training Loss: 0.6100, Validation Loss: 0.5343, Training Accuracy: 0.6930, Validation Accuracy: 0.7248\n",
      "Epoch 30/300, Training Loss: 0.5891, Validation Loss: 0.5500, Training Accuracy: 0.7102, Validation Accuracy: 0.7315\n",
      "Epoch 40/300, Training Loss: 0.5898, Validation Loss: 0.5549, Training Accuracy: 0.6915, Validation Accuracy: 0.7047\n",
      "Epoch 50/300, Training Loss: 0.5675, Validation Loss: 0.5308, Training Accuracy: 0.7389, Validation Accuracy: 0.7383\n",
      "Epoch 60/300, Training Loss: 0.5681, Validation Loss: 0.5223, Training Accuracy: 0.7202, Validation Accuracy: 0.7315\n",
      "Epoch 70/300, Training Loss: 0.5823, Validation Loss: 0.5233, Training Accuracy: 0.7001, Validation Accuracy: 0.7248\n",
      "Epoch 80/300, Training Loss: 0.5732, Validation Loss: 0.5379, Training Accuracy: 0.7188, Validation Accuracy: 0.7181\n",
      "Epoch 90/300, Training Loss: 0.5729, Validation Loss: 0.5405, Training Accuracy: 0.7274, Validation Accuracy: 0.7248\n",
      "Epoch 100/300, Training Loss: 0.5652, Validation Loss: 0.5335, Training Accuracy: 0.7016, Validation Accuracy: 0.7315\n",
      "Epoch 110/300, Training Loss: 0.5748, Validation Loss: 0.5404, Training Accuracy: 0.7159, Validation Accuracy: 0.7248\n",
      "Epoch 120/300, Training Loss: 0.5676, Validation Loss: 0.5302, Training Accuracy: 0.7073, Validation Accuracy: 0.7383\n",
      "Epoch 130/300, Training Loss: 0.5897, Validation Loss: 0.5331, Training Accuracy: 0.6930, Validation Accuracy: 0.7181\n",
      "Epoch 140/300, Training Loss: 0.5581, Validation Loss: 0.5330, Training Accuracy: 0.7217, Validation Accuracy: 0.7383\n",
      "Epoch 150/300, Training Loss: 0.5701, Validation Loss: 0.5424, Training Accuracy: 0.6915, Validation Accuracy: 0.6846\n",
      "Epoch 160/300, Training Loss: 0.5715, Validation Loss: 0.5256, Training Accuracy: 0.7131, Validation Accuracy: 0.7450\n",
      "Epoch 170/300, Training Loss: 0.5732, Validation Loss: 0.5351, Training Accuracy: 0.7188, Validation Accuracy: 0.7114\n",
      "Epoch 180/300, Training Loss: 0.5660, Validation Loss: 0.5241, Training Accuracy: 0.7145, Validation Accuracy: 0.7248\n",
      "Epoch 190/300, Training Loss: 0.5707, Validation Loss: 0.5291, Training Accuracy: 0.7102, Validation Accuracy: 0.7450\n",
      "Epoch 200/300, Training Loss: 0.5786, Validation Loss: 0.5312, Training Accuracy: 0.7202, Validation Accuracy: 0.7450\n",
      "Epoch 210/300, Training Loss: 0.5598, Validation Loss: 0.5369, Training Accuracy: 0.7102, Validation Accuracy: 0.7181\n",
      "Epoch 220/300, Training Loss: 0.5481, Validation Loss: 0.5320, Training Accuracy: 0.7288, Validation Accuracy: 0.7450\n",
      "Epoch 230/300, Training Loss: 0.5563, Validation Loss: 0.5275, Training Accuracy: 0.7389, Validation Accuracy: 0.7248\n",
      "Epoch 240/300, Training Loss: 0.5490, Validation Loss: 0.5194, Training Accuracy: 0.7145, Validation Accuracy: 0.7517\n",
      "Epoch 250/300, Training Loss: 0.5809, Validation Loss: 0.5297, Training Accuracy: 0.7030, Validation Accuracy: 0.7383\n",
      "Epoch 260/300, Training Loss: 0.5729, Validation Loss: 0.5409, Training Accuracy: 0.7188, Validation Accuracy: 0.7248\n",
      "Epoch 270/300, Training Loss: 0.5740, Validation Loss: 0.5254, Training Accuracy: 0.6901, Validation Accuracy: 0.7450\n",
      "Epoch 280/300, Training Loss: 0.5554, Validation Loss: 0.5257, Training Accuracy: 0.7217, Validation Accuracy: 0.7315\n",
      "Epoch 290/300, Training Loss: 0.5670, Validation Loss: 0.5314, Training Accuracy: 0.7174, Validation Accuracy: 0.7383\n",
      "Epoch 300/300, Training Loss: 0.5616, Validation Loss: 0.5281, Training Accuracy: 0.7288, Validation Accuracy: 0.7517\n",
      "Test Loss: 0.5412, Test Accuracy: 0.7400\n",
      "Batch size: 128\n",
      "Test Accuracy: 0.7400\n",
      "Test Precision: 0.7963\n",
      "Test Recall: 0.6056\n",
      "Test F1 Score: 0.6880\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "batch_size_list = [32, 64, 128]\n",
    "for batch in batch_size_list:\n",
    "    \n",
    "    num_epochs = 300\n",
    "    batch_size = batch\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "                val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "                correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluation on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_losses = [test_loss] * num_epochs\n",
    "    test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Learning Rate:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.7759, Validation Loss: 0.6789, Training Accuracy: 0.5065, Validation Accuracy: 0.6443\n",
      "Epoch 20/300, Training Loss: 0.7066, Validation Loss: 0.6834, Training Accuracy: 0.5308, Validation Accuracy: 0.5973\n",
      "Epoch 30/300, Training Loss: 0.6874, Validation Loss: 0.6841, Training Accuracy: 0.5509, Validation Accuracy: 0.5503\n",
      "Epoch 40/300, Training Loss: 0.6799, Validation Loss: 0.6791, Training Accuracy: 0.5567, Validation Accuracy: 0.5906\n",
      "Epoch 50/300, Training Loss: 0.6739, Validation Loss: 0.6731, Training Accuracy: 0.5954, Validation Accuracy: 0.5906\n",
      "Epoch 60/300, Training Loss: 0.6665, Validation Loss: 0.6667, Training Accuracy: 0.6011, Validation Accuracy: 0.5772\n",
      "Epoch 70/300, Training Loss: 0.6602, Validation Loss: 0.6631, Training Accuracy: 0.6098, Validation Accuracy: 0.5973\n",
      "Epoch 80/300, Training Loss: 0.6588, Validation Loss: 0.6617, Training Accuracy: 0.6069, Validation Accuracy: 0.5906\n",
      "Epoch 90/300, Training Loss: 0.6651, Validation Loss: 0.6540, Training Accuracy: 0.5968, Validation Accuracy: 0.5906\n",
      "Epoch 100/300, Training Loss: 0.6563, Validation Loss: 0.6558, Training Accuracy: 0.6083, Validation Accuracy: 0.5973\n",
      "Epoch 110/300, Training Loss: 0.6379, Validation Loss: 0.6384, Training Accuracy: 0.6413, Validation Accuracy: 0.6174\n",
      "Epoch 120/300, Training Loss: 0.6499, Validation Loss: 0.6386, Training Accuracy: 0.6385, Validation Accuracy: 0.6107\n",
      "Epoch 130/300, Training Loss: 0.6401, Validation Loss: 0.6427, Training Accuracy: 0.6557, Validation Accuracy: 0.6040\n",
      "Epoch 140/300, Training Loss: 0.6308, Validation Loss: 0.6204, Training Accuracy: 0.6485, Validation Accuracy: 0.6577\n",
      "Epoch 150/300, Training Loss: 0.6357, Validation Loss: 0.6273, Training Accuracy: 0.6514, Validation Accuracy: 0.6040\n",
      "Epoch 160/300, Training Loss: 0.6255, Validation Loss: 0.6183, Training Accuracy: 0.6557, Validation Accuracy: 0.6510\n",
      "Epoch 170/300, Training Loss: 0.6174, Validation Loss: 0.6137, Training Accuracy: 0.6801, Validation Accuracy: 0.6443\n",
      "Epoch 180/300, Training Loss: 0.6137, Validation Loss: 0.6050, Training Accuracy: 0.6758, Validation Accuracy: 0.7047\n",
      "Epoch 190/300, Training Loss: 0.6043, Validation Loss: 0.6026, Training Accuracy: 0.6987, Validation Accuracy: 0.6779\n",
      "Epoch 200/300, Training Loss: 0.6049, Validation Loss: 0.5988, Training Accuracy: 0.6901, Validation Accuracy: 0.6846\n",
      "Epoch 210/300, Training Loss: 0.6090, Validation Loss: 0.6095, Training Accuracy: 0.6901, Validation Accuracy: 0.6174\n",
      "Epoch 220/300, Training Loss: 0.5947, Validation Loss: 0.5961, Training Accuracy: 0.6714, Validation Accuracy: 0.6644\n",
      "Epoch 230/300, Training Loss: 0.6178, Validation Loss: 0.6066, Training Accuracy: 0.6557, Validation Accuracy: 0.6376\n",
      "Epoch 240/300, Training Loss: 0.6092, Validation Loss: 0.5991, Training Accuracy: 0.6858, Validation Accuracy: 0.6443\n",
      "Epoch 250/300, Training Loss: 0.6022, Validation Loss: 0.5979, Training Accuracy: 0.6887, Validation Accuracy: 0.6644\n",
      "Epoch 260/300, Training Loss: 0.5986, Validation Loss: 0.5941, Training Accuracy: 0.6772, Validation Accuracy: 0.6510\n",
      "Epoch 270/300, Training Loss: 0.6014, Validation Loss: 0.6053, Training Accuracy: 0.6772, Validation Accuracy: 0.6242\n",
      "Epoch 280/300, Training Loss: 0.6047, Validation Loss: 0.6026, Training Accuracy: 0.6758, Validation Accuracy: 0.6242\n",
      "Epoch 290/300, Training Loss: 0.5823, Validation Loss: 0.6028, Training Accuracy: 0.6872, Validation Accuracy: 0.6242\n",
      "Epoch 300/300, Training Loss: 0.6026, Validation Loss: 0.5995, Training Accuracy: 0.6944, Validation Accuracy: 0.6376\n",
      "Test Loss: 0.6146, Test Accuracy: 0.6667\n",
      "Learning Rate: 0.001\n",
      "Test Accuracy: 0.6667\n",
      "Test Precision: 0.8000\n",
      "Test Recall: 0.3944\n",
      "Test F1 Score: 0.5283\n",
      "Epoch 10/300, Training Loss: 0.6548, Validation Loss: 0.6257, Training Accuracy: 0.6155, Validation Accuracy: 0.6309\n",
      "Epoch 20/300, Training Loss: 0.6386, Validation Loss: 0.6030, Training Accuracy: 0.6298, Validation Accuracy: 0.6577\n",
      "Epoch 30/300, Training Loss: 0.6181, Validation Loss: 0.6059, Training Accuracy: 0.6872, Validation Accuracy: 0.6711\n",
      "Epoch 40/300, Training Loss: 0.6293, Validation Loss: 0.5959, Training Accuracy: 0.6700, Validation Accuracy: 0.6577\n",
      "Epoch 50/300, Training Loss: 0.6303, Validation Loss: 0.5972, Training Accuracy: 0.6815, Validation Accuracy: 0.6644\n",
      "Epoch 60/300, Training Loss: 0.6144, Validation Loss: 0.6154, Training Accuracy: 0.6671, Validation Accuracy: 0.6242\n",
      "Epoch 70/300, Training Loss: 0.6273, Validation Loss: 0.5974, Training Accuracy: 0.6528, Validation Accuracy: 0.7181\n",
      "Epoch 80/300, Training Loss: 0.5983, Validation Loss: 0.6244, Training Accuracy: 0.6858, Validation Accuracy: 0.6309\n",
      "Epoch 90/300, Training Loss: 0.6005, Validation Loss: 0.5626, Training Accuracy: 0.6729, Validation Accuracy: 0.7181\n",
      "Epoch 100/300, Training Loss: 0.6808, Validation Loss: 0.5847, Training Accuracy: 0.6930, Validation Accuracy: 0.7047\n",
      "Epoch 110/300, Training Loss: 0.5906, Validation Loss: 0.5740, Training Accuracy: 0.6858, Validation Accuracy: 0.6779\n",
      "Epoch 120/300, Training Loss: 0.6408, Validation Loss: 0.6525, Training Accuracy: 0.6887, Validation Accuracy: 0.6644\n",
      "Epoch 130/300, Training Loss: 0.6104, Validation Loss: 0.5551, Training Accuracy: 0.6901, Validation Accuracy: 0.7248\n",
      "Epoch 140/300, Training Loss: 0.5675, Validation Loss: 0.5523, Training Accuracy: 0.7016, Validation Accuracy: 0.7114\n",
      "Epoch 150/300, Training Loss: 0.6083, Validation Loss: 0.5502, Training Accuracy: 0.6930, Validation Accuracy: 0.7181\n",
      "Epoch 160/300, Training Loss: 0.5918, Validation Loss: 0.5348, Training Accuracy: 0.6915, Validation Accuracy: 0.7383\n",
      "Epoch 170/300, Training Loss: 0.6065, Validation Loss: 0.5511, Training Accuracy: 0.6930, Validation Accuracy: 0.7248\n",
      "Epoch 180/300, Training Loss: 0.6255, Validation Loss: 0.5455, Training Accuracy: 0.6944, Validation Accuracy: 0.7315\n",
      "Epoch 190/300, Training Loss: 0.5981, Validation Loss: 0.5566, Training Accuracy: 0.6944, Validation Accuracy: 0.7181\n",
      "Epoch 200/300, Training Loss: 0.5846, Validation Loss: 0.5309, Training Accuracy: 0.7131, Validation Accuracy: 0.7248\n",
      "Epoch 210/300, Training Loss: 0.6032, Validation Loss: 0.5347, Training Accuracy: 0.6786, Validation Accuracy: 0.7114\n",
      "Epoch 220/300, Training Loss: 0.5691, Validation Loss: 0.5321, Training Accuracy: 0.7102, Validation Accuracy: 0.7181\n",
      "Epoch 230/300, Training Loss: 0.6360, Validation Loss: 0.5592, Training Accuracy: 0.6887, Validation Accuracy: 0.6846\n",
      "Epoch 240/300, Training Loss: 0.6096, Validation Loss: 0.5318, Training Accuracy: 0.6958, Validation Accuracy: 0.7450\n",
      "Epoch 250/300, Training Loss: 0.5628, Validation Loss: 0.5418, Training Accuracy: 0.7260, Validation Accuracy: 0.7114\n",
      "Epoch 260/300, Training Loss: 0.6003, Validation Loss: 0.5327, Training Accuracy: 0.6858, Validation Accuracy: 0.7315\n",
      "Epoch 270/300, Training Loss: 0.5742, Validation Loss: 0.5872, Training Accuracy: 0.7131, Validation Accuracy: 0.6376\n",
      "Epoch 280/300, Training Loss: 0.6081, Validation Loss: 0.5346, Training Accuracy: 0.6987, Validation Accuracy: 0.7383\n",
      "Epoch 290/300, Training Loss: 0.5841, Validation Loss: 0.5341, Training Accuracy: 0.7116, Validation Accuracy: 0.7383\n",
      "Epoch 300/300, Training Loss: 0.6123, Validation Loss: 0.5513, Training Accuracy: 0.6729, Validation Accuracy: 0.7450\n",
      "Test Loss: 0.5837, Test Accuracy: 0.7133\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.7133\n",
      "Test Precision: 0.7593\n",
      "Test Recall: 0.5775\n",
      "Test F1 Score: 0.6560\n",
      "Epoch 10/300, Training Loss: 0.6103, Validation Loss: 0.5466, Training Accuracy: 0.6901, Validation Accuracy: 0.7047\n",
      "Epoch 20/300, Training Loss: 0.6248, Validation Loss: 0.5864, Training Accuracy: 0.6758, Validation Accuracy: 0.7248\n",
      "Epoch 30/300, Training Loss: 0.6533, Validation Loss: 0.5714, Training Accuracy: 0.6557, Validation Accuracy: 0.7114\n",
      "Epoch 40/300, Training Loss: 0.6103, Validation Loss: 0.5362, Training Accuracy: 0.6729, Validation Accuracy: 0.7315\n",
      "Epoch 50/300, Training Loss: 0.5976, Validation Loss: 0.5716, Training Accuracy: 0.6901, Validation Accuracy: 0.7114\n",
      "Epoch 60/300, Training Loss: 0.6085, Validation Loss: 0.5737, Training Accuracy: 0.6944, Validation Accuracy: 0.6913\n",
      "Epoch 70/300, Training Loss: 0.6092, Validation Loss: 0.5554, Training Accuracy: 0.6801, Validation Accuracy: 0.7114\n",
      "Epoch 80/300, Training Loss: 0.6165, Validation Loss: 0.5394, Training Accuracy: 0.6686, Validation Accuracy: 0.7047\n",
      "Epoch 90/300, Training Loss: 0.5878, Validation Loss: 0.5211, Training Accuracy: 0.6872, Validation Accuracy: 0.7517\n",
      "Epoch 100/300, Training Loss: 0.6012, Validation Loss: 0.5532, Training Accuracy: 0.6801, Validation Accuracy: 0.7383\n",
      "Epoch 110/300, Training Loss: 0.5919, Validation Loss: 0.5398, Training Accuracy: 0.6815, Validation Accuracy: 0.7114\n",
      "Epoch 120/300, Training Loss: 0.5992, Validation Loss: 0.5400, Training Accuracy: 0.6858, Validation Accuracy: 0.7450\n",
      "Epoch 130/300, Training Loss: 0.5928, Validation Loss: 0.5608, Training Accuracy: 0.6772, Validation Accuracy: 0.6846\n",
      "Epoch 140/300, Training Loss: 0.5750, Validation Loss: 0.5344, Training Accuracy: 0.7188, Validation Accuracy: 0.7248\n",
      "Epoch 150/300, Training Loss: 0.6390, Validation Loss: 0.5551, Training Accuracy: 0.6758, Validation Accuracy: 0.7450\n",
      "Epoch 160/300, Training Loss: 0.6023, Validation Loss: 0.5334, Training Accuracy: 0.6887, Validation Accuracy: 0.7383\n",
      "Epoch 170/300, Training Loss: 0.6550, Validation Loss: 0.5704, Training Accuracy: 0.6686, Validation Accuracy: 0.6846\n",
      "Epoch 180/300, Training Loss: 0.6140, Validation Loss: 0.5494, Training Accuracy: 0.6714, Validation Accuracy: 0.7517\n",
      "Epoch 190/300, Training Loss: 0.6103, Validation Loss: 0.5533, Training Accuracy: 0.6844, Validation Accuracy: 0.6913\n",
      "Epoch 200/300, Training Loss: 0.5964, Validation Loss: 0.5363, Training Accuracy: 0.6557, Validation Accuracy: 0.7248\n",
      "Epoch 210/300, Training Loss: 0.5981, Validation Loss: 0.5874, Training Accuracy: 0.6714, Validation Accuracy: 0.7181\n",
      "Epoch 220/300, Training Loss: 0.6045, Validation Loss: 0.5269, Training Accuracy: 0.7030, Validation Accuracy: 0.7584\n",
      "Epoch 230/300, Training Loss: 0.6217, Validation Loss: 0.5688, Training Accuracy: 0.7159, Validation Accuracy: 0.6980\n",
      "Epoch 240/300, Training Loss: 0.5934, Validation Loss: 0.5697, Training Accuracy: 0.7044, Validation Accuracy: 0.7248\n",
      "Epoch 250/300, Training Loss: 0.6137, Validation Loss: 0.5352, Training Accuracy: 0.6729, Validation Accuracy: 0.7383\n",
      "Epoch 260/300, Training Loss: 0.6069, Validation Loss: 0.5505, Training Accuracy: 0.6987, Validation Accuracy: 0.7181\n",
      "Epoch 270/300, Training Loss: 0.6116, Validation Loss: 0.5596, Training Accuracy: 0.6600, Validation Accuracy: 0.7248\n",
      "Epoch 280/300, Training Loss: 0.6076, Validation Loss: 0.5341, Training Accuracy: 0.6628, Validation Accuracy: 0.7315\n",
      "Epoch 290/300, Training Loss: 0.6016, Validation Loss: 0.5400, Training Accuracy: 0.6930, Validation Accuracy: 0.7315\n",
      "Epoch 300/300, Training Loss: 0.5952, Validation Loss: 0.5533, Training Accuracy: 0.6815, Validation Accuracy: 0.7181\n",
      "Test Loss: 0.5432, Test Accuracy: 0.7533\n",
      "Learning Rate: 0.01\n",
      "Test Accuracy: 0.7533\n",
      "Test Precision: 0.7931\n",
      "Test Recall: 0.6479\n",
      "Test F1 Score: 0.7132\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning_rates = [0.001, 0.007, 0.01]\n",
    "for learning in learning_rates:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "    num_epochs = 300\n",
    "    batch_size = 64\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "                val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "                correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluation on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_losses = [test_loss] * num_epochs\n",
    "    test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"Learning Rate: {learning}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_batchsize_lr(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN_batchsize_lr, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6521, Validation Loss: 0.6205, Training Accuracy: 0.6528, Validation Accuracy: 0.7248\n",
      "Epoch 10/300, Test Loss: 0.6444, Test Accuracy: 0.6200\n",
      "Epoch 20/300, Training Loss: 0.6543, Validation Loss: 0.6081, Training Accuracy: 0.6356, Validation Accuracy: 0.7114\n",
      "Epoch 20/300, Test Loss: 0.6368, Test Accuracy: 0.6267\n",
      "Epoch 30/300, Training Loss: 0.6155, Validation Loss: 0.5712, Training Accuracy: 0.6901, Validation Accuracy: 0.7181\n",
      "Epoch 30/300, Test Loss: 0.6047, Test Accuracy: 0.6533\n",
      "Epoch 40/300, Training Loss: 0.6216, Validation Loss: 0.5872, Training Accuracy: 0.6743, Validation Accuracy: 0.6846\n",
      "Epoch 40/300, Test Loss: 0.6132, Test Accuracy: 0.6400\n",
      "Epoch 50/300, Training Loss: 0.6011, Validation Loss: 0.5903, Training Accuracy: 0.6944, Validation Accuracy: 0.6711\n",
      "Epoch 50/300, Test Loss: 0.6128, Test Accuracy: 0.6600\n",
      "Epoch 60/300, Training Loss: 0.6024, Validation Loss: 0.5821, Training Accuracy: 0.6901, Validation Accuracy: 0.6913\n",
      "Epoch 60/300, Test Loss: 0.5904, Test Accuracy: 0.6933\n",
      "Epoch 70/300, Training Loss: 0.5907, Validation Loss: 0.5619, Training Accuracy: 0.6844, Validation Accuracy: 0.7315\n",
      "Epoch 70/300, Test Loss: 0.5756, Test Accuracy: 0.6733\n",
      "Epoch 80/300, Training Loss: 0.6129, Validation Loss: 0.5923, Training Accuracy: 0.6829, Validation Accuracy: 0.7114\n",
      "Epoch 80/300, Test Loss: 0.5808, Test Accuracy: 0.6800\n",
      "Epoch 90/300, Training Loss: 0.5876, Validation Loss: 0.5731, Training Accuracy: 0.6958, Validation Accuracy: 0.6779\n",
      "Epoch 90/300, Test Loss: 0.5780, Test Accuracy: 0.7467\n",
      "Epoch 100/300, Training Loss: 0.5681, Validation Loss: 0.5639, Training Accuracy: 0.7217, Validation Accuracy: 0.6779\n",
      "Epoch 100/300, Test Loss: 0.5601, Test Accuracy: 0.7467\n",
      "Epoch 110/300, Training Loss: 0.6022, Validation Loss: 0.5968, Training Accuracy: 0.6887, Validation Accuracy: 0.6510\n",
      "Epoch 110/300, Test Loss: 0.5792, Test Accuracy: 0.7200\n",
      "Epoch 120/300, Training Loss: 0.5775, Validation Loss: 0.5449, Training Accuracy: 0.7145, Validation Accuracy: 0.7315\n",
      "Epoch 120/300, Test Loss: 0.5563, Test Accuracy: 0.7200\n",
      "Epoch 130/300, Training Loss: 0.5569, Validation Loss: 0.5775, Training Accuracy: 0.7231, Validation Accuracy: 0.6913\n",
      "Epoch 130/300, Test Loss: 0.5376, Test Accuracy: 0.7533\n",
      "Epoch 140/300, Training Loss: 0.5566, Validation Loss: 0.5744, Training Accuracy: 0.7102, Validation Accuracy: 0.7047\n",
      "Epoch 140/300, Test Loss: 0.5467, Test Accuracy: 0.7267\n",
      "Epoch 150/300, Training Loss: 0.5500, Validation Loss: 0.5521, Training Accuracy: 0.7217, Validation Accuracy: 0.7315\n",
      "Epoch 150/300, Test Loss: 0.5434, Test Accuracy: 0.7333\n",
      "Epoch 160/300, Training Loss: 0.5623, Validation Loss: 0.5627, Training Accuracy: 0.7159, Validation Accuracy: 0.6779\n",
      "Epoch 160/300, Test Loss: 0.5179, Test Accuracy: 0.7667\n",
      "Epoch 170/300, Training Loss: 0.5347, Validation Loss: 0.5615, Training Accuracy: 0.7389, Validation Accuracy: 0.7114\n",
      "Epoch 170/300, Test Loss: 0.5203, Test Accuracy: 0.7533\n",
      "Epoch 180/300, Training Loss: 0.5816, Validation Loss: 0.5775, Training Accuracy: 0.7059, Validation Accuracy: 0.7047\n",
      "Epoch 180/300, Test Loss: 0.5312, Test Accuracy: 0.7400\n",
      "Epoch 190/300, Training Loss: 0.6004, Validation Loss: 0.5455, Training Accuracy: 0.7016, Validation Accuracy: 0.7047\n",
      "Epoch 190/300, Test Loss: 0.5473, Test Accuracy: 0.7600\n",
      "Epoch 200/300, Training Loss: 0.5530, Validation Loss: 0.5479, Training Accuracy: 0.7346, Validation Accuracy: 0.6913\n",
      "Epoch 200/300, Test Loss: 0.5158, Test Accuracy: 0.7733\n",
      "Epoch 210/300, Training Loss: 0.5763, Validation Loss: 0.5507, Training Accuracy: 0.7131, Validation Accuracy: 0.6913\n",
      "Epoch 210/300, Test Loss: 0.5302, Test Accuracy: 0.7733\n",
      "Epoch 220/300, Training Loss: 0.5595, Validation Loss: 0.5432, Training Accuracy: 0.6958, Validation Accuracy: 0.6913\n",
      "Epoch 220/300, Test Loss: 0.5034, Test Accuracy: 0.7667\n",
      "Epoch 230/300, Training Loss: 0.5503, Validation Loss: 0.5478, Training Accuracy: 0.7260, Validation Accuracy: 0.7114\n",
      "Epoch 230/300, Test Loss: 0.5355, Test Accuracy: 0.7667\n",
      "Epoch 240/300, Training Loss: 0.5429, Validation Loss: 0.5565, Training Accuracy: 0.7303, Validation Accuracy: 0.6711\n",
      "Epoch 240/300, Test Loss: 0.5211, Test Accuracy: 0.7733\n",
      "Epoch 250/300, Training Loss: 0.5396, Validation Loss: 0.5424, Training Accuracy: 0.7116, Validation Accuracy: 0.7047\n",
      "Epoch 250/300, Test Loss: 0.5084, Test Accuracy: 0.7933\n",
      "Epoch 260/300, Training Loss: 0.5602, Validation Loss: 0.5519, Training Accuracy: 0.6930, Validation Accuracy: 0.6846\n",
      "Epoch 260/300, Test Loss: 0.5368, Test Accuracy: 0.7667\n",
      "Epoch 270/300, Training Loss: 0.5678, Validation Loss: 0.5772, Training Accuracy: 0.6858, Validation Accuracy: 0.6510\n",
      "Epoch 270/300, Test Loss: 0.5313, Test Accuracy: 0.7800\n",
      "Epoch 280/300, Training Loss: 0.5675, Validation Loss: 0.5510, Training Accuracy: 0.7217, Validation Accuracy: 0.7248\n",
      "Epoch 280/300, Test Loss: 0.5281, Test Accuracy: 0.7400\n",
      "Epoch 290/300, Training Loss: 0.5123, Validation Loss: 0.5374, Training Accuracy: 0.7331, Validation Accuracy: 0.6779\n",
      "Epoch 290/300, Test Loss: 0.5002, Test Accuracy: 0.7733\n",
      "Epoch 300/300, Training Loss: 0.5341, Validation Loss: 0.5344, Training Accuracy: 0.7274, Validation Accuracy: 0.7248\n",
      "Epoch 300/300, Test Loss: 0.5199, Test Accuracy: 0.7667\n",
      "Training completed in: 0.37 minutes\n",
      "Test Loss: 0.5199, Test Accuracy: 0.7667\n",
      "Batch size: 64\n",
      "Test Accuracy: 0.7667\n",
      "Test Precision: 0.7432\n",
      "Test Recall: 0.7746\n",
      "Test F1 Score: 0.7586\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "    \n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_accuracies_base_model = []\n",
    "start_time_base_model = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    test_accuracies_base_model.append(test_accuracy)  # Store test accuracy for this epoch\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "end_time_base_model = time.time()\n",
    "print(f\"Training completed in: {(end_time_base_model - start_time_base_model) / 60:.2f} minutes\")\n",
    "        \n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6845, Validation Loss: 0.6709, Training Accuracy: 0.5653, Validation Accuracy: 0.5906\n",
      "Epoch 20/300, Training Loss: 0.6787, Validation Loss: 0.6732, Training Accuracy: 0.5983, Validation Accuracy: 0.5302\n",
      "Epoch 30/300, Training Loss: 0.6505, Validation Loss: 0.6575, Training Accuracy: 0.6255, Validation Accuracy: 0.6107\n",
      "Epoch 40/300, Training Loss: 0.6574, Validation Loss: 0.6562, Training Accuracy: 0.6026, Validation Accuracy: 0.5906\n",
      "Epoch 50/300, Training Loss: 0.6579, Validation Loss: 0.6473, Training Accuracy: 0.6298, Validation Accuracy: 0.6376\n",
      "Early stopping triggered after 57 epochs!\n",
      "Test Loss: 0.6300, Test Accuracy: 0.6733\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.6733\n",
      "Test Precision: 0.6774\n",
      "Test Recall: 0.5915\n",
      "Test F1 Score: 0.6316\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "best_val_loss = float('inf')  # Initialize with a very large value\n",
    "early_stopping_patience = 10  # Number of epochs to wait for improvement before stopping\n",
    "no_improvement_epochs = 0  # Counter for epochs without improvement\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    \n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Early Stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0  # Reset counter if validation loss improves\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        no_improvement_epochs += 1  # Increase counter if no improvement\n",
    "    \n",
    "    if no_improvement_epochs >= early_stopping_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Load the best model saved during training\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Learning Rate: {learning}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6831, Validation Loss: 0.6766, Training Accuracy: 0.5983, Validation Accuracy: 0.5973\n",
      "Epoch 20/300, Training Loss: 0.6781, Validation Loss: 0.6690, Training Accuracy: 0.5854, Validation Accuracy: 0.5839\n",
      "Epoch 00025: reducing learning rate of group 0 to 7.0000e-04.\n",
      "Epoch 30/300, Training Loss: 0.6573, Validation Loss: 0.6515, Training Accuracy: 0.6298, Validation Accuracy: 0.6376\n",
      "Epoch 40/300, Training Loss: 0.6563, Validation Loss: 0.6473, Training Accuracy: 0.6428, Validation Accuracy: 0.6510\n",
      "Epoch 00045: reducing learning rate of group 0 to 7.0000e-05.\n",
      "Epoch 50/300, Training Loss: 0.6521, Validation Loss: 0.6458, Training Accuracy: 0.6456, Validation Accuracy: 0.6510\n",
      "Epoch 00051: reducing learning rate of group 0 to 7.0000e-06.\n",
      "Epoch 00057: reducing learning rate of group 0 to 7.0000e-07.\n",
      "Epoch 60/300, Training Loss: 0.6490, Validation Loss: 0.6457, Training Accuracy: 0.6341, Validation Accuracy: 0.6577\n",
      "Epoch 00063: reducing learning rate of group 0 to 7.0000e-08.\n",
      "Epoch 00069: reducing learning rate of group 0 to 7.0000e-09.\n",
      "Epoch 70/300, Training Loss: 0.6388, Validation Loss: 0.6457, Training Accuracy: 0.6542, Validation Accuracy: 0.6577\n",
      "Epoch 80/300, Training Loss: 0.6369, Validation Loss: 0.6457, Training Accuracy: 0.6413, Validation Accuracy: 0.6577\n",
      "Epoch 90/300, Training Loss: 0.6423, Validation Loss: 0.6457, Training Accuracy: 0.6714, Validation Accuracy: 0.6577\n",
      "Epoch 100/300, Training Loss: 0.6598, Validation Loss: 0.6457, Training Accuracy: 0.6341, Validation Accuracy: 0.6577\n",
      "Epoch 110/300, Training Loss: 0.6460, Validation Loss: 0.6457, Training Accuracy: 0.6198, Validation Accuracy: 0.6577\n",
      "Epoch 120/300, Training Loss: 0.6470, Validation Loss: 0.6457, Training Accuracy: 0.6628, Validation Accuracy: 0.6577\n",
      "Epoch 130/300, Training Loss: 0.6451, Validation Loss: 0.6457, Training Accuracy: 0.6370, Validation Accuracy: 0.6577\n",
      "Epoch 140/300, Training Loss: 0.6568, Validation Loss: 0.6457, Training Accuracy: 0.6341, Validation Accuracy: 0.6577\n",
      "Epoch 150/300, Training Loss: 0.6469, Validation Loss: 0.6457, Training Accuracy: 0.6485, Validation Accuracy: 0.6577\n",
      "Epoch 160/300, Training Loss: 0.6418, Validation Loss: 0.6457, Training Accuracy: 0.6356, Validation Accuracy: 0.6577\n",
      "Epoch 170/300, Training Loss: 0.6499, Validation Loss: 0.6457, Training Accuracy: 0.6428, Validation Accuracy: 0.6577\n",
      "Epoch 180/300, Training Loss: 0.6573, Validation Loss: 0.6457, Training Accuracy: 0.6341, Validation Accuracy: 0.6577\n",
      "Epoch 190/300, Training Loss: 0.6509, Validation Loss: 0.6457, Training Accuracy: 0.6284, Validation Accuracy: 0.6577\n",
      "Epoch 200/300, Training Loss: 0.6491, Validation Loss: 0.6457, Training Accuracy: 0.6198, Validation Accuracy: 0.6577\n",
      "Epoch 210/300, Training Loss: 0.6365, Validation Loss: 0.6457, Training Accuracy: 0.6428, Validation Accuracy: 0.6577\n",
      "Epoch 220/300, Training Loss: 0.6491, Validation Loss: 0.6457, Training Accuracy: 0.6270, Validation Accuracy: 0.6577\n",
      "Epoch 230/300, Training Loss: 0.6438, Validation Loss: 0.6457, Training Accuracy: 0.6700, Validation Accuracy: 0.6577\n",
      "Epoch 240/300, Training Loss: 0.6438, Validation Loss: 0.6457, Training Accuracy: 0.6456, Validation Accuracy: 0.6577\n",
      "Epoch 250/300, Training Loss: 0.6582, Validation Loss: 0.6457, Training Accuracy: 0.6270, Validation Accuracy: 0.6577\n",
      "Epoch 260/300, Training Loss: 0.6522, Validation Loss: 0.6457, Training Accuracy: 0.6284, Validation Accuracy: 0.6577\n",
      "Epoch 270/300, Training Loss: 0.6477, Validation Loss: 0.6457, Training Accuracy: 0.6614, Validation Accuracy: 0.6577\n",
      "Epoch 280/300, Training Loss: 0.6479, Validation Loss: 0.6457, Training Accuracy: 0.6571, Validation Accuracy: 0.6577\n",
      "Epoch 290/300, Training Loss: 0.6530, Validation Loss: 0.6457, Training Accuracy: 0.6341, Validation Accuracy: 0.6577\n",
      "Epoch 300/300, Training Loss: 0.6577, Validation Loss: 0.6457, Training Accuracy: 0.6141, Validation Accuracy: 0.6577\n",
      "Test Loss: 0.6567, Test Accuracy: 0.6667\n",
      "Test Accuracy: 0.6667\n",
      "Test Precision: 0.7059\n",
      "Test Recall: 0.5070\n",
      "Test F1 Score: 0.5902\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_batchnorm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN_batchnorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)  # Add BatchNorm\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(self.bn1(x))  # Apply BatchNorm\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(self.bn2(x))  # Apply BatchNorm\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(self.bn3(x))  # Apply BatchNorm\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Replace the model with the one using batch normalization\n",
    "model = NN_batchnorm(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.5063, Validation Loss: 0.5475, Training Accuracy: 0.7561, Validation Accuracy: 0.7047\n",
      "Epoch 10/300, Test Loss: 0.5215, Test Accuracy: 0.7267\n",
      "Epoch 20/300, Training Loss: 0.4938, Validation Loss: 0.5697, Training Accuracy: 0.7547, Validation Accuracy: 0.6711\n",
      "Epoch 20/300, Test Loss: 0.5292, Test Accuracy: 0.6933\n",
      "Epoch 30/300, Training Loss: 0.4908, Validation Loss: 0.5579, Training Accuracy: 0.7618, Validation Accuracy: 0.6980\n",
      "Epoch 30/300, Test Loss: 0.4923, Test Accuracy: 0.7867\n",
      "Epoch 40/300, Training Loss: 0.4645, Validation Loss: 0.5525, Training Accuracy: 0.7805, Validation Accuracy: 0.6913\n",
      "Epoch 40/300, Test Loss: 0.4913, Test Accuracy: 0.7800\n",
      "Epoch 50/300, Training Loss: 0.4773, Validation Loss: 0.5514, Training Accuracy: 0.7805, Validation Accuracy: 0.6846\n",
      "Epoch 50/300, Test Loss: 0.4884, Test Accuracy: 0.7867\n",
      "Epoch 60/300, Training Loss: 0.4615, Validation Loss: 0.5461, Training Accuracy: 0.7747, Validation Accuracy: 0.7315\n",
      "Epoch 60/300, Test Loss: 0.4768, Test Accuracy: 0.7800\n",
      "Epoch 70/300, Training Loss: 0.4711, Validation Loss: 0.5634, Training Accuracy: 0.7819, Validation Accuracy: 0.6779\n",
      "Epoch 70/300, Test Loss: 0.4852, Test Accuracy: 0.7733\n",
      "Epoch 80/300, Training Loss: 0.4566, Validation Loss: 0.5500, Training Accuracy: 0.7905, Validation Accuracy: 0.6980\n",
      "Epoch 80/300, Test Loss: 0.4934, Test Accuracy: 0.7800\n",
      "Epoch 90/300, Training Loss: 0.4492, Validation Loss: 0.5659, Training Accuracy: 0.7848, Validation Accuracy: 0.7047\n",
      "Epoch 90/300, Test Loss: 0.5003, Test Accuracy: 0.7800\n",
      "Epoch 100/300, Training Loss: 0.4599, Validation Loss: 0.5715, Training Accuracy: 0.7633, Validation Accuracy: 0.6644\n",
      "Epoch 100/300, Test Loss: 0.4879, Test Accuracy: 0.7867\n",
      "Epoch 110/300, Training Loss: 0.4313, Validation Loss: 0.5817, Training Accuracy: 0.7977, Validation Accuracy: 0.7047\n",
      "Epoch 110/300, Test Loss: 0.4727, Test Accuracy: 0.7733\n",
      "Epoch 120/300, Training Loss: 0.4154, Validation Loss: 0.5454, Training Accuracy: 0.7977, Validation Accuracy: 0.7181\n",
      "Epoch 120/300, Test Loss: 0.4737, Test Accuracy: 0.8000\n",
      "Epoch 130/300, Training Loss: 0.4494, Validation Loss: 0.5596, Training Accuracy: 0.7834, Validation Accuracy: 0.7181\n",
      "Epoch 130/300, Test Loss: 0.4592, Test Accuracy: 0.8067\n",
      "Epoch 140/300, Training Loss: 0.3985, Validation Loss: 0.5447, Training Accuracy: 0.8164, Validation Accuracy: 0.7517\n",
      "Epoch 140/300, Test Loss: 0.4899, Test Accuracy: 0.7733\n",
      "Epoch 150/300, Training Loss: 0.4134, Validation Loss: 0.5905, Training Accuracy: 0.8020, Validation Accuracy: 0.7315\n",
      "Epoch 150/300, Test Loss: 0.4959, Test Accuracy: 0.7800\n",
      "Epoch 160/300, Training Loss: 0.4149, Validation Loss: 0.5969, Training Accuracy: 0.8077, Validation Accuracy: 0.7315\n",
      "Epoch 160/300, Test Loss: 0.4897, Test Accuracy: 0.7733\n",
      "Epoch 170/300, Training Loss: 0.4087, Validation Loss: 0.5987, Training Accuracy: 0.8149, Validation Accuracy: 0.7181\n",
      "Epoch 170/300, Test Loss: 0.4693, Test Accuracy: 0.7867\n",
      "Epoch 180/300, Training Loss: 0.3935, Validation Loss: 0.5689, Training Accuracy: 0.8178, Validation Accuracy: 0.7248\n",
      "Epoch 180/300, Test Loss: 0.4624, Test Accuracy: 0.7933\n",
      "Epoch 190/300, Training Loss: 0.3972, Validation Loss: 0.5575, Training Accuracy: 0.8049, Validation Accuracy: 0.7315\n",
      "Epoch 190/300, Test Loss: 0.4793, Test Accuracy: 0.7800\n",
      "Epoch 200/300, Training Loss: 0.4072, Validation Loss: 0.5459, Training Accuracy: 0.8049, Validation Accuracy: 0.7584\n",
      "Epoch 200/300, Test Loss: 0.4582, Test Accuracy: 0.8000\n",
      "Epoch 210/300, Training Loss: 0.4019, Validation Loss: 0.5744, Training Accuracy: 0.7963, Validation Accuracy: 0.7248\n",
      "Epoch 210/300, Test Loss: 0.4830, Test Accuracy: 0.7933\n",
      "Epoch 220/300, Training Loss: 0.3791, Validation Loss: 0.5785, Training Accuracy: 0.8364, Validation Accuracy: 0.7383\n",
      "Epoch 220/300, Test Loss: 0.4675, Test Accuracy: 0.8000\n",
      "Epoch 230/300, Training Loss: 0.3767, Validation Loss: 0.5839, Training Accuracy: 0.8121, Validation Accuracy: 0.7383\n",
      "Epoch 230/300, Test Loss: 0.4575, Test Accuracy: 0.8067\n",
      "Epoch 240/300, Training Loss: 0.4273, Validation Loss: 0.5670, Training Accuracy: 0.8063, Validation Accuracy: 0.7517\n",
      "Epoch 240/300, Test Loss: 0.4671, Test Accuracy: 0.7867\n",
      "Epoch 250/300, Training Loss: 0.3643, Validation Loss: 0.5512, Training Accuracy: 0.8321, Validation Accuracy: 0.7584\n",
      "Epoch 250/300, Test Loss: 0.4497, Test Accuracy: 0.8067\n",
      "Epoch 260/300, Training Loss: 0.3646, Validation Loss: 0.5733, Training Accuracy: 0.8522, Validation Accuracy: 0.7181\n",
      "Epoch 260/300, Test Loss: 0.4474, Test Accuracy: 0.7800\n",
      "Epoch 270/300, Training Loss: 0.3889, Validation Loss: 0.5613, Training Accuracy: 0.8221, Validation Accuracy: 0.7181\n",
      "Epoch 270/300, Test Loss: 0.4832, Test Accuracy: 0.7733\n",
      "Epoch 280/300, Training Loss: 0.3808, Validation Loss: 0.5675, Training Accuracy: 0.8235, Validation Accuracy: 0.7517\n",
      "Epoch 280/300, Test Loss: 0.4849, Test Accuracy: 0.7667\n",
      "Epoch 290/300, Training Loss: 0.3683, Validation Loss: 0.5606, Training Accuracy: 0.8321, Validation Accuracy: 0.7785\n",
      "Epoch 290/300, Test Loss: 0.4533, Test Accuracy: 0.8000\n",
      "Epoch 300/300, Training Loss: 0.3705, Validation Loss: 0.5983, Training Accuracy: 0.8393, Validation Accuracy: 0.7450\n",
      "Epoch 300/300, Test Loss: 0.4898, Test Accuracy: 0.7867\n",
      "Training completed in: 0.59 minutes\n",
      "Test Loss: 0.4898, Test Accuracy: 0.7867\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.7867\n",
      "Test Precision: 0.7532\n",
      "Test Recall: 0.8169\n",
      "Test F1 Score: 0.7838\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchnorm(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_accuracies_improved = []\n",
    "start_time_improved = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    test_accuracies_improved.append(test_accuracy)  # Store test accuracy for this epoch\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "end_time_improved = time.time()\n",
    "print(f\"Training completed in: {(end_time_improved - start_time_improved) / 60:.2f} minutes\")\n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracy = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Learning Rate: {learning}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6980, Validation Loss: 0.6880, Training Accuracy: 0.5194, Validation Accuracy: 0.6040\n",
      "Epoch 20/300, Training Loss: 0.6808, Validation Loss: 0.6791, Training Accuracy: 0.5782, Validation Accuracy: 0.6040\n",
      "Epoch 30/300, Training Loss: 0.6721, Validation Loss: 0.6756, Training Accuracy: 0.5854, Validation Accuracy: 0.6644\n",
      "Epoch 40/300, Training Loss: 0.6745, Validation Loss: 0.6546, Training Accuracy: 0.5940, Validation Accuracy: 0.6779\n",
      "Epoch 50/300, Training Loss: 0.6538, Validation Loss: 0.6511, Training Accuracy: 0.6026, Validation Accuracy: 0.6846\n",
      "Epoch 60/300, Training Loss: 0.6390, Validation Loss: 0.6440, Training Accuracy: 0.6370, Validation Accuracy: 0.6711\n",
      "Epoch 70/300, Training Loss: 0.6602, Validation Loss: 0.6310, Training Accuracy: 0.6370, Validation Accuracy: 0.6980\n",
      "Epoch 80/300, Training Loss: 0.6424, Validation Loss: 0.6250, Training Accuracy: 0.6485, Validation Accuracy: 0.6577\n",
      "Epoch 90/300, Training Loss: 0.6307, Validation Loss: 0.6316, Training Accuracy: 0.6528, Validation Accuracy: 0.6913\n",
      "Epoch 100/300, Training Loss: 0.6342, Validation Loss: 0.6189, Training Accuracy: 0.6557, Validation Accuracy: 0.6846\n",
      "Epoch 110/300, Training Loss: 0.6351, Validation Loss: 0.6077, Training Accuracy: 0.6341, Validation Accuracy: 0.7248\n",
      "Epoch 120/300, Training Loss: 0.6307, Validation Loss: 0.6031, Training Accuracy: 0.6456, Validation Accuracy: 0.7181\n",
      "Epoch 130/300, Training Loss: 0.6484, Validation Loss: 0.6104, Training Accuracy: 0.6227, Validation Accuracy: 0.7047\n",
      "Epoch 140/300, Training Loss: 0.6090, Validation Loss: 0.6080, Training Accuracy: 0.6858, Validation Accuracy: 0.6846\n",
      "Epoch 150/300, Training Loss: 0.6331, Validation Loss: 0.6132, Training Accuracy: 0.6528, Validation Accuracy: 0.6376\n",
      "Epoch 160/300, Training Loss: 0.6178, Validation Loss: 0.6060, Training Accuracy: 0.6872, Validation Accuracy: 0.6711\n",
      "Epoch 170/300, Training Loss: 0.6212, Validation Loss: 0.5953, Training Accuracy: 0.6585, Validation Accuracy: 0.7181\n",
      "Epoch 180/300, Training Loss: 0.6010, Validation Loss: 0.5880, Training Accuracy: 0.6758, Validation Accuracy: 0.7181\n",
      "Epoch 190/300, Training Loss: 0.6310, Validation Loss: 0.5858, Training Accuracy: 0.6700, Validation Accuracy: 0.7248\n",
      "Epoch 200/300, Training Loss: 0.6176, Validation Loss: 0.5920, Training Accuracy: 0.6729, Validation Accuracy: 0.7114\n",
      "Epoch 210/300, Training Loss: 0.6025, Validation Loss: 0.5897, Training Accuracy: 0.6844, Validation Accuracy: 0.6644\n",
      "Epoch 220/300, Training Loss: 0.6004, Validation Loss: 0.5766, Training Accuracy: 0.6987, Validation Accuracy: 0.7114\n",
      "Epoch 230/300, Training Loss: 0.5980, Validation Loss: 0.5822, Training Accuracy: 0.6815, Validation Accuracy: 0.6980\n",
      "Epoch 240/300, Training Loss: 0.5981, Validation Loss: 0.5766, Training Accuracy: 0.6887, Validation Accuracy: 0.7315\n",
      "Epoch 250/300, Training Loss: 0.5834, Validation Loss: 0.5754, Training Accuracy: 0.6958, Validation Accuracy: 0.7315\n",
      "Epoch 260/300, Training Loss: 0.6009, Validation Loss: 0.5793, Training Accuracy: 0.6872, Validation Accuracy: 0.7315\n",
      "Epoch 270/300, Training Loss: 0.6177, Validation Loss: 0.5738, Training Accuracy: 0.6829, Validation Accuracy: 0.7383\n",
      "Epoch 280/300, Training Loss: 0.5966, Validation Loss: 0.5828, Training Accuracy: 0.6872, Validation Accuracy: 0.6980\n",
      "Epoch 290/300, Training Loss: 0.5951, Validation Loss: 0.5778, Training Accuracy: 0.6915, Validation Accuracy: 0.7315\n",
      "Epoch 300/300, Training Loss: 0.5706, Validation Loss: 0.5743, Training Accuracy: 0.7030, Validation Accuracy: 0.7047\n",
      "Test Loss: 0.5895, Test Accuracy: 0.6867\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.6867\n",
      "Test Precision: 0.7069\n",
      "Test Recall: 0.5775\n",
      "Test F1 Score: 0.6357\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "accumulation_steps = 4  # Number of batches over which gradients will be accumulated\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    optimizer.zero_grad()  # Reset the gradients before accumulating\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        \n",
    "        # Scale loss for accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()  # Accumulate the gradients\n",
    "        \n",
    "        # Accumulate gradients and perform optimizer step every 'accumulation_steps' batches\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()  # Update the weights\n",
    "            optimizer.zero_grad()  # Reset the gradients for the next set of accumulation steps\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0) * accumulation_steps  # Adjust the loss accumulation\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Learning Rate: {learning}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_batchnorm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN_batchnorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)  # Add BatchNorm\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(self.bn1(x))  # Apply BatchNorm\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(self.bn2(x))  # Apply BatchNorm\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(self.bn3(x))  # Apply BatchNorm\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Replace the model with the one using batch normalization\n",
    "model = NN_batchnorm(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchnorm(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_accuracies_improved = []\n",
    "start_time_improved = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    # Save the model if validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pkl')\n",
    "    \n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses = test_losses.append(test_loss)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    test_accuracies_improved.append(test_accuracy)  # Store test accuracy for this epoch\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "end_time_improved = time.time()\n",
    "print(f\"Training completed in: {(end_time_improved - start_time_improved) / 60:.2f} minutes\")\n",
    "model.load_state_dict(torch.load('best_model.pkl'))\n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Learning Rate: {learning}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test accuracy data for the two models (you should replace these with your actual values)\n",
    "# Assuming the accuracies are stored in test_accuracies_base and test_accuracies_improved lists\n",
    "epochs_base = len(test_accuracies_base_model)  # Number of epochs run by the base model\n",
    "epochs_improved = len(test_accuracies_improved)  # Number of epochs run by the improved model\n",
    "\n",
    "# Assuming test_accuracies are for the first (base) model and test_accuracies_improved for the second (improved) model\n",
    "epoch_range_base = np.arange(1, epochs_base + 1)\n",
    "epoch_range_improved = np.arange(1, epochs_improved + 1)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot test accuracy for the base model\n",
    "plt.plot(epoch_range_base, test_accuracies_base_model, label='Base Model Test Accuracy', color='b', linestyle='--', marker='o')\n",
    "\n",
    "# Plot test accuracy for the improved model\n",
    "plt.plot(epoch_range_improved, test_accuracies_improved, label='Improved Model Test Accuracy', color='g', linestyle='-', marker='x')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Test Accuracy Comparison: Base Model vs Improved Model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Accuracy')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training, validation, and test accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_epochs), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(num_epochs), val_accuracies, label='Validation Accuracy')\n",
    "plt.plot(range(num_epochs), test_accuracies_improved, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training, Validation, and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training, validation, and test loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_epochs), train_losses, label='Training Loss')\n",
    "plt.plot(range(num_epochs), val_losses, label='Validation Loss')\n",
    "plt.plot(range(num_epochs), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training, Validation, and Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "y_true_tensor = torch.tensor(y_true)\n",
    "y_pred_tensor = torch.tensor(y_pred).float()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true_tensor, y_pred_tensor)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
