{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part - 2:\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               f3      target\n",
      "count  766.000000  766.000000\n",
      "mean    69.118799    0.349869\n",
      "std     19.376901    0.477240\n",
      "min      0.000000    0.000000\n",
      "25%     62.500000    0.000000\n",
      "50%     72.000000    0.000000\n",
      "75%     80.000000    1.000000\n",
      "max    122.000000    1.000000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of             f1     f2  f3         f4          f5        f6        f7  target\n",
       "0     6.000000  148.0  72  35.000000    0.000000  33.60000  0.627000       1\n",
       "1     1.000000   85.0  66  29.000000    0.000000  26.60000  0.351000       0\n",
       "2     8.000000  183.0  64   0.000000    0.000000  23.30000  0.672000       1\n",
       "3     1.000000   89.0  66  23.000000   94.000000  28.10000  0.167000       0\n",
       "4     0.000000  137.0  40  35.000000  168.000000  43.10000  2.288000       1\n",
       "..         ...    ...  ..        ...         ...       ...       ...     ...\n",
       "761   9.000000   89.0  62   0.000000    0.000000  22.50000  0.472128       0\n",
       "762  10.000000  101.0  76  48.000000  180.000000  31.99817  0.171000       0\n",
       "763   2.000000  122.0  70  27.000000   80.091503  36.80000  0.340000       0\n",
       "764   3.849673  121.0  72  23.000000  112.000000  26.20000  0.245000       0\n",
       "765   1.000000  126.0  60  20.542484    0.000000  30.10000  0.349000       1\n",
       "\n",
       "[766 rows x 8 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that there are alphabets in numeric columns so we replace them with mean of that respective column\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "df.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                f1        f2        f3            f4        f5            f6  \\\n",
       "0    6.382139e-01  0.849057  0.148790  9.070150e-01 -0.695096  2.030729e-01   \n",
       "1   -8.457789e-01 -1.125480 -0.161060  5.305956e-01 -0.695096 -6.843559e-01   \n",
       "2    1.231811e+00  1.946022 -0.264343 -1.288765e+00 -0.695096 -1.102715e+00   \n",
       "3   -8.457789e-01 -1.000113 -0.161060  1.541762e-01  0.120709 -4.941926e-01   \n",
       "4   -1.142577e+00  0.504297 -1.503740  9.070150e-01  0.762938  1.407440e+00   \n",
       "..            ...       ...       ...           ...       ...           ...   \n",
       "761  1.528610e+00 -1.000113 -0.367626 -1.288765e+00 -0.695096 -1.204136e+00   \n",
       "762  1.825408e+00 -0.624010  0.355356  1.722590e+00  0.867083  4.503972e-16   \n",
       "763 -5.489804e-01  0.034169  0.045507  4.051224e-01  0.000000  6.087546e-01   \n",
       "764  1.318050e-16  0.002827  0.148790  1.541762e-01  0.276927 -7.350661e-01   \n",
       "765 -8.457789e-01  0.159536 -0.470909  2.228851e-16 -0.695096 -2.406415e-01   \n",
       "\n",
       "               f7  target  \n",
       "0    4.677335e-01       1  \n",
       "1   -3.658228e-01       0  \n",
       "2    6.036394e-01       1  \n",
       "3   -9.215270e-01       0  \n",
       "4    5.484172e+00       1  \n",
       "..            ...     ...  \n",
       "761  1.676510e-16       0  \n",
       "762 -9.094465e-01       0  \n",
       "763 -3.990443e-01       0  \n",
       "764 -6.859567e-01       0  \n",
       "765 -3.718631e-01       1  \n",
       "\n",
       "[766 rows x 8 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df1 = df.drop('target', axis=1)\n",
    "column = df1.columns\n",
    "df1[column] = scaler.fit_transform(df1[column])\n",
    "df1['target'] = df['target']\n",
    "df1.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  torch.Size([697, 7])\n",
      "Y_train size:  torch.Size([697])\n",
      "X_val size:  torch.Size([149, 7])\n",
      "Y_val size:  torch.Size([149])\n",
      "X_test size:  torch.Size([150, 7])\n",
      "Y_test size:  torch.Size([150])\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('target', axis=1)  # Replace 'target_column' with the actual target column name\n",
    "Y = df['target']\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X, Y = ros.fit_resample(X, Y)\n",
    "# Split the data into training and testing sets (80% train, 20% test by default)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.175, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "print(\"X_train size: \", X_train_tensor.shape)\n",
    "print(\"Y_train size: \", Y_train_tensor.shape)\n",
    "print(\"X_val size: \", X_val_tensor.shape)\n",
    "print(\"Y_val size: \", Y_val_tensor.shape)\n",
    "print(\"X_test size: \", X_test_tensor.shape)\n",
    "print(\"Y_test size: \", Y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning hyperparameters:\n",
    "=======================\n",
    "1. Dropout value:\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_dropout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(NN_dropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6724, Validation Loss: 0.6603, Training Accuracy: 0.6011, Validation Accuracy: 0.6107\n",
      "Epoch 20/300, Training Loss: 0.6377, Validation Loss: 0.6133, Training Accuracy: 0.6485, Validation Accuracy: 0.6913\n",
      "Epoch 30/300, Training Loss: 0.6252, Validation Loss: 0.5925, Training Accuracy: 0.6657, Validation Accuracy: 0.6980\n",
      "Epoch 40/300, Training Loss: 0.6042, Validation Loss: 0.5887, Training Accuracy: 0.6915, Validation Accuracy: 0.7114\n",
      "Epoch 50/300, Training Loss: 0.5996, Validation Loss: 0.5778, Training Accuracy: 0.6844, Validation Accuracy: 0.7383\n",
      "Epoch 60/300, Training Loss: 0.5896, Validation Loss: 0.5655, Training Accuracy: 0.6987, Validation Accuracy: 0.7383\n",
      "Epoch 70/300, Training Loss: 0.5806, Validation Loss: 0.5726, Training Accuracy: 0.7188, Validation Accuracy: 0.7450\n",
      "Epoch 80/300, Training Loss: 0.6052, Validation Loss: 0.5611, Training Accuracy: 0.6743, Validation Accuracy: 0.6980\n",
      "Epoch 90/300, Training Loss: 0.5718, Validation Loss: 0.5914, Training Accuracy: 0.6944, Validation Accuracy: 0.6376\n",
      "Epoch 100/300, Training Loss: 0.5777, Validation Loss: 0.5603, Training Accuracy: 0.7073, Validation Accuracy: 0.6913\n",
      "Epoch 110/300, Training Loss: 0.5685, Validation Loss: 0.5579, Training Accuracy: 0.7059, Validation Accuracy: 0.7450\n",
      "Epoch 120/300, Training Loss: 0.5868, Validation Loss: 0.5862, Training Accuracy: 0.6844, Validation Accuracy: 0.6980\n",
      "Epoch 130/300, Training Loss: 0.5765, Validation Loss: 0.5703, Training Accuracy: 0.7059, Validation Accuracy: 0.7114\n",
      "Epoch 140/300, Training Loss: 0.5837, Validation Loss: 0.5428, Training Accuracy: 0.7188, Validation Accuracy: 0.7315\n",
      "Epoch 150/300, Training Loss: 0.5861, Validation Loss: 0.5694, Training Accuracy: 0.6987, Validation Accuracy: 0.7383\n",
      "Epoch 160/300, Training Loss: 0.5489, Validation Loss: 0.5433, Training Accuracy: 0.7317, Validation Accuracy: 0.7450\n",
      "Epoch 170/300, Training Loss: 0.5592, Validation Loss: 0.5612, Training Accuracy: 0.7131, Validation Accuracy: 0.7383\n",
      "Epoch 180/300, Training Loss: 0.5624, Validation Loss: 0.5483, Training Accuracy: 0.7389, Validation Accuracy: 0.7383\n",
      "Epoch 190/300, Training Loss: 0.5657, Validation Loss: 0.5523, Training Accuracy: 0.7116, Validation Accuracy: 0.7517\n",
      "Epoch 200/300, Training Loss: 0.5881, Validation Loss: 0.5677, Training Accuracy: 0.6829, Validation Accuracy: 0.7114\n",
      "Epoch 210/300, Training Loss: 0.5804, Validation Loss: 0.5613, Training Accuracy: 0.7116, Validation Accuracy: 0.7248\n",
      "Epoch 220/300, Training Loss: 0.5623, Validation Loss: 0.5380, Training Accuracy: 0.7102, Validation Accuracy: 0.7651\n",
      "Epoch 230/300, Training Loss: 0.5487, Validation Loss: 0.5737, Training Accuracy: 0.7217, Validation Accuracy: 0.6913\n",
      "Epoch 240/300, Training Loss: 0.5677, Validation Loss: 0.5686, Training Accuracy: 0.7001, Validation Accuracy: 0.7181\n",
      "Epoch 250/300, Training Loss: 0.5736, Validation Loss: 0.5605, Training Accuracy: 0.7231, Validation Accuracy: 0.7114\n",
      "Epoch 260/300, Training Loss: 0.5547, Validation Loss: 0.5587, Training Accuracy: 0.7274, Validation Accuracy: 0.7181\n",
      "Epoch 270/300, Training Loss: 0.5573, Validation Loss: 0.5365, Training Accuracy: 0.7217, Validation Accuracy: 0.7383\n",
      "Epoch 280/300, Training Loss: 0.5658, Validation Loss: 0.5695, Training Accuracy: 0.7260, Validation Accuracy: 0.7181\n",
      "Epoch 290/300, Training Loss: 0.5477, Validation Loss: 0.5525, Training Accuracy: 0.7245, Validation Accuracy: 0.7114\n",
      "Epoch 300/300, Training Loss: 0.5208, Validation Loss: 0.5541, Training Accuracy: 0.7547, Validation Accuracy: 0.7047\n",
      "Test Loss: 0.5418, Test Accuracy: 0.7067\n",
      "Dropout: 0.3\n",
      "Test Accuracy: 0.7067\n",
      "Test Precision: 0.6484\n",
      "Test Recall: 0.8310\n",
      "Test F1 Score: 0.7284\n",
      "Epoch 10/300, Training Loss: 0.6899, Validation Loss: 0.6799, Training Accuracy: 0.5638, Validation Accuracy: 0.5503\n",
      "Epoch 20/300, Training Loss: 0.6724, Validation Loss: 0.6686, Training Accuracy: 0.6069, Validation Accuracy: 0.5503\n",
      "Epoch 30/300, Training Loss: 0.6569, Validation Loss: 0.6326, Training Accuracy: 0.6227, Validation Accuracy: 0.6980\n",
      "Epoch 40/300, Training Loss: 0.6441, Validation Loss: 0.6285, Training Accuracy: 0.6169, Validation Accuracy: 0.6711\n",
      "Epoch 50/300, Training Loss: 0.6386, Validation Loss: 0.6288, Training Accuracy: 0.6743, Validation Accuracy: 0.6846\n",
      "Epoch 60/300, Training Loss: 0.6332, Validation Loss: 0.6063, Training Accuracy: 0.6499, Validation Accuracy: 0.7181\n",
      "Epoch 70/300, Training Loss: 0.6512, Validation Loss: 0.6258, Training Accuracy: 0.6385, Validation Accuracy: 0.6309\n",
      "Epoch 80/300, Training Loss: 0.6326, Validation Loss: 0.5821, Training Accuracy: 0.6514, Validation Accuracy: 0.6913\n",
      "Epoch 90/300, Training Loss: 0.6349, Validation Loss: 0.6001, Training Accuracy: 0.6442, Validation Accuracy: 0.6779\n",
      "Epoch 100/300, Training Loss: 0.6070, Validation Loss: 0.5709, Training Accuracy: 0.6786, Validation Accuracy: 0.7248\n",
      "Epoch 110/300, Training Loss: 0.6067, Validation Loss: 0.5685, Training Accuracy: 0.6858, Validation Accuracy: 0.7315\n",
      "Epoch 120/300, Training Loss: 0.5945, Validation Loss: 0.5734, Training Accuracy: 0.6958, Validation Accuracy: 0.7114\n",
      "Epoch 130/300, Training Loss: 0.6315, Validation Loss: 0.5572, Training Accuracy: 0.6585, Validation Accuracy: 0.7517\n",
      "Epoch 140/300, Training Loss: 0.6392, Validation Loss: 0.5858, Training Accuracy: 0.6227, Validation Accuracy: 0.6846\n",
      "Epoch 150/300, Training Loss: 0.5747, Validation Loss: 0.5589, Training Accuracy: 0.7159, Validation Accuracy: 0.7383\n",
      "Epoch 160/300, Training Loss: 0.5948, Validation Loss: 0.5458, Training Accuracy: 0.6958, Validation Accuracy: 0.7383\n",
      "Epoch 170/300, Training Loss: 0.6211, Validation Loss: 0.5628, Training Accuracy: 0.6729, Validation Accuracy: 0.7114\n",
      "Epoch 180/300, Training Loss: 0.5849, Validation Loss: 0.5407, Training Accuracy: 0.7088, Validation Accuracy: 0.7383\n",
      "Epoch 190/300, Training Loss: 0.6093, Validation Loss: 0.5587, Training Accuracy: 0.6872, Validation Accuracy: 0.7248\n",
      "Epoch 200/300, Training Loss: 0.5914, Validation Loss: 0.5476, Training Accuracy: 0.6872, Validation Accuracy: 0.7248\n",
      "Epoch 210/300, Training Loss: 0.5961, Validation Loss: 0.5532, Training Accuracy: 0.6643, Validation Accuracy: 0.7114\n",
      "Epoch 220/300, Training Loss: 0.6132, Validation Loss: 0.5437, Training Accuracy: 0.6844, Validation Accuracy: 0.6980\n",
      "Epoch 230/300, Training Loss: 0.5866, Validation Loss: 0.5627, Training Accuracy: 0.6772, Validation Accuracy: 0.7047\n",
      "Epoch 240/300, Training Loss: 0.5837, Validation Loss: 0.5275, Training Accuracy: 0.6872, Validation Accuracy: 0.7383\n",
      "Epoch 250/300, Training Loss: 0.6128, Validation Loss: 0.6299, Training Accuracy: 0.6686, Validation Accuracy: 0.6309\n",
      "Epoch 260/300, Training Loss: 0.5838, Validation Loss: 0.5356, Training Accuracy: 0.6973, Validation Accuracy: 0.7248\n",
      "Epoch 270/300, Training Loss: 0.5763, Validation Loss: 0.5341, Training Accuracy: 0.7288, Validation Accuracy: 0.7248\n",
      "Epoch 280/300, Training Loss: 0.5925, Validation Loss: 0.5455, Training Accuracy: 0.7016, Validation Accuracy: 0.7047\n",
      "Epoch 290/300, Training Loss: 0.5752, Validation Loss: 0.5305, Training Accuracy: 0.7159, Validation Accuracy: 0.7181\n",
      "Epoch 300/300, Training Loss: 0.5680, Validation Loss: 0.5276, Training Accuracy: 0.6915, Validation Accuracy: 0.7248\n",
      "Test Loss: 0.5314, Test Accuracy: 0.7933\n",
      "Dropout: 0.5\n",
      "Test Accuracy: 0.7933\n",
      "Test Precision: 0.7564\n",
      "Test Recall: 0.8310\n",
      "Test F1 Score: 0.7919\n",
      "Epoch 10/300, Training Loss: 0.6885, Validation Loss: 0.6867, Training Accuracy: 0.5567, Validation Accuracy: 0.5302\n",
      "Epoch 20/300, Training Loss: 0.6992, Validation Loss: 0.6860, Training Accuracy: 0.5768, Validation Accuracy: 0.5235\n",
      "Epoch 30/300, Training Loss: 0.6918, Validation Loss: 0.6800, Training Accuracy: 0.5739, Validation Accuracy: 0.5570\n",
      "Epoch 40/300, Training Loss: 0.6732, Validation Loss: 0.6759, Training Accuracy: 0.6184, Validation Accuracy: 0.5638\n",
      "Epoch 50/300, Training Loss: 0.6886, Validation Loss: 0.6780, Training Accuracy: 0.5954, Validation Accuracy: 0.5705\n",
      "Epoch 60/300, Training Loss: 0.6615, Validation Loss: 0.6784, Training Accuracy: 0.6184, Validation Accuracy: 0.5503\n",
      "Epoch 70/300, Training Loss: 0.6844, Validation Loss: 0.6845, Training Accuracy: 0.5653, Validation Accuracy: 0.5302\n",
      "Epoch 80/300, Training Loss: 0.7122, Validation Loss: 0.6852, Training Accuracy: 0.5538, Validation Accuracy: 0.5369\n",
      "Epoch 90/300, Training Loss: 0.6882, Validation Loss: 0.6840, Training Accuracy: 0.5653, Validation Accuracy: 0.5503\n",
      "Epoch 100/300, Training Loss: 0.6877, Validation Loss: 0.6868, Training Accuracy: 0.5825, Validation Accuracy: 0.5302\n",
      "Epoch 110/300, Training Loss: 0.6834, Validation Loss: 0.6774, Training Accuracy: 0.5452, Validation Accuracy: 0.5638\n",
      "Epoch 120/300, Training Loss: 0.6730, Validation Loss: 0.6826, Training Accuracy: 0.5825, Validation Accuracy: 0.5436\n",
      "Epoch 130/300, Training Loss: 0.6728, Validation Loss: 0.6826, Training Accuracy: 0.6083, Validation Accuracy: 0.5436\n",
      "Epoch 140/300, Training Loss: 0.6863, Validation Loss: 0.6906, Training Accuracy: 0.5739, Validation Accuracy: 0.5235\n",
      "Epoch 150/300, Training Loss: 0.6833, Validation Loss: 0.6815, Training Accuracy: 0.5911, Validation Accuracy: 0.5436\n",
      "Epoch 160/300, Training Loss: 0.6866, Validation Loss: 0.6803, Training Accuracy: 0.5882, Validation Accuracy: 0.5772\n",
      "Epoch 170/300, Training Loss: 0.6802, Validation Loss: 0.6831, Training Accuracy: 0.5811, Validation Accuracy: 0.5369\n",
      "Epoch 180/300, Training Loss: 0.6678, Validation Loss: 0.6673, Training Accuracy: 0.6241, Validation Accuracy: 0.6107\n",
      "Epoch 190/300, Training Loss: 0.6813, Validation Loss: 0.6873, Training Accuracy: 0.5811, Validation Accuracy: 0.5235\n",
      "Epoch 200/300, Training Loss: 0.6746, Validation Loss: 0.6709, Training Accuracy: 0.5825, Validation Accuracy: 0.5772\n",
      "Epoch 210/300, Training Loss: 0.6808, Validation Loss: 0.6692, Training Accuracy: 0.6198, Validation Accuracy: 0.5973\n",
      "Epoch 220/300, Training Loss: 0.6617, Validation Loss: 0.6654, Training Accuracy: 0.6212, Validation Accuracy: 0.6040\n",
      "Epoch 230/300, Training Loss: 0.6575, Validation Loss: 0.6714, Training Accuracy: 0.6141, Validation Accuracy: 0.5772\n",
      "Epoch 240/300, Training Loss: 0.6801, Validation Loss: 0.6729, Training Accuracy: 0.5768, Validation Accuracy: 0.5839\n",
      "Epoch 250/300, Training Loss: 0.6825, Validation Loss: 0.6735, Training Accuracy: 0.6141, Validation Accuracy: 0.5705\n",
      "Epoch 260/300, Training Loss: 0.6641, Validation Loss: 0.6769, Training Accuracy: 0.5911, Validation Accuracy: 0.5436\n",
      "Epoch 270/300, Training Loss: 0.6547, Validation Loss: 0.6591, Training Accuracy: 0.6327, Validation Accuracy: 0.6242\n",
      "Epoch 280/300, Training Loss: 0.6670, Validation Loss: 0.6678, Training Accuracy: 0.6040, Validation Accuracy: 0.5839\n",
      "Epoch 290/300, Training Loss: 0.6627, Validation Loss: 0.6740, Training Accuracy: 0.6341, Validation Accuracy: 0.5772\n",
      "Epoch 300/300, Training Loss: 0.6641, Validation Loss: 0.6613, Training Accuracy: 0.6499, Validation Accuracy: 0.5906\n",
      "Test Loss: 0.6552, Test Accuracy: 0.6133\n",
      "Dropout: 0.7\n",
      "Test Accuracy: 0.6133\n",
      "Test Precision: 0.9333\n",
      "Test Recall: 0.1972\n",
      "Test F1 Score: 0.3256\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "dropout_rate = [0.3, 0.5, 0.7]\n",
    "for dropout in dropout_rate:\n",
    "    model = NN_dropout(input_size, hidden_size, output_size, dropout)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "\n",
    "    num_epochs = 300\n",
    "    batch_size = 64\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "                val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "                correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluation on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_losses = [test_loss] * num_epochs\n",
    "    test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"Dropout: {dropout}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Batch size:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_batchsize_lr(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN_batchsize_lr, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        print(self.dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6830, Validation Loss: 0.6587, Training Accuracy: 0.5739, Validation Accuracy: 0.6242\n",
      "Epoch 20/300, Training Loss: 0.6702, Validation Loss: 0.6396, Training Accuracy: 0.6198, Validation Accuracy: 0.7047\n",
      "Epoch 30/300, Training Loss: 0.6781, Validation Loss: 0.6266, Training Accuracy: 0.5753, Validation Accuracy: 0.6510\n",
      "Epoch 40/300, Training Loss: 0.6706, Validation Loss: 0.6400, Training Accuracy: 0.6098, Validation Accuracy: 0.6174\n",
      "Epoch 50/300, Training Loss: 0.6498, Validation Loss: 0.6358, Training Accuracy: 0.6284, Validation Accuracy: 0.6376\n",
      "Epoch 60/300, Training Loss: 0.6950, Validation Loss: 0.6422, Training Accuracy: 0.5653, Validation Accuracy: 0.5973\n",
      "Epoch 70/300, Training Loss: 0.6578, Validation Loss: 0.6333, Training Accuracy: 0.6313, Validation Accuracy: 0.6711\n",
      "Epoch 80/300, Training Loss: 0.6570, Validation Loss: 0.6275, Training Accuracy: 0.6241, Validation Accuracy: 0.6711\n",
      "Epoch 90/300, Training Loss: 0.6512, Validation Loss: 0.6050, Training Accuracy: 0.6341, Validation Accuracy: 0.6711\n",
      "Epoch 100/300, Training Loss: 0.6407, Validation Loss: 0.6121, Training Accuracy: 0.6356, Validation Accuracy: 0.6711\n",
      "Epoch 110/300, Training Loss: 0.6367, Validation Loss: 0.5932, Training Accuracy: 0.6471, Validation Accuracy: 0.6577\n",
      "Epoch 120/300, Training Loss: 0.6189, Validation Loss: 0.5698, Training Accuracy: 0.6772, Validation Accuracy: 0.7181\n",
      "Epoch 130/300, Training Loss: 0.6052, Validation Loss: 0.5452, Training Accuracy: 0.6801, Validation Accuracy: 0.7315\n",
      "Epoch 140/300, Training Loss: 0.6032, Validation Loss: 0.5596, Training Accuracy: 0.6987, Validation Accuracy: 0.6980\n",
      "Epoch 150/300, Training Loss: 0.6134, Validation Loss: 0.5678, Training Accuracy: 0.6801, Validation Accuracy: 0.6913\n",
      "Epoch 160/300, Training Loss: 0.6268, Validation Loss: 0.5941, Training Accuracy: 0.6729, Validation Accuracy: 0.6510\n",
      "Epoch 170/300, Training Loss: 0.6121, Validation Loss: 0.5833, Training Accuracy: 0.6542, Validation Accuracy: 0.6510\n",
      "Epoch 180/300, Training Loss: 0.5966, Validation Loss: 0.5526, Training Accuracy: 0.7059, Validation Accuracy: 0.7181\n",
      "Epoch 190/300, Training Loss: 0.6100, Validation Loss: 0.5732, Training Accuracy: 0.7145, Validation Accuracy: 0.7047\n",
      "Epoch 200/300, Training Loss: 0.5983, Validation Loss: 0.6061, Training Accuracy: 0.6772, Validation Accuracy: 0.6846\n",
      "Epoch 210/300, Training Loss: 0.6121, Validation Loss: 0.5695, Training Accuracy: 0.6887, Validation Accuracy: 0.6913\n",
      "Epoch 220/300, Training Loss: 0.6138, Validation Loss: 0.5568, Training Accuracy: 0.6872, Validation Accuracy: 0.7315\n",
      "Epoch 230/300, Training Loss: 0.6262, Validation Loss: 0.5797, Training Accuracy: 0.6743, Validation Accuracy: 0.6779\n",
      "Epoch 240/300, Training Loss: 0.6395, Validation Loss: 0.5510, Training Accuracy: 0.6772, Validation Accuracy: 0.7517\n",
      "Epoch 250/300, Training Loss: 0.7776, Validation Loss: 0.5777, Training Accuracy: 0.6671, Validation Accuracy: 0.6846\n",
      "Epoch 260/300, Training Loss: 0.5851, Validation Loss: 0.5780, Training Accuracy: 0.6915, Validation Accuracy: 0.6846\n",
      "Epoch 270/300, Training Loss: 0.6098, Validation Loss: 0.5504, Training Accuracy: 0.7088, Validation Accuracy: 0.7383\n",
      "Epoch 280/300, Training Loss: 0.5882, Validation Loss: 0.5427, Training Accuracy: 0.6944, Validation Accuracy: 0.7517\n",
      "Epoch 290/300, Training Loss: 0.6138, Validation Loss: 0.5489, Training Accuracy: 0.6829, Validation Accuracy: 0.7383\n",
      "Epoch 300/300, Training Loss: 0.5973, Validation Loss: 0.5438, Training Accuracy: 0.6944, Validation Accuracy: 0.7383\n",
      "Test Loss: 0.5326, Test Accuracy: 0.7533\n",
      "Batch size: 32\n",
      "Test Accuracy: 0.7533\n",
      "Test Precision: 0.7931\n",
      "Test Recall: 0.6479\n",
      "Test F1 Score: 0.7132\n",
      "Epoch 10/300, Training Loss: 0.5749, Validation Loss: 0.5396, Training Accuracy: 0.7145, Validation Accuracy: 0.7315\n",
      "Epoch 20/300, Training Loss: 0.5991, Validation Loss: 0.5499, Training Accuracy: 0.6786, Validation Accuracy: 0.7517\n",
      "Epoch 30/300, Training Loss: 0.6075, Validation Loss: 0.5571, Training Accuracy: 0.6786, Validation Accuracy: 0.7517\n",
      "Epoch 40/300, Training Loss: 0.5891, Validation Loss: 0.5382, Training Accuracy: 0.7059, Validation Accuracy: 0.7383\n",
      "Epoch 50/300, Training Loss: 0.5650, Validation Loss: 0.5488, Training Accuracy: 0.7145, Validation Accuracy: 0.7248\n",
      "Epoch 60/300, Training Loss: 0.6163, Validation Loss: 0.5404, Training Accuracy: 0.6973, Validation Accuracy: 0.7248\n",
      "Epoch 70/300, Training Loss: 0.5780, Validation Loss: 0.5406, Training Accuracy: 0.7231, Validation Accuracy: 0.7383\n",
      "Epoch 80/300, Training Loss: 0.5797, Validation Loss: 0.5407, Training Accuracy: 0.7231, Validation Accuracy: 0.7248\n",
      "Epoch 90/300, Training Loss: 0.5657, Validation Loss: 0.5373, Training Accuracy: 0.7116, Validation Accuracy: 0.7248\n",
      "Epoch 100/300, Training Loss: 0.5897, Validation Loss: 0.5516, Training Accuracy: 0.6815, Validation Accuracy: 0.7450\n",
      "Epoch 110/300, Training Loss: 0.6072, Validation Loss: 0.5640, Training Accuracy: 0.7145, Validation Accuracy: 0.7248\n",
      "Epoch 120/300, Training Loss: 0.5728, Validation Loss: 0.5472, Training Accuracy: 0.7159, Validation Accuracy: 0.7248\n",
      "Epoch 130/300, Training Loss: 0.5806, Validation Loss: 0.5332, Training Accuracy: 0.6944, Validation Accuracy: 0.7315\n",
      "Epoch 140/300, Training Loss: 0.5921, Validation Loss: 0.5429, Training Accuracy: 0.6973, Validation Accuracy: 0.7315\n",
      "Epoch 150/300, Training Loss: 0.5735, Validation Loss: 0.5321, Training Accuracy: 0.7303, Validation Accuracy: 0.7181\n",
      "Epoch 160/300, Training Loss: 0.5714, Validation Loss: 0.5548, Training Accuracy: 0.7260, Validation Accuracy: 0.7181\n",
      "Epoch 170/300, Training Loss: 0.5880, Validation Loss: 0.5315, Training Accuracy: 0.6872, Validation Accuracy: 0.7383\n",
      "Epoch 180/300, Training Loss: 0.5731, Validation Loss: 0.5517, Training Accuracy: 0.7102, Validation Accuracy: 0.7047\n",
      "Epoch 190/300, Training Loss: 0.5971, Validation Loss: 0.5584, Training Accuracy: 0.6801, Validation Accuracy: 0.6913\n",
      "Epoch 200/300, Training Loss: 0.5902, Validation Loss: 0.5447, Training Accuracy: 0.7088, Validation Accuracy: 0.7450\n",
      "Epoch 210/300, Training Loss: 0.5766, Validation Loss: 0.5289, Training Accuracy: 0.6901, Validation Accuracy: 0.7315\n",
      "Epoch 220/300, Training Loss: 0.5981, Validation Loss: 0.5586, Training Accuracy: 0.6901, Validation Accuracy: 0.7450\n",
      "Epoch 230/300, Training Loss: 0.6103, Validation Loss: 0.5502, Training Accuracy: 0.6901, Validation Accuracy: 0.7114\n",
      "Epoch 240/300, Training Loss: 0.5923, Validation Loss: 0.5560, Training Accuracy: 0.6887, Validation Accuracy: 0.7047\n",
      "Epoch 250/300, Training Loss: 0.5976, Validation Loss: 0.5427, Training Accuracy: 0.6930, Validation Accuracy: 0.7450\n",
      "Epoch 260/300, Training Loss: 0.5939, Validation Loss: 0.5461, Training Accuracy: 0.6930, Validation Accuracy: 0.7248\n",
      "Epoch 270/300, Training Loss: 0.5827, Validation Loss: 0.5232, Training Accuracy: 0.6887, Validation Accuracy: 0.7315\n",
      "Epoch 280/300, Training Loss: 0.5620, Validation Loss: 0.5367, Training Accuracy: 0.7245, Validation Accuracy: 0.7315\n",
      "Epoch 290/300, Training Loss: 0.6002, Validation Loss: 0.5404, Training Accuracy: 0.6958, Validation Accuracy: 0.7181\n",
      "Epoch 300/300, Training Loss: 0.6102, Validation Loss: 0.5422, Training Accuracy: 0.6686, Validation Accuracy: 0.7383\n",
      "Test Loss: 0.5489, Test Accuracy: 0.7400\n",
      "Batch size: 64\n",
      "Test Accuracy: 0.7400\n",
      "Test Precision: 0.8200\n",
      "Test Recall: 0.5775\n",
      "Test F1 Score: 0.6777\n",
      "Epoch 10/300, Training Loss: 0.5712, Validation Loss: 0.5446, Training Accuracy: 0.7131, Validation Accuracy: 0.7181\n",
      "Epoch 20/300, Training Loss: 0.5670, Validation Loss: 0.5340, Training Accuracy: 0.7217, Validation Accuracy: 0.7315\n",
      "Epoch 30/300, Training Loss: 0.5853, Validation Loss: 0.5340, Training Accuracy: 0.6915, Validation Accuracy: 0.7315\n",
      "Epoch 40/300, Training Loss: 0.5687, Validation Loss: 0.5519, Training Accuracy: 0.7102, Validation Accuracy: 0.7114\n",
      "Epoch 50/300, Training Loss: 0.5609, Validation Loss: 0.5311, Training Accuracy: 0.7202, Validation Accuracy: 0.7383\n",
      "Epoch 60/300, Training Loss: 0.6282, Validation Loss: 0.5527, Training Accuracy: 0.6700, Validation Accuracy: 0.7181\n",
      "Epoch 70/300, Training Loss: 0.5723, Validation Loss: 0.5445, Training Accuracy: 0.7188, Validation Accuracy: 0.7383\n",
      "Epoch 80/300, Training Loss: 0.5764, Validation Loss: 0.5499, Training Accuracy: 0.7202, Validation Accuracy: 0.7114\n",
      "Epoch 90/300, Training Loss: 0.5954, Validation Loss: 0.5251, Training Accuracy: 0.6987, Validation Accuracy: 0.7383\n",
      "Epoch 100/300, Training Loss: 0.5608, Validation Loss: 0.5454, Training Accuracy: 0.7088, Validation Accuracy: 0.7248\n",
      "Epoch 110/300, Training Loss: 0.5725, Validation Loss: 0.5349, Training Accuracy: 0.7016, Validation Accuracy: 0.7517\n",
      "Epoch 120/300, Training Loss: 0.5691, Validation Loss: 0.5301, Training Accuracy: 0.7145, Validation Accuracy: 0.7517\n",
      "Epoch 130/300, Training Loss: 0.5697, Validation Loss: 0.5429, Training Accuracy: 0.7030, Validation Accuracy: 0.7248\n",
      "Epoch 140/300, Training Loss: 0.5855, Validation Loss: 0.5336, Training Accuracy: 0.7260, Validation Accuracy: 0.7450\n",
      "Epoch 150/300, Training Loss: 0.5710, Validation Loss: 0.5518, Training Accuracy: 0.7202, Validation Accuracy: 0.7248\n",
      "Epoch 160/300, Training Loss: 0.5777, Validation Loss: 0.5328, Training Accuracy: 0.7131, Validation Accuracy: 0.7383\n",
      "Epoch 170/300, Training Loss: 0.5656, Validation Loss: 0.5460, Training Accuracy: 0.7159, Validation Accuracy: 0.7248\n",
      "Epoch 180/300, Training Loss: 0.5662, Validation Loss: 0.5419, Training Accuracy: 0.7073, Validation Accuracy: 0.7383\n",
      "Epoch 190/300, Training Loss: 0.6089, Validation Loss: 0.5459, Training Accuracy: 0.7116, Validation Accuracy: 0.7383\n",
      "Epoch 200/300, Training Loss: 0.5648, Validation Loss: 0.5403, Training Accuracy: 0.7303, Validation Accuracy: 0.7315\n",
      "Epoch 210/300, Training Loss: 0.5662, Validation Loss: 0.5440, Training Accuracy: 0.6987, Validation Accuracy: 0.7114\n",
      "Epoch 220/300, Training Loss: 0.5465, Validation Loss: 0.5469, Training Accuracy: 0.7131, Validation Accuracy: 0.7114\n",
      "Epoch 230/300, Training Loss: 0.5754, Validation Loss: 0.5340, Training Accuracy: 0.7231, Validation Accuracy: 0.7383\n",
      "Epoch 240/300, Training Loss: 0.5703, Validation Loss: 0.5425, Training Accuracy: 0.7174, Validation Accuracy: 0.7315\n",
      "Epoch 250/300, Training Loss: 0.6105, Validation Loss: 0.5419, Training Accuracy: 0.7001, Validation Accuracy: 0.7450\n",
      "Epoch 260/300, Training Loss: 0.5896, Validation Loss: 0.5299, Training Accuracy: 0.7102, Validation Accuracy: 0.7383\n",
      "Epoch 270/300, Training Loss: 0.5710, Validation Loss: 0.5323, Training Accuracy: 0.7231, Validation Accuracy: 0.7383\n",
      "Epoch 280/300, Training Loss: 0.5960, Validation Loss: 0.5359, Training Accuracy: 0.7303, Validation Accuracy: 0.7383\n",
      "Epoch 290/300, Training Loss: 0.5561, Validation Loss: 0.5297, Training Accuracy: 0.7073, Validation Accuracy: 0.7383\n",
      "Epoch 300/300, Training Loss: 0.5786, Validation Loss: 0.5430, Training Accuracy: 0.7088, Validation Accuracy: 0.7181\n",
      "Test Loss: 0.5509, Test Accuracy: 0.7400\n",
      "Batch size: 128\n",
      "Test Accuracy: 0.7400\n",
      "Test Precision: 0.8200\n",
      "Test Recall: 0.5775\n",
      "Test F1 Score: 0.6777\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "batch_size_list = [32, 64, 128]\n",
    "for batch in batch_size_list:\n",
    "    \n",
    "    num_epochs = 300\n",
    "    batch_size = batch\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "                val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "                correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluation on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_losses = [test_loss] * num_epochs\n",
    "    test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Learning Rate:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.7929, Validation Loss: 0.6870, Training Accuracy: 0.5093, Validation Accuracy: 0.6040\n",
      "Epoch 20/300, Training Loss: 0.7054, Validation Loss: 0.6836, Training Accuracy: 0.5294, Validation Accuracy: 0.6040\n",
      "Epoch 30/300, Training Loss: 0.6985, Validation Loss: 0.6834, Training Accuracy: 0.5395, Validation Accuracy: 0.6040\n",
      "Epoch 40/300, Training Loss: 0.6655, Validation Loss: 0.6775, Training Accuracy: 0.6055, Validation Accuracy: 0.5906\n",
      "Epoch 50/300, Training Loss: 0.6637, Validation Loss: 0.6736, Training Accuracy: 0.5882, Validation Accuracy: 0.5705\n",
      "Epoch 60/300, Training Loss: 0.6683, Validation Loss: 0.6656, Training Accuracy: 0.6069, Validation Accuracy: 0.5839\n",
      "Epoch 70/300, Training Loss: 0.6592, Validation Loss: 0.6683, Training Accuracy: 0.6255, Validation Accuracy: 0.5906\n",
      "Epoch 80/300, Training Loss: 0.6507, Validation Loss: 0.6551, Training Accuracy: 0.6370, Validation Accuracy: 0.6107\n",
      "Epoch 90/300, Training Loss: 0.6577, Validation Loss: 0.6438, Training Accuracy: 0.6126, Validation Accuracy: 0.6443\n",
      "Epoch 100/300, Training Loss: 0.6571, Validation Loss: 0.6452, Training Accuracy: 0.6083, Validation Accuracy: 0.5973\n",
      "Epoch 110/300, Training Loss: 0.6386, Validation Loss: 0.6339, Training Accuracy: 0.6499, Validation Accuracy: 0.6443\n",
      "Epoch 120/300, Training Loss: 0.6492, Validation Loss: 0.6356, Training Accuracy: 0.6456, Validation Accuracy: 0.6040\n",
      "Epoch 130/300, Training Loss: 0.6375, Validation Loss: 0.6186, Training Accuracy: 0.6370, Validation Accuracy: 0.6443\n",
      "Epoch 140/300, Training Loss: 0.6094, Validation Loss: 0.6152, Training Accuracy: 0.6700, Validation Accuracy: 0.6644\n",
      "Epoch 150/300, Training Loss: 0.6253, Validation Loss: 0.6230, Training Accuracy: 0.6657, Validation Accuracy: 0.6242\n",
      "Epoch 160/300, Training Loss: 0.6225, Validation Loss: 0.6159, Training Accuracy: 0.6571, Validation Accuracy: 0.6644\n",
      "Epoch 170/300, Training Loss: 0.6213, Validation Loss: 0.6148, Training Accuracy: 0.6585, Validation Accuracy: 0.6510\n",
      "Epoch 180/300, Training Loss: 0.6217, Validation Loss: 0.6176, Training Accuracy: 0.6714, Validation Accuracy: 0.6107\n",
      "Epoch 190/300, Training Loss: 0.6084, Validation Loss: 0.6061, Training Accuracy: 0.6686, Validation Accuracy: 0.6443\n",
      "Epoch 200/300, Training Loss: 0.6170, Validation Loss: 0.6080, Training Accuracy: 0.6729, Validation Accuracy: 0.6443\n",
      "Epoch 210/300, Training Loss: 0.6031, Validation Loss: 0.6168, Training Accuracy: 0.6930, Validation Accuracy: 0.6107\n",
      "Epoch 220/300, Training Loss: 0.6152, Validation Loss: 0.6070, Training Accuracy: 0.6714, Validation Accuracy: 0.6443\n",
      "Epoch 230/300, Training Loss: 0.6261, Validation Loss: 0.6071, Training Accuracy: 0.6743, Validation Accuracy: 0.6174\n",
      "Epoch 240/300, Training Loss: 0.6100, Validation Loss: 0.6027, Training Accuracy: 0.6844, Validation Accuracy: 0.6309\n",
      "Epoch 250/300, Training Loss: 0.5985, Validation Loss: 0.6038, Training Accuracy: 0.7044, Validation Accuracy: 0.6107\n",
      "Epoch 260/300, Training Loss: 0.5936, Validation Loss: 0.5927, Training Accuracy: 0.6786, Validation Accuracy: 0.6510\n",
      "Epoch 270/300, Training Loss: 0.5984, Validation Loss: 0.5956, Training Accuracy: 0.6944, Validation Accuracy: 0.6644\n",
      "Epoch 280/300, Training Loss: 0.6212, Validation Loss: 0.6061, Training Accuracy: 0.6714, Validation Accuracy: 0.6309\n",
      "Epoch 290/300, Training Loss: 0.6081, Validation Loss: 0.6067, Training Accuracy: 0.6758, Validation Accuracy: 0.6309\n",
      "Epoch 300/300, Training Loss: 0.5887, Validation Loss: 0.5803, Training Accuracy: 0.7102, Validation Accuracy: 0.6913\n",
      "Test Loss: 0.6135, Test Accuracy: 0.6667\n",
      "Learning Rate: 0.001\n",
      "Test Accuracy: 0.6667\n",
      "Test Precision: 0.7234\n",
      "Test Recall: 0.4789\n",
      "Test F1 Score: 0.5763\n",
      "Epoch 10/300, Training Loss: 0.6528, Validation Loss: 0.6128, Training Accuracy: 0.6571, Validation Accuracy: 0.6309\n",
      "Epoch 20/300, Training Loss: 0.6337, Validation Loss: 0.5902, Training Accuracy: 0.6485, Validation Accuracy: 0.6779\n",
      "Epoch 30/300, Training Loss: 0.6220, Validation Loss: 0.6221, Training Accuracy: 0.6671, Validation Accuracy: 0.6174\n",
      "Epoch 40/300, Training Loss: 0.6273, Validation Loss: 0.5964, Training Accuracy: 0.6628, Validation Accuracy: 0.6711\n",
      "Epoch 50/300, Training Loss: 0.6332, Validation Loss: 0.6033, Training Accuracy: 0.6542, Validation Accuracy: 0.6577\n",
      "Epoch 60/300, Training Loss: 0.6442, Validation Loss: 0.5869, Training Accuracy: 0.6356, Validation Accuracy: 0.7114\n",
      "Epoch 70/300, Training Loss: 0.6342, Validation Loss: 0.5870, Training Accuracy: 0.6628, Validation Accuracy: 0.6443\n",
      "Epoch 80/300, Training Loss: 0.6171, Validation Loss: 0.5782, Training Accuracy: 0.6341, Validation Accuracy: 0.6711\n",
      "Epoch 90/300, Training Loss: 0.6091, Validation Loss: 0.5604, Training Accuracy: 0.7131, Validation Accuracy: 0.7315\n",
      "Epoch 100/300, Training Loss: 0.5986, Validation Loss: 0.5831, Training Accuracy: 0.6987, Validation Accuracy: 0.7114\n",
      "Epoch 110/300, Training Loss: 0.6186, Validation Loss: 0.5905, Training Accuracy: 0.6958, Validation Accuracy: 0.6644\n",
      "Epoch 120/300, Training Loss: 0.6106, Validation Loss: 0.5707, Training Accuracy: 0.6887, Validation Accuracy: 0.6846\n",
      "Epoch 130/300, Training Loss: 0.5854, Validation Loss: 0.5591, Training Accuracy: 0.6915, Validation Accuracy: 0.7047\n",
      "Epoch 140/300, Training Loss: 0.6075, Validation Loss: 0.5794, Training Accuracy: 0.6643, Validation Accuracy: 0.7315\n",
      "Epoch 150/300, Training Loss: 0.6193, Validation Loss: 0.5706, Training Accuracy: 0.7088, Validation Accuracy: 0.6913\n",
      "Epoch 160/300, Training Loss: 0.5952, Validation Loss: 0.5640, Training Accuracy: 0.6786, Validation Accuracy: 0.7450\n",
      "Epoch 170/300, Training Loss: 0.5876, Validation Loss: 0.5470, Training Accuracy: 0.7030, Validation Accuracy: 0.6913\n",
      "Epoch 180/300, Training Loss: 0.6020, Validation Loss: 0.5796, Training Accuracy: 0.7001, Validation Accuracy: 0.7315\n",
      "Epoch 190/300, Training Loss: 0.6583, Validation Loss: 0.6094, Training Accuracy: 0.6542, Validation Accuracy: 0.7047\n",
      "Epoch 200/300, Training Loss: 0.5631, Validation Loss: 0.5460, Training Accuracy: 0.7102, Validation Accuracy: 0.7114\n",
      "Epoch 210/300, Training Loss: 0.6077, Validation Loss: 0.5599, Training Accuracy: 0.6858, Validation Accuracy: 0.7315\n",
      "Epoch 220/300, Training Loss: 0.5833, Validation Loss: 0.5417, Training Accuracy: 0.7001, Validation Accuracy: 0.7248\n",
      "Epoch 230/300, Training Loss: 0.5960, Validation Loss: 0.5605, Training Accuracy: 0.6930, Validation Accuracy: 0.7181\n",
      "Epoch 240/300, Training Loss: 0.6429, Validation Loss: 0.5915, Training Accuracy: 0.6557, Validation Accuracy: 0.7047\n",
      "Epoch 250/300, Training Loss: 0.6116, Validation Loss: 0.5453, Training Accuracy: 0.6887, Validation Accuracy: 0.7248\n",
      "Epoch 260/300, Training Loss: 0.6071, Validation Loss: 0.5574, Training Accuracy: 0.6887, Validation Accuracy: 0.7181\n",
      "Epoch 270/300, Training Loss: 0.5817, Validation Loss: 0.5463, Training Accuracy: 0.6901, Validation Accuracy: 0.7383\n",
      "Epoch 280/300, Training Loss: 0.6348, Validation Loss: 0.5747, Training Accuracy: 0.6542, Validation Accuracy: 0.7315\n",
      "Epoch 290/300, Training Loss: 0.6058, Validation Loss: 0.5523, Training Accuracy: 0.6829, Validation Accuracy: 0.7315\n",
      "Epoch 300/300, Training Loss: 0.6140, Validation Loss: 0.5439, Training Accuracy: 0.6958, Validation Accuracy: 0.7181\n",
      "Test Loss: 0.5308, Test Accuracy: 0.7533\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.7533\n",
      "Test Precision: 0.7656\n",
      "Test Recall: 0.6901\n",
      "Test F1 Score: 0.7259\n",
      "Epoch 10/300, Training Loss: 0.5979, Validation Loss: 0.5500, Training Accuracy: 0.6829, Validation Accuracy: 0.7114\n",
      "Epoch 20/300, Training Loss: 0.6189, Validation Loss: 0.5783, Training Accuracy: 0.6671, Validation Accuracy: 0.6846\n",
      "Epoch 30/300, Training Loss: 0.5941, Validation Loss: 0.5238, Training Accuracy: 0.7016, Validation Accuracy: 0.7383\n",
      "Epoch 40/300, Training Loss: 0.6367, Validation Loss: 0.5395, Training Accuracy: 0.6958, Validation Accuracy: 0.7248\n",
      "Epoch 50/300, Training Loss: 0.7043, Validation Loss: 0.5796, Training Accuracy: 0.6786, Validation Accuracy: 0.7181\n",
      "Epoch 60/300, Training Loss: 0.6377, Validation Loss: 0.5545, Training Accuracy: 0.6628, Validation Accuracy: 0.7114\n",
      "Epoch 70/300, Training Loss: 0.6147, Validation Loss: 0.5708, Training Accuracy: 0.6671, Validation Accuracy: 0.6779\n",
      "Epoch 80/300, Training Loss: 0.5875, Validation Loss: 0.5335, Training Accuracy: 0.7059, Validation Accuracy: 0.7383\n",
      "Epoch 90/300, Training Loss: 0.5816, Validation Loss: 0.5402, Training Accuracy: 0.6844, Validation Accuracy: 0.7383\n",
      "Epoch 100/300, Training Loss: 0.5943, Validation Loss: 0.5615, Training Accuracy: 0.7044, Validation Accuracy: 0.7114\n",
      "Epoch 110/300, Training Loss: 0.5927, Validation Loss: 0.6175, Training Accuracy: 0.6930, Validation Accuracy: 0.7315\n",
      "Epoch 120/300, Training Loss: 0.6117, Validation Loss: 0.5666, Training Accuracy: 0.6772, Validation Accuracy: 0.6980\n",
      "Epoch 130/300, Training Loss: 0.5914, Validation Loss: 0.5476, Training Accuracy: 0.6944, Validation Accuracy: 0.7114\n",
      "Epoch 140/300, Training Loss: 0.5843, Validation Loss: 0.5298, Training Accuracy: 0.6872, Validation Accuracy: 0.7315\n",
      "Epoch 150/300, Training Loss: 0.6101, Validation Loss: 0.5443, Training Accuracy: 0.6657, Validation Accuracy: 0.7450\n",
      "Epoch 160/300, Training Loss: 0.7191, Validation Loss: 0.5476, Training Accuracy: 0.7044, Validation Accuracy: 0.7114\n",
      "Epoch 170/300, Training Loss: 0.5978, Validation Loss: 0.5586, Training Accuracy: 0.6743, Validation Accuracy: 0.7315\n",
      "Epoch 180/300, Training Loss: 0.5905, Validation Loss: 0.5447, Training Accuracy: 0.6987, Validation Accuracy: 0.7248\n",
      "Epoch 190/300, Training Loss: 0.6062, Validation Loss: 0.5468, Training Accuracy: 0.6844, Validation Accuracy: 0.7114\n",
      "Epoch 200/300, Training Loss: 0.6292, Validation Loss: 0.5673, Training Accuracy: 0.6729, Validation Accuracy: 0.6779\n",
      "Epoch 210/300, Training Loss: 0.6117, Validation Loss: 0.5527, Training Accuracy: 0.6858, Validation Accuracy: 0.7248\n",
      "Epoch 220/300, Training Loss: 0.5735, Validation Loss: 0.5395, Training Accuracy: 0.7174, Validation Accuracy: 0.7315\n",
      "Epoch 230/300, Training Loss: 0.5983, Validation Loss: 0.5352, Training Accuracy: 0.6872, Validation Accuracy: 0.7248\n",
      "Epoch 240/300, Training Loss: 0.6079, Validation Loss: 0.5512, Training Accuracy: 0.6786, Validation Accuracy: 0.7383\n",
      "Epoch 250/300, Training Loss: 0.6011, Validation Loss: 0.5354, Training Accuracy: 0.6686, Validation Accuracy: 0.7248\n",
      "Epoch 260/300, Training Loss: 0.6492, Validation Loss: 0.5332, Training Accuracy: 0.6958, Validation Accuracy: 0.7248\n",
      "Epoch 270/300, Training Loss: 0.6186, Validation Loss: 0.5495, Training Accuracy: 0.7073, Validation Accuracy: 0.7315\n",
      "Epoch 280/300, Training Loss: 0.6068, Validation Loss: 0.5376, Training Accuracy: 0.6887, Validation Accuracy: 0.7450\n",
      "Epoch 290/300, Training Loss: 0.5964, Validation Loss: 0.5327, Training Accuracy: 0.6700, Validation Accuracy: 0.7450\n",
      "Epoch 300/300, Training Loss: 0.5775, Validation Loss: 0.5414, Training Accuracy: 0.6987, Validation Accuracy: 0.7315\n",
      "Test Loss: 0.5502, Test Accuracy: 0.7333\n",
      "Learning Rate: 0.01\n",
      "Test Accuracy: 0.7333\n",
      "Test Precision: 0.8298\n",
      "Test Recall: 0.5493\n",
      "Test F1 Score: 0.6610\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning_rates = [0.001, 0.007, 0.01]\n",
    "for learning in learning_rates:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "    num_epochs = 300\n",
    "    batch_size = 64\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "                val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "                correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluation on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_losses = [test_loss] * num_epochs\n",
    "    test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"Learning Rate: {learning}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6841, Validation Loss: 0.6715, Training Accuracy: 0.5739, Validation Accuracy: 0.5772\n",
      "Epoch 20/300, Training Loss: 0.6739, Validation Loss: 0.6644, Training Accuracy: 0.5940, Validation Accuracy: 0.5973\n",
      "Epoch 30/300, Training Loss: 0.6571, Validation Loss: 0.6322, Training Accuracy: 0.6112, Validation Accuracy: 0.6711\n",
      "Epoch 40/300, Training Loss: 0.6294, Validation Loss: 0.6217, Training Accuracy: 0.6485, Validation Accuracy: 0.6913\n",
      "Epoch 50/300, Training Loss: 0.6413, Validation Loss: 0.6221, Training Accuracy: 0.6198, Validation Accuracy: 0.6510\n",
      "Epoch 60/300, Training Loss: 0.6486, Validation Loss: 0.6093, Training Accuracy: 0.6485, Validation Accuracy: 0.6443\n",
      "Epoch 70/300, Training Loss: 0.6517, Validation Loss: 0.6437, Training Accuracy: 0.6341, Validation Accuracy: 0.6107\n",
      "Epoch 80/300, Training Loss: 0.6303, Validation Loss: 0.6037, Training Accuracy: 0.6456, Validation Accuracy: 0.6913\n",
      "Epoch 90/300, Training Loss: 0.6234, Validation Loss: 0.6027, Training Accuracy: 0.6585, Validation Accuracy: 0.6644\n",
      "Epoch 100/300, Training Loss: 0.6477, Validation Loss: 0.6265, Training Accuracy: 0.6485, Validation Accuracy: 0.6174\n",
      "Epoch 110/300, Training Loss: 0.6478, Validation Loss: 0.6047, Training Accuracy: 0.6370, Validation Accuracy: 0.6577\n",
      "Epoch 120/300, Training Loss: 0.6200, Validation Loss: 0.6044, Training Accuracy: 0.6614, Validation Accuracy: 0.6846\n",
      "Epoch 130/300, Training Loss: 0.6370, Validation Loss: 0.6007, Training Accuracy: 0.6514, Validation Accuracy: 0.7248\n",
      "Epoch 140/300, Training Loss: 0.5937, Validation Loss: 0.5789, Training Accuracy: 0.6973, Validation Accuracy: 0.6644\n",
      "Epoch 150/300, Training Loss: 0.6136, Validation Loss: 0.5794, Training Accuracy: 0.6829, Validation Accuracy: 0.6913\n",
      "Epoch 160/300, Training Loss: 0.6312, Validation Loss: 0.6266, Training Accuracy: 0.6600, Validation Accuracy: 0.6040\n",
      "Epoch 170/300, Training Loss: 0.5978, Validation Loss: 0.6075, Training Accuracy: 0.6829, Validation Accuracy: 0.6577\n",
      "Epoch 180/300, Training Loss: 0.5856, Validation Loss: 0.5678, Training Accuracy: 0.6844, Validation Accuracy: 0.7181\n",
      "Epoch 190/300, Training Loss: 0.5774, Validation Loss: 0.5604, Training Accuracy: 0.7174, Validation Accuracy: 0.6913\n",
      "Epoch 200/300, Training Loss: 0.6112, Validation Loss: 0.5706, Training Accuracy: 0.6628, Validation Accuracy: 0.7114\n",
      "Epoch 210/300, Training Loss: 0.5865, Validation Loss: 0.5593, Training Accuracy: 0.6815, Validation Accuracy: 0.7181\n",
      "Epoch 220/300, Training Loss: 0.5833, Validation Loss: 0.5610, Training Accuracy: 0.6973, Validation Accuracy: 0.7248\n",
      "Epoch 230/300, Training Loss: 0.6123, Validation Loss: 0.6184, Training Accuracy: 0.6614, Validation Accuracy: 0.6779\n",
      "Epoch 240/300, Training Loss: 0.5639, Validation Loss: 0.5490, Training Accuracy: 0.6973, Validation Accuracy: 0.6980\n",
      "Epoch 250/300, Training Loss: 0.6056, Validation Loss: 0.5633, Training Accuracy: 0.6887, Validation Accuracy: 0.7248\n",
      "Epoch 260/300, Training Loss: 0.5856, Validation Loss: 0.5543, Training Accuracy: 0.6872, Validation Accuracy: 0.7584\n",
      "Epoch 270/300, Training Loss: 0.5901, Validation Loss: 0.5496, Training Accuracy: 0.6887, Validation Accuracy: 0.7383\n",
      "Epoch 280/300, Training Loss: 0.5833, Validation Loss: 0.5456, Training Accuracy: 0.7073, Validation Accuracy: 0.7181\n",
      "Epoch 290/300, Training Loss: 0.5968, Validation Loss: 0.5450, Training Accuracy: 0.7016, Validation Accuracy: 0.7181\n",
      "Epoch 300/300, Training Loss: 0.5875, Validation Loss: 0.5479, Training Accuracy: 0.7059, Validation Accuracy: 0.7248\n",
      "Test Loss: 0.5271, Test Accuracy: 0.7867\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.7867\n",
      "Test Precision: 0.7671\n",
      "Test Recall: 0.7887\n",
      "Test F1 Score: 0.7778\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Learning Rate: {learning}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping implementation\n",
    "early_stopping_patience = 10  # Number of epochs with no improvement after which training will be stopped\n",
    "no_improvement_epochs = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels.unsqueeze(1).float())\n",
    "            val_loss += loss.item() * val_inputs.size(0)\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0  # Reset counter\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "    \n",
    "    if no_improvement_epochs >= early_stopping_patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
