{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part - 2:\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               f3      target\n",
      "count  766.000000  766.000000\n",
      "mean    69.118799    0.349869\n",
      "std     19.376901    0.477240\n",
      "min      0.000000    0.000000\n",
      "25%     62.500000    0.000000\n",
      "50%     72.000000    0.000000\n",
      "75%     80.000000    1.000000\n",
      "max    122.000000    1.000000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of             f1     f2  f3         f4          f5        f6        f7  target\n",
       "0     6.000000  148.0  72  35.000000    0.000000  33.60000  0.627000       1\n",
       "1     1.000000   85.0  66  29.000000    0.000000  26.60000  0.351000       0\n",
       "2     8.000000  183.0  64   0.000000    0.000000  23.30000  0.672000       1\n",
       "3     1.000000   89.0  66  23.000000   94.000000  28.10000  0.167000       0\n",
       "4     0.000000  137.0  40  35.000000  168.000000  43.10000  2.288000       1\n",
       "..         ...    ...  ..        ...         ...       ...       ...     ...\n",
       "761   9.000000   89.0  62   0.000000    0.000000  22.50000  0.472128       0\n",
       "762  10.000000  101.0  76  48.000000  180.000000  31.99817  0.171000       0\n",
       "763   2.000000  122.0  70  27.000000   80.091503  36.80000  0.340000       0\n",
       "764   3.849673  121.0  72  23.000000  112.000000  26.20000  0.245000       0\n",
       "765   1.000000  126.0  60  20.542484    0.000000  30.10000  0.349000       1\n",
       "\n",
       "[766 rows x 8 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that there are alphabets in numeric columns so we replace them with mean of that respective column\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "df.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                f1        f2        f3            f4        f5            f6  \\\n",
       "0    6.382139e-01  0.849057  0.148790  9.070150e-01 -0.695096  2.030729e-01   \n",
       "1   -8.457789e-01 -1.125480 -0.161060  5.305956e-01 -0.695096 -6.843559e-01   \n",
       "2    1.231811e+00  1.946022 -0.264343 -1.288765e+00 -0.695096 -1.102715e+00   \n",
       "3   -8.457789e-01 -1.000113 -0.161060  1.541762e-01  0.120709 -4.941926e-01   \n",
       "4   -1.142577e+00  0.504297 -1.503740  9.070150e-01  0.762938  1.407440e+00   \n",
       "..            ...       ...       ...           ...       ...           ...   \n",
       "761  1.528610e+00 -1.000113 -0.367626 -1.288765e+00 -0.695096 -1.204136e+00   \n",
       "762  1.825408e+00 -0.624010  0.355356  1.722590e+00  0.867083  4.503972e-16   \n",
       "763 -5.489804e-01  0.034169  0.045507  4.051224e-01  0.000000  6.087546e-01   \n",
       "764  1.318050e-16  0.002827  0.148790  1.541762e-01  0.276927 -7.350661e-01   \n",
       "765 -8.457789e-01  0.159536 -0.470909  2.228851e-16 -0.695096 -2.406415e-01   \n",
       "\n",
       "               f7  target  \n",
       "0    4.677335e-01       1  \n",
       "1   -3.658228e-01       0  \n",
       "2    6.036394e-01       1  \n",
       "3   -9.215270e-01       0  \n",
       "4    5.484172e+00       1  \n",
       "..            ...     ...  \n",
       "761  1.676510e-16       0  \n",
       "762 -9.094465e-01       0  \n",
       "763 -3.990443e-01       0  \n",
       "764 -6.859567e-01       0  \n",
       "765 -3.718631e-01       1  \n",
       "\n",
       "[766 rows x 8 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df1 = df.drop('target', axis=1)\n",
    "column = df1.columns\n",
    "df1[column] = scaler.fit_transform(df1[column])\n",
    "df1['target'] = df['target']\n",
    "df1.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  torch.Size([697, 7])\n",
      "Y_train size:  torch.Size([697])\n",
      "X_val size:  torch.Size([149, 7])\n",
      "Y_val size:  torch.Size([149])\n",
      "X_test size:  torch.Size([150, 7])\n",
      "Y_test size:  torch.Size([150])\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('target', axis=1)  # Replace 'target_column' with the actual target column name\n",
    "Y = df['target']\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X, Y = ros.fit_resample(X, Y)\n",
    "# Split the data into training and testing sets (80% train, 20% test by default)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.175, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "print(\"X_train size: \", X_train_tensor.shape)\n",
    "print(\"Y_train size: \", Y_train_tensor.shape)\n",
    "print(\"X_val size: \", X_val_tensor.shape)\n",
    "print(\"Y_val size: \", Y_val_tensor.shape)\n",
    "print(\"X_test size: \", X_test_tensor.shape)\n",
    "print(\"Y_test size: \", Y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning hyperparameters:\n",
    "=======================\n",
    "1. Dropout value:\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_dropout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(NN_dropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6783, Validation Loss: 0.6423, Training Accuracy: 0.5710, Validation Accuracy: 0.6711\n",
      "Epoch 20/300, Training Loss: 0.6290, Validation Loss: 0.5994, Training Accuracy: 0.6686, Validation Accuracy: 0.6846\n",
      "Epoch 30/300, Training Loss: 0.6097, Validation Loss: 0.5737, Training Accuracy: 0.6700, Validation Accuracy: 0.7248\n",
      "Epoch 40/300, Training Loss: 0.6285, Validation Loss: 0.5890, Training Accuracy: 0.6542, Validation Accuracy: 0.6779\n",
      "Epoch 50/300, Training Loss: 0.5895, Validation Loss: 0.5581, Training Accuracy: 0.6887, Validation Accuracy: 0.7517\n",
      "Epoch 60/300, Training Loss: 0.5836, Validation Loss: 0.5528, Training Accuracy: 0.6901, Validation Accuracy: 0.7114\n",
      "Epoch 70/300, Training Loss: 0.5616, Validation Loss: 0.5605, Training Accuracy: 0.7174, Validation Accuracy: 0.7450\n",
      "Epoch 80/300, Training Loss: 0.5791, Validation Loss: 0.5532, Training Accuracy: 0.7231, Validation Accuracy: 0.7181\n",
      "Epoch 90/300, Training Loss: 0.5799, Validation Loss: 0.5457, Training Accuracy: 0.7059, Validation Accuracy: 0.7383\n",
      "Epoch 100/300, Training Loss: 0.5692, Validation Loss: 0.5352, Training Accuracy: 0.7145, Validation Accuracy: 0.7383\n",
      "Epoch 110/300, Training Loss: 0.5841, Validation Loss: 0.5419, Training Accuracy: 0.7044, Validation Accuracy: 0.7584\n",
      "Epoch 120/300, Training Loss: 0.5670, Validation Loss: 0.5334, Training Accuracy: 0.7360, Validation Accuracy: 0.7450\n",
      "Epoch 130/300, Training Loss: 0.5467, Validation Loss: 0.5388, Training Accuracy: 0.7274, Validation Accuracy: 0.7651\n",
      "Epoch 140/300, Training Loss: 0.5568, Validation Loss: 0.5360, Training Accuracy: 0.7346, Validation Accuracy: 0.7718\n",
      "Epoch 150/300, Training Loss: 0.5663, Validation Loss: 0.5323, Training Accuracy: 0.7073, Validation Accuracy: 0.7315\n",
      "Epoch 160/300, Training Loss: 0.5636, Validation Loss: 0.5530, Training Accuracy: 0.7217, Validation Accuracy: 0.7248\n",
      "Epoch 170/300, Training Loss: 0.5431, Validation Loss: 0.5304, Training Accuracy: 0.7044, Validation Accuracy: 0.7315\n",
      "Epoch 180/300, Training Loss: 0.6084, Validation Loss: 0.5395, Training Accuracy: 0.6915, Validation Accuracy: 0.7315\n",
      "Epoch 190/300, Training Loss: 0.5750, Validation Loss: 0.5516, Training Accuracy: 0.7145, Validation Accuracy: 0.6980\n",
      "Epoch 200/300, Training Loss: 0.5526, Validation Loss: 0.5355, Training Accuracy: 0.7288, Validation Accuracy: 0.7383\n",
      "Epoch 210/300, Training Loss: 0.5750, Validation Loss: 0.5417, Training Accuracy: 0.7116, Validation Accuracy: 0.7383\n",
      "Epoch 220/300, Training Loss: 0.5790, Validation Loss: 0.5476, Training Accuracy: 0.7030, Validation Accuracy: 0.7450\n",
      "Epoch 230/300, Training Loss: 0.5628, Validation Loss: 0.5520, Training Accuracy: 0.7131, Validation Accuracy: 0.7114\n",
      "Epoch 240/300, Training Loss: 0.5467, Validation Loss: 0.5313, Training Accuracy: 0.7260, Validation Accuracy: 0.7584\n",
      "Epoch 250/300, Training Loss: 0.5623, Validation Loss: 0.5852, Training Accuracy: 0.7188, Validation Accuracy: 0.7114\n",
      "Epoch 260/300, Training Loss: 0.5467, Validation Loss: 0.5538, Training Accuracy: 0.7274, Validation Accuracy: 0.7181\n",
      "Epoch 270/300, Training Loss: 0.5595, Validation Loss: 0.5553, Training Accuracy: 0.7202, Validation Accuracy: 0.7450\n",
      "Epoch 280/300, Training Loss: 0.5564, Validation Loss: 0.5308, Training Accuracy: 0.7303, Validation Accuracy: 0.7315\n",
      "Epoch 290/300, Training Loss: 0.5790, Validation Loss: 0.5432, Training Accuracy: 0.7016, Validation Accuracy: 0.7651\n",
      "Epoch 300/300, Training Loss: 0.5335, Validation Loss: 0.5380, Training Accuracy: 0.7346, Validation Accuracy: 0.7450\n",
      "Test Loss: 0.5400, Test Accuracy: 0.6733\n",
      "Dropout: 0.3\n",
      "Test Accuracy: 0.6733\n",
      "Test Precision: 0.6250\n",
      "Test Recall: 0.7746\n",
      "Test F1 Score: 0.6918\n",
      "Epoch 10/300, Training Loss: 0.6797, Validation Loss: 0.6736, Training Accuracy: 0.5882, Validation Accuracy: 0.5839\n",
      "Epoch 20/300, Training Loss: 0.6707, Validation Loss: 0.6476, Training Accuracy: 0.6126, Validation Accuracy: 0.6443\n",
      "Epoch 30/300, Training Loss: 0.6641, Validation Loss: 0.6486, Training Accuracy: 0.6255, Validation Accuracy: 0.6309\n",
      "Epoch 40/300, Training Loss: 0.6582, Validation Loss: 0.6446, Training Accuracy: 0.6083, Validation Accuracy: 0.6174\n",
      "Epoch 50/300, Training Loss: 0.6708, Validation Loss: 0.6454, Training Accuracy: 0.5954, Validation Accuracy: 0.5839\n",
      "Epoch 60/300, Training Loss: 0.6414, Validation Loss: 0.6289, Training Accuracy: 0.6700, Validation Accuracy: 0.6242\n",
      "Epoch 70/300, Training Loss: 0.6469, Validation Loss: 0.6075, Training Accuracy: 0.6643, Validation Accuracy: 0.7248\n",
      "Epoch 80/300, Training Loss: 0.6521, Validation Loss: 0.5995, Training Accuracy: 0.6499, Validation Accuracy: 0.6577\n",
      "Epoch 90/300, Training Loss: 0.6373, Validation Loss: 0.6250, Training Accuracy: 0.6585, Validation Accuracy: 0.6376\n",
      "Epoch 100/300, Training Loss: 0.6354, Validation Loss: 0.5886, Training Accuracy: 0.6700, Validation Accuracy: 0.7047\n",
      "Epoch 110/300, Training Loss: 0.6258, Validation Loss: 0.5774, Training Accuracy: 0.6758, Validation Accuracy: 0.7248\n",
      "Epoch 120/300, Training Loss: 0.6193, Validation Loss: 0.5690, Training Accuracy: 0.6571, Validation Accuracy: 0.6913\n",
      "Epoch 130/300, Training Loss: 0.6013, Validation Loss: 0.5675, Training Accuracy: 0.6729, Validation Accuracy: 0.7181\n",
      "Epoch 140/300, Training Loss: 0.6253, Validation Loss: 0.5667, Training Accuracy: 0.6700, Validation Accuracy: 0.7114\n",
      "Epoch 150/300, Training Loss: 0.6171, Validation Loss: 0.5693, Training Accuracy: 0.6628, Validation Accuracy: 0.6779\n",
      "Epoch 160/300, Training Loss: 0.6086, Validation Loss: 0.5542, Training Accuracy: 0.6686, Validation Accuracy: 0.7383\n",
      "Epoch 170/300, Training Loss: 0.6018, Validation Loss: 0.5566, Training Accuracy: 0.6901, Validation Accuracy: 0.7315\n",
      "Epoch 180/300, Training Loss: 0.6217, Validation Loss: 0.5619, Training Accuracy: 0.6643, Validation Accuracy: 0.7114\n",
      "Epoch 190/300, Training Loss: 0.5904, Validation Loss: 0.5571, Training Accuracy: 0.7044, Validation Accuracy: 0.7383\n",
      "Epoch 200/300, Training Loss: 0.5920, Validation Loss: 0.5591, Training Accuracy: 0.6772, Validation Accuracy: 0.7114\n",
      "Epoch 210/300, Training Loss: 0.5747, Validation Loss: 0.5613, Training Accuracy: 0.7073, Validation Accuracy: 0.7114\n",
      "Epoch 220/300, Training Loss: 0.5836, Validation Loss: 0.5580, Training Accuracy: 0.7088, Validation Accuracy: 0.7315\n",
      "Epoch 230/300, Training Loss: 0.6153, Validation Loss: 0.5581, Training Accuracy: 0.6758, Validation Accuracy: 0.7517\n",
      "Epoch 240/300, Training Loss: 0.5949, Validation Loss: 0.5542, Training Accuracy: 0.6829, Validation Accuracy: 0.7248\n",
      "Epoch 250/300, Training Loss: 0.5835, Validation Loss: 0.5525, Training Accuracy: 0.6973, Validation Accuracy: 0.7315\n",
      "Epoch 260/300, Training Loss: 0.5802, Validation Loss: 0.5513, Training Accuracy: 0.7088, Validation Accuracy: 0.7450\n",
      "Epoch 270/300, Training Loss: 0.5876, Validation Loss: 0.5571, Training Accuracy: 0.7145, Validation Accuracy: 0.7718\n",
      "Epoch 280/300, Training Loss: 0.6250, Validation Loss: 0.5592, Training Accuracy: 0.6643, Validation Accuracy: 0.6913\n",
      "Epoch 290/300, Training Loss: 0.5788, Validation Loss: 0.5557, Training Accuracy: 0.7088, Validation Accuracy: 0.7315\n",
      "Epoch 300/300, Training Loss: 0.5919, Validation Loss: 0.5577, Training Accuracy: 0.7044, Validation Accuracy: 0.7248\n",
      "Test Loss: 0.5888, Test Accuracy: 0.7000\n",
      "Dropout: 0.5\n",
      "Test Accuracy: 0.7000\n",
      "Test Precision: 0.6548\n",
      "Test Recall: 0.7746\n",
      "Test F1 Score: 0.7097\n",
      "Epoch 10/300, Training Loss: 0.6690, Validation Loss: 0.6884, Training Accuracy: 0.5997, Validation Accuracy: 0.5235\n",
      "Epoch 20/300, Training Loss: 0.6807, Validation Loss: 0.6879, Training Accuracy: 0.5753, Validation Accuracy: 0.5369\n",
      "Epoch 30/300, Training Loss: 0.6874, Validation Loss: 0.6877, Training Accuracy: 0.5380, Validation Accuracy: 0.5369\n",
      "Epoch 40/300, Training Loss: 0.6849, Validation Loss: 0.6864, Training Accuracy: 0.5667, Validation Accuracy: 0.5369\n",
      "Epoch 50/300, Training Loss: 0.6689, Validation Loss: 0.6795, Training Accuracy: 0.6026, Validation Accuracy: 0.5503\n",
      "Epoch 60/300, Training Loss: 0.6864, Validation Loss: 0.6698, Training Accuracy: 0.5423, Validation Accuracy: 0.5839\n",
      "Epoch 70/300, Training Loss: 0.6752, Validation Loss: 0.6621, Training Accuracy: 0.5696, Validation Accuracy: 0.5906\n",
      "Epoch 80/300, Training Loss: 0.6742, Validation Loss: 0.6517, Training Accuracy: 0.5954, Validation Accuracy: 0.6376\n",
      "Epoch 90/300, Training Loss: 0.6591, Validation Loss: 0.6317, Training Accuracy: 0.6270, Validation Accuracy: 0.6846\n",
      "Epoch 100/300, Training Loss: 0.6544, Validation Loss: 0.6234, Training Accuracy: 0.6169, Validation Accuracy: 0.6980\n",
      "Epoch 110/300, Training Loss: 0.6500, Validation Loss: 0.6143, Training Accuracy: 0.6270, Validation Accuracy: 0.7181\n",
      "Epoch 120/300, Training Loss: 0.6546, Validation Loss: 0.6175, Training Accuracy: 0.6241, Validation Accuracy: 0.6779\n",
      "Epoch 130/300, Training Loss: 0.6505, Validation Loss: 0.6145, Training Accuracy: 0.6298, Validation Accuracy: 0.6980\n",
      "Epoch 140/300, Training Loss: 0.6363, Validation Loss: 0.6051, Training Accuracy: 0.6542, Validation Accuracy: 0.7114\n",
      "Epoch 150/300, Training Loss: 0.6259, Validation Loss: 0.6007, Training Accuracy: 0.6628, Validation Accuracy: 0.6980\n",
      "Epoch 160/300, Training Loss: 0.6297, Validation Loss: 0.6012, Training Accuracy: 0.6671, Validation Accuracy: 0.6980\n",
      "Epoch 170/300, Training Loss: 0.6307, Validation Loss: 0.5983, Training Accuracy: 0.6628, Validation Accuracy: 0.6913\n",
      "Epoch 180/300, Training Loss: 0.6450, Validation Loss: 0.5988, Training Accuracy: 0.6370, Validation Accuracy: 0.6913\n",
      "Epoch 190/300, Training Loss: 0.6259, Validation Loss: 0.5932, Training Accuracy: 0.6686, Validation Accuracy: 0.7315\n",
      "Epoch 200/300, Training Loss: 0.6397, Validation Loss: 0.5993, Training Accuracy: 0.6643, Validation Accuracy: 0.6913\n",
      "Epoch 210/300, Training Loss: 0.6162, Validation Loss: 0.5935, Training Accuracy: 0.6844, Validation Accuracy: 0.7315\n",
      "Epoch 220/300, Training Loss: 0.6561, Validation Loss: 0.6092, Training Accuracy: 0.6356, Validation Accuracy: 0.6711\n",
      "Epoch 230/300, Training Loss: 0.6436, Validation Loss: 0.6001, Training Accuracy: 0.6298, Validation Accuracy: 0.6913\n",
      "Epoch 240/300, Training Loss: 0.6363, Validation Loss: 0.5972, Training Accuracy: 0.6442, Validation Accuracy: 0.6846\n",
      "Epoch 250/300, Training Loss: 0.6315, Validation Loss: 0.5947, Training Accuracy: 0.6542, Validation Accuracy: 0.7047\n",
      "Epoch 260/300, Training Loss: 0.6500, Validation Loss: 0.5995, Training Accuracy: 0.6428, Validation Accuracy: 0.6846\n",
      "Epoch 270/300, Training Loss: 0.6535, Validation Loss: 0.6014, Training Accuracy: 0.6298, Validation Accuracy: 0.6913\n",
      "Epoch 280/300, Training Loss: 0.6436, Validation Loss: 0.6016, Training Accuracy: 0.6557, Validation Accuracy: 0.6846\n",
      "Epoch 290/300, Training Loss: 0.6291, Validation Loss: 0.5986, Training Accuracy: 0.6327, Validation Accuracy: 0.6980\n",
      "Epoch 300/300, Training Loss: 0.6333, Validation Loss: 0.6010, Training Accuracy: 0.6341, Validation Accuracy: 0.6913\n",
      "Test Loss: 0.6169, Test Accuracy: 0.6933\n",
      "Dropout: 0.7\n",
      "Test Accuracy: 0.6933\n",
      "Test Precision: 0.6984\n",
      "Test Recall: 0.6197\n",
      "Test F1 Score: 0.6567\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "dropout_rate = [0.3, 0.5, 0.7]\n",
    "for dropout in dropout_rate:\n",
    "    model = NN_dropout(input_size, hidden_size, output_size, dropout)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "\n",
    "    num_epochs = 300\n",
    "    batch_size = 64\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "                val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "                correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluation on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_losses = [test_loss] * num_epochs\n",
    "    test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"Dropout: {dropout}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Batch size:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_batchsize_lr(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN_batchsize_lr, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5, inplace=False)\n",
      "Epoch 10/300, Training Loss: 0.6808, Validation Loss: 0.6629, Training Accuracy: 0.5638, Validation Accuracy: 0.6309\n",
      "Epoch 20/300, Training Loss: 0.6682, Validation Loss: 0.6489, Training Accuracy: 0.5868, Validation Accuracy: 0.6309\n",
      "Epoch 30/300, Training Loss: 0.6691, Validation Loss: 0.6571, Training Accuracy: 0.5954, Validation Accuracy: 0.5839\n",
      "Epoch 40/300, Training Loss: 0.6625, Validation Loss: 0.6225, Training Accuracy: 0.6341, Validation Accuracy: 0.7114\n",
      "Epoch 50/300, Training Loss: 0.6600, Validation Loss: 0.6155, Training Accuracy: 0.6327, Validation Accuracy: 0.7114\n",
      "Epoch 60/300, Training Loss: 0.6846, Validation Loss: 0.6472, Training Accuracy: 0.5624, Validation Accuracy: 0.6510\n",
      "Epoch 70/300, Training Loss: 0.6355, Validation Loss: 0.6320, Training Accuracy: 0.6600, Validation Accuracy: 0.6242\n",
      "Epoch 80/300, Training Loss: 0.6528, Validation Loss: 0.6163, Training Accuracy: 0.6298, Validation Accuracy: 0.6980\n",
      "Epoch 90/300, Training Loss: 0.6435, Validation Loss: 0.6040, Training Accuracy: 0.6428, Validation Accuracy: 0.7181\n",
      "Epoch 100/300, Training Loss: 0.6458, Validation Loss: 0.5970, Training Accuracy: 0.6428, Validation Accuracy: 0.6779\n",
      "Epoch 110/300, Training Loss: 0.6306, Validation Loss: 0.5776, Training Accuracy: 0.6801, Validation Accuracy: 0.7450\n",
      "Epoch 120/300, Training Loss: 0.6350, Validation Loss: 0.5744, Training Accuracy: 0.6671, Validation Accuracy: 0.7047\n",
      "Epoch 130/300, Training Loss: 0.6428, Validation Loss: 0.5641, Training Accuracy: 0.6758, Validation Accuracy: 0.7315\n",
      "Epoch 140/300, Training Loss: 0.6155, Validation Loss: 0.6224, Training Accuracy: 0.6743, Validation Accuracy: 0.6644\n",
      "Epoch 150/300, Training Loss: 0.6040, Validation Loss: 0.5524, Training Accuracy: 0.6944, Validation Accuracy: 0.7517\n",
      "Epoch 160/300, Training Loss: 0.6543, Validation Loss: 0.6110, Training Accuracy: 0.6356, Validation Accuracy: 0.7181\n",
      "Epoch 170/300, Training Loss: 0.6177, Validation Loss: 0.5625, Training Accuracy: 0.6700, Validation Accuracy: 0.7114\n",
      "Epoch 180/300, Training Loss: 0.6638, Validation Loss: 0.6536, Training Accuracy: 0.6758, Validation Accuracy: 0.6174\n",
      "Epoch 190/300, Training Loss: 0.6245, Validation Loss: 0.5487, Training Accuracy: 0.6671, Validation Accuracy: 0.7315\n",
      "Epoch 200/300, Training Loss: 0.6093, Validation Loss: 0.5763, Training Accuracy: 0.7073, Validation Accuracy: 0.6846\n",
      "Epoch 210/300, Training Loss: 0.6136, Validation Loss: 0.5801, Training Accuracy: 0.6872, Validation Accuracy: 0.6711\n",
      "Epoch 220/300, Training Loss: 0.6023, Validation Loss: 0.5684, Training Accuracy: 0.6987, Validation Accuracy: 0.6779\n",
      "Epoch 230/300, Training Loss: 0.5972, Validation Loss: 0.5367, Training Accuracy: 0.6829, Validation Accuracy: 0.7584\n",
      "Epoch 240/300, Training Loss: 0.6014, Validation Loss: 0.5475, Training Accuracy: 0.6915, Validation Accuracy: 0.7383\n",
      "Epoch 250/300, Training Loss: 0.6378, Validation Loss: 0.6131, Training Accuracy: 0.6341, Validation Accuracy: 0.6174\n",
      "Epoch 260/300, Training Loss: 0.6001, Validation Loss: 0.5429, Training Accuracy: 0.6844, Validation Accuracy: 0.7181\n",
      "Epoch 270/300, Training Loss: 0.6265, Validation Loss: 0.5704, Training Accuracy: 0.6571, Validation Accuracy: 0.6913\n",
      "Epoch 280/300, Training Loss: 0.6213, Validation Loss: 0.6164, Training Accuracy: 0.7001, Validation Accuracy: 0.6174\n",
      "Epoch 290/300, Training Loss: 0.6519, Validation Loss: 0.6206, Training Accuracy: 0.6872, Validation Accuracy: 0.7047\n",
      "Epoch 300/300, Training Loss: 0.6580, Validation Loss: 0.6751, Training Accuracy: 0.6542, Validation Accuracy: 0.6040\n",
      "Test Loss: 0.6673, Test Accuracy: 0.5733\n",
      "Batch size: 32\n",
      "Test Accuracy: 0.5733\n",
      "Test Precision: 0.8182\n",
      "Test Recall: 0.1268\n",
      "Test F1 Score: 0.2195\n",
      "Epoch 10/300, Training Loss: 0.5844, Validation Loss: 0.5423, Training Accuracy: 0.7073, Validation Accuracy: 0.7181\n",
      "Epoch 20/300, Training Loss: 0.6036, Validation Loss: 0.5509, Training Accuracy: 0.6930, Validation Accuracy: 0.7315\n",
      "Epoch 30/300, Training Loss: 0.5862, Validation Loss: 0.5483, Training Accuracy: 0.7044, Validation Accuracy: 0.7047\n",
      "Epoch 40/300, Training Loss: 0.5836, Validation Loss: 0.5547, Training Accuracy: 0.7059, Validation Accuracy: 0.7047\n",
      "Epoch 50/300, Training Loss: 0.5669, Validation Loss: 0.5413, Training Accuracy: 0.7174, Validation Accuracy: 0.7181\n",
      "Epoch 60/300, Training Loss: 0.6216, Validation Loss: 0.5762, Training Accuracy: 0.6887, Validation Accuracy: 0.7181\n",
      "Epoch 70/300, Training Loss: 0.6194, Validation Loss: 0.5527, Training Accuracy: 0.7044, Validation Accuracy: 0.7047\n",
      "Epoch 80/300, Training Loss: 0.5712, Validation Loss: 0.5521, Training Accuracy: 0.7245, Validation Accuracy: 0.6980\n",
      "Epoch 90/300, Training Loss: 0.5912, Validation Loss: 0.5445, Training Accuracy: 0.6915, Validation Accuracy: 0.7114\n",
      "Epoch 100/300, Training Loss: 0.5846, Validation Loss: 0.5553, Training Accuracy: 0.6987, Validation Accuracy: 0.7181\n",
      "Epoch 110/300, Training Loss: 0.5813, Validation Loss: 0.5378, Training Accuracy: 0.7044, Validation Accuracy: 0.7315\n",
      "Epoch 120/300, Training Loss: 0.5590, Validation Loss: 0.5492, Training Accuracy: 0.7145, Validation Accuracy: 0.7315\n",
      "Epoch 130/300, Training Loss: 0.5713, Validation Loss: 0.5661, Training Accuracy: 0.6987, Validation Accuracy: 0.6980\n",
      "Epoch 140/300, Training Loss: 0.6085, Validation Loss: 0.5721, Training Accuracy: 0.6887, Validation Accuracy: 0.6644\n",
      "Epoch 150/300, Training Loss: 0.5898, Validation Loss: 0.5497, Training Accuracy: 0.6973, Validation Accuracy: 0.7114\n",
      "Epoch 160/300, Training Loss: 0.5826, Validation Loss: 0.5493, Training Accuracy: 0.7131, Validation Accuracy: 0.7450\n",
      "Epoch 170/300, Training Loss: 0.5834, Validation Loss: 0.5496, Training Accuracy: 0.7116, Validation Accuracy: 0.6913\n",
      "Epoch 180/300, Training Loss: 0.5930, Validation Loss: 0.5496, Training Accuracy: 0.7059, Validation Accuracy: 0.7181\n",
      "Epoch 190/300, Training Loss: 0.5799, Validation Loss: 0.5443, Training Accuracy: 0.7116, Validation Accuracy: 0.7315\n",
      "Epoch 200/300, Training Loss: 0.5738, Validation Loss: 0.5362, Training Accuracy: 0.7044, Validation Accuracy: 0.7315\n",
      "Epoch 210/300, Training Loss: 0.5751, Validation Loss: 0.5555, Training Accuracy: 0.7145, Validation Accuracy: 0.7047\n",
      "Epoch 220/300, Training Loss: 0.5818, Validation Loss: 0.5436, Training Accuracy: 0.7073, Validation Accuracy: 0.7517\n",
      "Epoch 230/300, Training Loss: 0.6117, Validation Loss: 0.5544, Training Accuracy: 0.7102, Validation Accuracy: 0.6913\n",
      "Epoch 240/300, Training Loss: 0.5824, Validation Loss: 0.5531, Training Accuracy: 0.7131, Validation Accuracy: 0.7315\n",
      "Epoch 250/300, Training Loss: 0.6003, Validation Loss: 0.5492, Training Accuracy: 0.6987, Validation Accuracy: 0.7450\n",
      "Epoch 260/300, Training Loss: 0.5784, Validation Loss: 0.5363, Training Accuracy: 0.6958, Validation Accuracy: 0.7315\n",
      "Epoch 270/300, Training Loss: 0.5924, Validation Loss: 0.5294, Training Accuracy: 0.7088, Validation Accuracy: 0.7450\n",
      "Epoch 280/300, Training Loss: 0.5662, Validation Loss: 0.5405, Training Accuracy: 0.7073, Validation Accuracy: 0.7181\n",
      "Epoch 290/300, Training Loss: 0.6186, Validation Loss: 0.5412, Training Accuracy: 0.6901, Validation Accuracy: 0.7047\n",
      "Epoch 300/300, Training Loss: 0.6224, Validation Loss: 0.5775, Training Accuracy: 0.7202, Validation Accuracy: 0.6711\n",
      "Test Loss: 0.5726, Test Accuracy: 0.7067\n",
      "Batch size: 64\n",
      "Test Accuracy: 0.7067\n",
      "Test Precision: 0.7547\n",
      "Test Recall: 0.5634\n",
      "Test F1 Score: 0.6452\n",
      "Epoch 10/300, Training Loss: 0.5746, Validation Loss: 0.5525, Training Accuracy: 0.7073, Validation Accuracy: 0.6913\n",
      "Epoch 20/300, Training Loss: 0.5742, Validation Loss: 0.5381, Training Accuracy: 0.7044, Validation Accuracy: 0.7181\n",
      "Epoch 30/300, Training Loss: 0.5727, Validation Loss: 0.5481, Training Accuracy: 0.7202, Validation Accuracy: 0.7047\n",
      "Epoch 40/300, Training Loss: 0.5869, Validation Loss: 0.5392, Training Accuracy: 0.6915, Validation Accuracy: 0.7181\n",
      "Epoch 50/300, Training Loss: 0.5719, Validation Loss: 0.5337, Training Accuracy: 0.7102, Validation Accuracy: 0.7248\n",
      "Epoch 60/300, Training Loss: 0.5725, Validation Loss: 0.5415, Training Accuracy: 0.7088, Validation Accuracy: 0.7047\n",
      "Epoch 70/300, Training Loss: 0.5652, Validation Loss: 0.5373, Training Accuracy: 0.7088, Validation Accuracy: 0.7248\n",
      "Epoch 80/300, Training Loss: 0.5786, Validation Loss: 0.5351, Training Accuracy: 0.7145, Validation Accuracy: 0.7315\n",
      "Epoch 90/300, Training Loss: 0.5839, Validation Loss: 0.5424, Training Accuracy: 0.7102, Validation Accuracy: 0.7248\n",
      "Epoch 100/300, Training Loss: 0.5547, Validation Loss: 0.5423, Training Accuracy: 0.7418, Validation Accuracy: 0.7181\n",
      "Epoch 110/300, Training Loss: 0.5604, Validation Loss: 0.5271, Training Accuracy: 0.7188, Validation Accuracy: 0.7450\n",
      "Epoch 120/300, Training Loss: 0.5714, Validation Loss: 0.5460, Training Accuracy: 0.7116, Validation Accuracy: 0.6980\n",
      "Epoch 130/300, Training Loss: 0.5644, Validation Loss: 0.5399, Training Accuracy: 0.7231, Validation Accuracy: 0.7114\n",
      "Epoch 140/300, Training Loss: 0.5799, Validation Loss: 0.5404, Training Accuracy: 0.6930, Validation Accuracy: 0.6980\n",
      "Epoch 150/300, Training Loss: 0.6032, Validation Loss: 0.5203, Training Accuracy: 0.7145, Validation Accuracy: 0.7383\n",
      "Epoch 160/300, Training Loss: 0.5730, Validation Loss: 0.5446, Training Accuracy: 0.7288, Validation Accuracy: 0.7181\n",
      "Epoch 170/300, Training Loss: 0.5685, Validation Loss: 0.5504, Training Accuracy: 0.7102, Validation Accuracy: 0.6913\n",
      "Epoch 180/300, Training Loss: 0.5571, Validation Loss: 0.5437, Training Accuracy: 0.7317, Validation Accuracy: 0.7047\n",
      "Epoch 190/300, Training Loss: 0.5800, Validation Loss: 0.5327, Training Accuracy: 0.7188, Validation Accuracy: 0.7383\n",
      "Epoch 200/300, Training Loss: 0.5649, Validation Loss: 0.5397, Training Accuracy: 0.7159, Validation Accuracy: 0.7315\n",
      "Epoch 210/300, Training Loss: 0.5625, Validation Loss: 0.5447, Training Accuracy: 0.6973, Validation Accuracy: 0.7181\n",
      "Epoch 220/300, Training Loss: 0.6094, Validation Loss: 0.5298, Training Accuracy: 0.6958, Validation Accuracy: 0.7383\n",
      "Epoch 230/300, Training Loss: 0.6382, Validation Loss: 0.5279, Training Accuracy: 0.6829, Validation Accuracy: 0.7248\n",
      "Epoch 240/300, Training Loss: 0.5799, Validation Loss: 0.5360, Training Accuracy: 0.7073, Validation Accuracy: 0.7181\n",
      "Epoch 250/300, Training Loss: 0.5734, Validation Loss: 0.5344, Training Accuracy: 0.6944, Validation Accuracy: 0.7181\n",
      "Epoch 260/300, Training Loss: 0.5888, Validation Loss: 0.5370, Training Accuracy: 0.6815, Validation Accuracy: 0.7248\n",
      "Epoch 270/300, Training Loss: 0.5619, Validation Loss: 0.5255, Training Accuracy: 0.7231, Validation Accuracy: 0.7181\n",
      "Epoch 280/300, Training Loss: 0.5842, Validation Loss: 0.5271, Training Accuracy: 0.7059, Validation Accuracy: 0.7248\n",
      "Epoch 290/300, Training Loss: 0.5712, Validation Loss: 0.5363, Training Accuracy: 0.7159, Validation Accuracy: 0.7047\n",
      "Epoch 300/300, Training Loss: 0.5610, Validation Loss: 0.5228, Training Accuracy: 0.7073, Validation Accuracy: 0.7248\n",
      "Test Loss: 0.5313, Test Accuracy: 0.7600\n",
      "Batch size: 128\n",
      "Test Accuracy: 0.7600\n",
      "Test Precision: 0.8070\n",
      "Test Recall: 0.6479\n",
      "Test F1 Score: 0.7188\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "batch_size_list = [32, 64, 128]\n",
    "for batch in batch_size_list:\n",
    "    \n",
    "    num_epochs = 300\n",
    "    batch_size = batch\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "                val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "                correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluation on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_losses = [test_loss] * num_epochs\n",
    "    test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Learning Rate:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.7635, Validation Loss: 0.6814, Training Accuracy: 0.5007, Validation Accuracy: 0.6644\n",
      "Epoch 20/300, Training Loss: 0.6999, Validation Loss: 0.6816, Training Accuracy: 0.5179, Validation Accuracy: 0.6309\n",
      "Epoch 30/300, Training Loss: 0.6742, Validation Loss: 0.6738, Training Accuracy: 0.5839, Validation Accuracy: 0.6980\n",
      "Epoch 40/300, Training Loss: 0.6854, Validation Loss: 0.6737, Training Accuracy: 0.5495, Validation Accuracy: 0.6242\n",
      "Epoch 50/300, Training Loss: 0.6611, Validation Loss: 0.6660, Training Accuracy: 0.6155, Validation Accuracy: 0.6510\n",
      "Epoch 60/300, Training Loss: 0.6522, Validation Loss: 0.6476, Training Accuracy: 0.6270, Validation Accuracy: 0.6779\n",
      "Epoch 70/300, Training Loss: 0.6548, Validation Loss: 0.6489, Training Accuracy: 0.6141, Validation Accuracy: 0.6846\n",
      "Epoch 80/300, Training Loss: 0.6515, Validation Loss: 0.6382, Training Accuracy: 0.6385, Validation Accuracy: 0.6846\n",
      "Epoch 90/300, Training Loss: 0.6386, Validation Loss: 0.6360, Training Accuracy: 0.6557, Validation Accuracy: 0.6510\n",
      "Epoch 100/300, Training Loss: 0.6398, Validation Loss: 0.6276, Training Accuracy: 0.6542, Validation Accuracy: 0.6711\n",
      "Epoch 110/300, Training Loss: 0.6361, Validation Loss: 0.6157, Training Accuracy: 0.6557, Validation Accuracy: 0.6846\n",
      "Epoch 120/300, Training Loss: 0.6242, Validation Loss: 0.6042, Training Accuracy: 0.6341, Validation Accuracy: 0.6779\n",
      "Epoch 130/300, Training Loss: 0.6339, Validation Loss: 0.6134, Training Accuracy: 0.6485, Validation Accuracy: 0.6376\n",
      "Epoch 140/300, Training Loss: 0.6375, Validation Loss: 0.6149, Training Accuracy: 0.6614, Validation Accuracy: 0.6577\n",
      "Epoch 150/300, Training Loss: 0.6268, Validation Loss: 0.6028, Training Accuracy: 0.6643, Validation Accuracy: 0.6846\n",
      "Epoch 160/300, Training Loss: 0.6310, Validation Loss: 0.6099, Training Accuracy: 0.6442, Validation Accuracy: 0.6443\n",
      "Epoch 170/300, Training Loss: 0.6098, Validation Loss: 0.5983, Training Accuracy: 0.6901, Validation Accuracy: 0.6510\n",
      "Epoch 180/300, Training Loss: 0.6064, Validation Loss: 0.5893, Training Accuracy: 0.6944, Validation Accuracy: 0.7047\n",
      "Epoch 190/300, Training Loss: 0.6167, Validation Loss: 0.5902, Training Accuracy: 0.6600, Validation Accuracy: 0.6913\n",
      "Epoch 200/300, Training Loss: 0.5980, Validation Loss: 0.5856, Training Accuracy: 0.6786, Validation Accuracy: 0.6913\n",
      "Epoch 210/300, Training Loss: 0.5967, Validation Loss: 0.5851, Training Accuracy: 0.6815, Validation Accuracy: 0.6913\n",
      "Epoch 220/300, Training Loss: 0.6024, Validation Loss: 0.5865, Training Accuracy: 0.7044, Validation Accuracy: 0.7114\n",
      "Epoch 230/300, Training Loss: 0.6256, Validation Loss: 0.5953, Training Accuracy: 0.6915, Validation Accuracy: 0.6779\n",
      "Epoch 240/300, Training Loss: 0.6053, Validation Loss: 0.5921, Training Accuracy: 0.6915, Validation Accuracy: 0.6711\n",
      "Epoch 250/300, Training Loss: 0.5876, Validation Loss: 0.5956, Training Accuracy: 0.6887, Validation Accuracy: 0.6443\n",
      "Epoch 260/300, Training Loss: 0.6023, Validation Loss: 0.5917, Training Accuracy: 0.6758, Validation Accuracy: 0.6376\n",
      "Epoch 270/300, Training Loss: 0.5993, Validation Loss: 0.5897, Training Accuracy: 0.7188, Validation Accuracy: 0.6510\n",
      "Epoch 280/300, Training Loss: 0.5977, Validation Loss: 0.5731, Training Accuracy: 0.6872, Validation Accuracy: 0.6711\n",
      "Epoch 290/300, Training Loss: 0.5827, Validation Loss: 0.5895, Training Accuracy: 0.7159, Validation Accuracy: 0.6443\n",
      "Epoch 300/300, Training Loss: 0.5821, Validation Loss: 0.5862, Training Accuracy: 0.7145, Validation Accuracy: 0.6376\n",
      "Test Loss: 0.6146, Test Accuracy: 0.6467\n",
      "Learning Rate: 0.001\n",
      "Test Accuracy: 0.6467\n",
      "Test Precision: 0.7045\n",
      "Test Recall: 0.4366\n",
      "Test F1 Score: 0.5391\n",
      "Epoch 10/300, Training Loss: 0.6163, Validation Loss: 0.6070, Training Accuracy: 0.6944, Validation Accuracy: 0.6309\n",
      "Epoch 20/300, Training Loss: 0.6417, Validation Loss: 0.6075, Training Accuracy: 0.6471, Validation Accuracy: 0.6443\n",
      "Epoch 30/300, Training Loss: 0.6120, Validation Loss: 0.5923, Training Accuracy: 0.6700, Validation Accuracy: 0.6980\n",
      "Epoch 40/300, Training Loss: 0.6427, Validation Loss: 0.5993, Training Accuracy: 0.6600, Validation Accuracy: 0.6510\n",
      "Epoch 50/300, Training Loss: 0.6573, Validation Loss: 0.6221, Training Accuracy: 0.6284, Validation Accuracy: 0.6242\n",
      "Epoch 60/300, Training Loss: 0.6439, Validation Loss: 0.5731, Training Accuracy: 0.6643, Validation Accuracy: 0.6913\n",
      "Epoch 70/300, Training Loss: 0.5756, Validation Loss: 0.5784, Training Accuracy: 0.7059, Validation Accuracy: 0.7248\n",
      "Epoch 80/300, Training Loss: 0.5983, Validation Loss: 0.5614, Training Accuracy: 0.6772, Validation Accuracy: 0.6980\n",
      "Epoch 90/300, Training Loss: 0.6150, Validation Loss: 0.5648, Training Accuracy: 0.6958, Validation Accuracy: 0.7248\n",
      "Epoch 100/300, Training Loss: 0.5989, Validation Loss: 0.5517, Training Accuracy: 0.7059, Validation Accuracy: 0.7383\n",
      "Epoch 110/300, Training Loss: 0.6024, Validation Loss: 0.5377, Training Accuracy: 0.6958, Validation Accuracy: 0.7383\n",
      "Epoch 120/300, Training Loss: 0.6009, Validation Loss: 0.5350, Training Accuracy: 0.6772, Validation Accuracy: 0.7383\n",
      "Epoch 130/300, Training Loss: 0.6149, Validation Loss: 0.5511, Training Accuracy: 0.6829, Validation Accuracy: 0.7047\n",
      "Epoch 140/300, Training Loss: 0.5933, Validation Loss: 0.5322, Training Accuracy: 0.6930, Validation Accuracy: 0.7181\n",
      "Epoch 150/300, Training Loss: 0.6289, Validation Loss: 0.5702, Training Accuracy: 0.6844, Validation Accuracy: 0.6913\n",
      "Epoch 160/300, Training Loss: 0.6049, Validation Loss: 0.5497, Training Accuracy: 0.7231, Validation Accuracy: 0.6980\n",
      "Epoch 170/300, Training Loss: 0.5812, Validation Loss: 0.5466, Training Accuracy: 0.6930, Validation Accuracy: 0.6913\n",
      "Epoch 180/300, Training Loss: 0.5770, Validation Loss: 0.5573, Training Accuracy: 0.7288, Validation Accuracy: 0.7047\n",
      "Epoch 190/300, Training Loss: 0.5996, Validation Loss: 0.5466, Training Accuracy: 0.6786, Validation Accuracy: 0.7383\n",
      "Epoch 200/300, Training Loss: 0.5903, Validation Loss: 0.5383, Training Accuracy: 0.7088, Validation Accuracy: 0.7315\n",
      "Epoch 210/300, Training Loss: 0.5883, Validation Loss: 0.5279, Training Accuracy: 0.6872, Validation Accuracy: 0.7248\n",
      "Epoch 220/300, Training Loss: 0.6095, Validation Loss: 0.5716, Training Accuracy: 0.6743, Validation Accuracy: 0.6779\n",
      "Epoch 230/300, Training Loss: 0.5839, Validation Loss: 0.5365, Training Accuracy: 0.6973, Validation Accuracy: 0.7248\n",
      "Epoch 240/300, Training Loss: 0.6003, Validation Loss: 0.5675, Training Accuracy: 0.6714, Validation Accuracy: 0.6846\n",
      "Epoch 250/300, Training Loss: 0.5996, Validation Loss: 0.5445, Training Accuracy: 0.6930, Validation Accuracy: 0.7181\n",
      "Epoch 260/300, Training Loss: 0.5741, Validation Loss: 0.5522, Training Accuracy: 0.7116, Validation Accuracy: 0.6846\n",
      "Epoch 270/300, Training Loss: 0.5856, Validation Loss: 0.5758, Training Accuracy: 0.7059, Validation Accuracy: 0.6779\n",
      "Epoch 280/300, Training Loss: 0.5837, Validation Loss: 0.5252, Training Accuracy: 0.7044, Validation Accuracy: 0.7315\n",
      "Epoch 290/300, Training Loss: 0.5783, Validation Loss: 0.5430, Training Accuracy: 0.6944, Validation Accuracy: 0.7248\n",
      "Epoch 300/300, Training Loss: 0.5764, Validation Loss: 0.5419, Training Accuracy: 0.6887, Validation Accuracy: 0.7181\n",
      "Test Loss: 0.5309, Test Accuracy: 0.7733\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.7733\n",
      "Test Precision: 0.7606\n",
      "Test Recall: 0.7606\n",
      "Test F1 Score: 0.7606\n",
      "Epoch 10/300, Training Loss: 0.5817, Validation Loss: 0.5499, Training Accuracy: 0.7001, Validation Accuracy: 0.7114\n",
      "Epoch 20/300, Training Loss: 0.5963, Validation Loss: 0.5265, Training Accuracy: 0.6772, Validation Accuracy: 0.7450\n",
      "Epoch 30/300, Training Loss: 0.5942, Validation Loss: 0.5270, Training Accuracy: 0.7044, Validation Accuracy: 0.7248\n",
      "Epoch 40/300, Training Loss: 0.5686, Validation Loss: 0.5333, Training Accuracy: 0.7116, Validation Accuracy: 0.7315\n",
      "Epoch 50/300, Training Loss: 0.6083, Validation Loss: 0.5448, Training Accuracy: 0.6815, Validation Accuracy: 0.7181\n",
      "Epoch 60/300, Training Loss: 0.5878, Validation Loss: 0.5322, Training Accuracy: 0.6930, Validation Accuracy: 0.7450\n",
      "Epoch 70/300, Training Loss: 0.5971, Validation Loss: 0.5574, Training Accuracy: 0.6829, Validation Accuracy: 0.6846\n",
      "Epoch 80/300, Training Loss: 0.6517, Validation Loss: 0.5473, Training Accuracy: 0.6370, Validation Accuracy: 0.7383\n",
      "Epoch 90/300, Training Loss: 0.6011, Validation Loss: 0.5638, Training Accuracy: 0.6628, Validation Accuracy: 0.7383\n",
      "Epoch 100/300, Training Loss: 0.6116, Validation Loss: 0.5767, Training Accuracy: 0.6743, Validation Accuracy: 0.6443\n",
      "Epoch 110/300, Training Loss: 0.6176, Validation Loss: 0.5515, Training Accuracy: 0.6858, Validation Accuracy: 0.7047\n",
      "Epoch 120/300, Training Loss: 0.6215, Validation Loss: 0.5518, Training Accuracy: 0.6872, Validation Accuracy: 0.6913\n",
      "Epoch 130/300, Training Loss: 0.6484, Validation Loss: 0.5591, Training Accuracy: 0.6514, Validation Accuracy: 0.7114\n",
      "Epoch 140/300, Training Loss: 0.5890, Validation Loss: 0.5586, Training Accuracy: 0.7044, Validation Accuracy: 0.6913\n",
      "Epoch 150/300, Training Loss: 0.5867, Validation Loss: 0.5512, Training Accuracy: 0.6944, Validation Accuracy: 0.6846\n",
      "Epoch 160/300, Training Loss: 0.5807, Validation Loss: 0.5301, Training Accuracy: 0.7131, Validation Accuracy: 0.7383\n",
      "Epoch 170/300, Training Loss: 0.5917, Validation Loss: 0.5330, Training Accuracy: 0.7030, Validation Accuracy: 0.7315\n",
      "Epoch 180/300, Training Loss: 0.5932, Validation Loss: 0.5538, Training Accuracy: 0.6987, Validation Accuracy: 0.7248\n",
      "Epoch 190/300, Training Loss: 0.5994, Validation Loss: 0.5393, Training Accuracy: 0.6657, Validation Accuracy: 0.7383\n",
      "Epoch 200/300, Training Loss: 0.5783, Validation Loss: 0.5416, Training Accuracy: 0.7073, Validation Accuracy: 0.7181\n",
      "Epoch 210/300, Training Loss: 0.6257, Validation Loss: 0.5623, Training Accuracy: 0.6844, Validation Accuracy: 0.7248\n",
      "Epoch 220/300, Training Loss: 0.6290, Validation Loss: 0.5653, Training Accuracy: 0.6714, Validation Accuracy: 0.7248\n",
      "Epoch 230/300, Training Loss: 0.6242, Validation Loss: 0.5357, Training Accuracy: 0.6858, Validation Accuracy: 0.7248\n",
      "Epoch 240/300, Training Loss: 0.5893, Validation Loss: 0.5313, Training Accuracy: 0.7131, Validation Accuracy: 0.7383\n",
      "Epoch 250/300, Training Loss: 0.5799, Validation Loss: 0.5630, Training Accuracy: 0.6872, Validation Accuracy: 0.7450\n",
      "Epoch 260/300, Training Loss: 0.6088, Validation Loss: 0.5281, Training Accuracy: 0.6887, Validation Accuracy: 0.7248\n",
      "Epoch 270/300, Training Loss: 0.6763, Validation Loss: 0.5338, Training Accuracy: 0.7044, Validation Accuracy: 0.7383\n",
      "Epoch 280/300, Training Loss: 0.5808, Validation Loss: 0.5242, Training Accuracy: 0.7059, Validation Accuracy: 0.7383\n",
      "Epoch 290/300, Training Loss: 0.6112, Validation Loss: 0.5561, Training Accuracy: 0.6801, Validation Accuracy: 0.6980\n",
      "Epoch 300/300, Training Loss: 0.6101, Validation Loss: 0.5306, Training Accuracy: 0.6958, Validation Accuracy: 0.7450\n",
      "Test Loss: 0.5354, Test Accuracy: 0.7467\n",
      "Learning Rate: 0.01\n",
      "Test Accuracy: 0.7467\n",
      "Test Precision: 0.8113\n",
      "Test Recall: 0.6056\n",
      "Test F1 Score: 0.6935\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning_rates = [0.001, 0.007, 0.01]\n",
    "for learning in learning_rates:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "    num_epochs = 300\n",
    "    batch_size = 64\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "                val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "                correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluation on the test data\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "            test_loss += loss.item() * test_inputs.size(0)\n",
    "            predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "            y_true.extend(test_labels.tolist())\n",
    "            y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_losses = [test_loss] * num_epochs\n",
    "    test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"Learning Rate: {learning}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6743, Validation Loss: 0.6789, Training Accuracy: 0.5911, Validation Accuracy: 0.5772\n",
      "Epoch 20/300, Training Loss: 0.6745, Validation Loss: 0.6569, Training Accuracy: 0.5954, Validation Accuracy: 0.6174\n",
      "Epoch 30/300, Training Loss: 0.6681, Validation Loss: 0.6442, Training Accuracy: 0.6184, Validation Accuracy: 0.6711\n",
      "Epoch 40/300, Training Loss: 0.6683, Validation Loss: 0.6526, Training Accuracy: 0.5725, Validation Accuracy: 0.6174\n",
      "Epoch 50/300, Training Loss: 0.6638, Validation Loss: 0.6356, Training Accuracy: 0.6026, Validation Accuracy: 0.6711\n",
      "Epoch 60/300, Training Loss: 0.6596, Validation Loss: 0.6227, Training Accuracy: 0.6184, Validation Accuracy: 0.7181\n",
      "Epoch 70/300, Training Loss: 0.6382, Validation Loss: 0.6247, Training Accuracy: 0.6428, Validation Accuracy: 0.6510\n",
      "Epoch 80/300, Training Loss: 0.6499, Validation Loss: 0.6231, Training Accuracy: 0.6399, Validation Accuracy: 0.6711\n",
      "Epoch 90/300, Training Loss: 0.6147, Validation Loss: 0.6241, Training Accuracy: 0.6628, Validation Accuracy: 0.6242\n",
      "Epoch 100/300, Training Loss: 0.6584, Validation Loss: 0.6407, Training Accuracy: 0.6471, Validation Accuracy: 0.6242\n",
      "Epoch 110/300, Training Loss: 0.6327, Validation Loss: 0.6248, Training Accuracy: 0.6456, Validation Accuracy: 0.6309\n",
      "Epoch 120/300, Training Loss: 0.6234, Validation Loss: 0.5925, Training Accuracy: 0.6428, Validation Accuracy: 0.6711\n",
      "Epoch 130/300, Training Loss: 0.6287, Validation Loss: 0.5873, Training Accuracy: 0.6700, Validation Accuracy: 0.7114\n",
      "Epoch 140/300, Training Loss: 0.6081, Validation Loss: 0.5998, Training Accuracy: 0.6585, Validation Accuracy: 0.6846\n",
      "Epoch 150/300, Training Loss: 0.5927, Validation Loss: 0.5708, Training Accuracy: 0.6829, Validation Accuracy: 0.7383\n",
      "Epoch 160/300, Training Loss: 0.6147, Validation Loss: 0.5940, Training Accuracy: 0.6829, Validation Accuracy: 0.6846\n",
      "Epoch 170/300, Training Loss: 0.6014, Validation Loss: 0.5616, Training Accuracy: 0.6958, Validation Accuracy: 0.7047\n",
      "Epoch 180/300, Training Loss: 0.6205, Validation Loss: 0.5748, Training Accuracy: 0.6528, Validation Accuracy: 0.6846\n",
      "Epoch 190/300, Training Loss: 0.6124, Validation Loss: 0.5778, Training Accuracy: 0.6714, Validation Accuracy: 0.6644\n",
      "Epoch 200/300, Training Loss: 0.5990, Validation Loss: 0.5733, Training Accuracy: 0.6872, Validation Accuracy: 0.6846\n",
      "Epoch 210/300, Training Loss: 0.5960, Validation Loss: 0.5606, Training Accuracy: 0.6686, Validation Accuracy: 0.6913\n",
      "Epoch 220/300, Training Loss: 0.6043, Validation Loss: 0.5627, Training Accuracy: 0.6801, Validation Accuracy: 0.7248\n",
      "Epoch 230/300, Training Loss: 0.5665, Validation Loss: 0.5553, Training Accuracy: 0.7088, Validation Accuracy: 0.6846\n",
      "Epoch 240/300, Training Loss: 0.5808, Validation Loss: 0.5555, Training Accuracy: 0.7260, Validation Accuracy: 0.7315\n",
      "Epoch 250/300, Training Loss: 0.5905, Validation Loss: 0.5626, Training Accuracy: 0.6829, Validation Accuracy: 0.7047\n",
      "Epoch 260/300, Training Loss: 0.5813, Validation Loss: 0.5579, Training Accuracy: 0.7044, Validation Accuracy: 0.6913\n",
      "Epoch 270/300, Training Loss: 0.5641, Validation Loss: 0.5666, Training Accuracy: 0.7102, Validation Accuracy: 0.6779\n",
      "Epoch 280/300, Training Loss: 0.6013, Validation Loss: 0.5717, Training Accuracy: 0.6944, Validation Accuracy: 0.6913\n",
      "Epoch 290/300, Training Loss: 0.5832, Validation Loss: 0.5521, Training Accuracy: 0.6815, Validation Accuracy: 0.7181\n",
      "Epoch 300/300, Training Loss: 0.5867, Validation Loss: 0.5557, Training Accuracy: 0.6944, Validation Accuracy: 0.7114\n",
      "Test Loss: 0.5606, Test Accuracy: 0.7200\n",
      "Batch size: 64\n",
      "Test Accuracy: 0.7200\n",
      "Test Precision: 0.8222\n",
      "Test Recall: 0.5211\n",
      "Test F1 Score: 0.6379\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "    \n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6765, Validation Loss: 0.6780, Training Accuracy: 0.5710, Validation Accuracy: 0.5839\n",
      "Epoch 20/300, Training Loss: 0.6692, Validation Loss: 0.6589, Training Accuracy: 0.6155, Validation Accuracy: 0.6107\n",
      "Epoch 30/300, Training Loss: 0.6501, Validation Loss: 0.6533, Training Accuracy: 0.6485, Validation Accuracy: 0.6443\n",
      "Epoch 40/300, Training Loss: 0.6800, Validation Loss: 0.6518, Training Accuracy: 0.6069, Validation Accuracy: 0.5906\n",
      "Early stopping triggered after 41 epochs!\n",
      "Test Loss: 0.6353, Test Accuracy: 0.6800\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.6800\n",
      "Test Precision: 0.6825\n",
      "Test Recall: 0.6056\n",
      "Test F1 Score: 0.6418\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "best_val_loss = float('inf')  # Initialize with a very large value\n",
    "early_stopping_patience = 10  # Number of epochs to wait for improvement before stopping\n",
    "no_improvement_epochs = 0  # Counter for epochs without improvement\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    \n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Early Stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0  # Reset counter if validation loss improves\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        no_improvement_epochs += 1  # Increase counter if no improvement\n",
    "    \n",
    "    if no_improvement_epochs >= early_stopping_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Load the best model saved during training\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Learning Rate: {learning}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6696, Validation Loss: 0.6708, Training Accuracy: 0.5768, Validation Accuracy: 0.6309\n",
      "Epoch 20/300, Training Loss: 0.6721, Validation Loss: 0.6420, Training Accuracy: 0.6011, Validation Accuracy: 0.6644\n",
      "Epoch 00025: reducing learning rate of group 0 to 7.0000e-04.\n",
      "Epoch 30/300, Training Loss: 0.6240, Validation Loss: 0.6393, Training Accuracy: 0.6614, Validation Accuracy: 0.6309\n",
      "Epoch 00031: reducing learning rate of group 0 to 7.0000e-05.\n",
      "Epoch 00037: reducing learning rate of group 0 to 7.0000e-06.\n",
      "Epoch 40/300, Training Loss: 0.6479, Validation Loss: 0.6398, Training Accuracy: 0.6241, Validation Accuracy: 0.6443\n",
      "Epoch 00043: reducing learning rate of group 0 to 7.0000e-07.\n",
      "Epoch 00049: reducing learning rate of group 0 to 7.0000e-08.\n",
      "Epoch 50/300, Training Loss: 0.6457, Validation Loss: 0.6398, Training Accuracy: 0.6385, Validation Accuracy: 0.6443\n",
      "Epoch 00055: reducing learning rate of group 0 to 7.0000e-09.\n",
      "Epoch 60/300, Training Loss: 0.6533, Validation Loss: 0.6398, Training Accuracy: 0.6399, Validation Accuracy: 0.6443\n",
      "Epoch 70/300, Training Loss: 0.6611, Validation Loss: 0.6398, Training Accuracy: 0.6370, Validation Accuracy: 0.6443\n",
      "Epoch 80/300, Training Loss: 0.6481, Validation Loss: 0.6398, Training Accuracy: 0.6456, Validation Accuracy: 0.6443\n",
      "Epoch 90/300, Training Loss: 0.6498, Validation Loss: 0.6398, Training Accuracy: 0.6298, Validation Accuracy: 0.6443\n",
      "Epoch 100/300, Training Loss: 0.6414, Validation Loss: 0.6398, Training Accuracy: 0.6413, Validation Accuracy: 0.6443\n",
      "Epoch 110/300, Training Loss: 0.6402, Validation Loss: 0.6398, Training Accuracy: 0.6356, Validation Accuracy: 0.6443\n",
      "Epoch 120/300, Training Loss: 0.6571, Validation Loss: 0.6398, Training Accuracy: 0.6126, Validation Accuracy: 0.6443\n",
      "Epoch 130/300, Training Loss: 0.6519, Validation Loss: 0.6398, Training Accuracy: 0.6327, Validation Accuracy: 0.6443\n",
      "Epoch 140/300, Training Loss: 0.6387, Validation Loss: 0.6398, Training Accuracy: 0.6456, Validation Accuracy: 0.6443\n",
      "Epoch 150/300, Training Loss: 0.6651, Validation Loss: 0.6398, Training Accuracy: 0.6126, Validation Accuracy: 0.6443\n",
      "Epoch 160/300, Training Loss: 0.6468, Validation Loss: 0.6398, Training Accuracy: 0.6442, Validation Accuracy: 0.6443\n",
      "Epoch 170/300, Training Loss: 0.6653, Validation Loss: 0.6398, Training Accuracy: 0.6327, Validation Accuracy: 0.6443\n",
      "Epoch 180/300, Training Loss: 0.6582, Validation Loss: 0.6398, Training Accuracy: 0.6313, Validation Accuracy: 0.6443\n",
      "Epoch 190/300, Training Loss: 0.6568, Validation Loss: 0.6398, Training Accuracy: 0.6270, Validation Accuracy: 0.6443\n",
      "Epoch 200/300, Training Loss: 0.6664, Validation Loss: 0.6398, Training Accuracy: 0.6098, Validation Accuracy: 0.6443\n",
      "Epoch 210/300, Training Loss: 0.6546, Validation Loss: 0.6398, Training Accuracy: 0.6298, Validation Accuracy: 0.6443\n",
      "Epoch 220/300, Training Loss: 0.6518, Validation Loss: 0.6398, Training Accuracy: 0.6442, Validation Accuracy: 0.6443\n",
      "Epoch 230/300, Training Loss: 0.6696, Validation Loss: 0.6398, Training Accuracy: 0.5997, Validation Accuracy: 0.6443\n",
      "Epoch 240/300, Training Loss: 0.6743, Validation Loss: 0.6398, Training Accuracy: 0.6098, Validation Accuracy: 0.6443\n",
      "Epoch 250/300, Training Loss: 0.6545, Validation Loss: 0.6398, Training Accuracy: 0.6370, Validation Accuracy: 0.6443\n",
      "Epoch 260/300, Training Loss: 0.6438, Validation Loss: 0.6398, Training Accuracy: 0.6571, Validation Accuracy: 0.6443\n",
      "Epoch 270/300, Training Loss: 0.6532, Validation Loss: 0.6398, Training Accuracy: 0.6385, Validation Accuracy: 0.6443\n",
      "Epoch 280/300, Training Loss: 0.6538, Validation Loss: 0.6398, Training Accuracy: 0.6227, Validation Accuracy: 0.6443\n",
      "Epoch 290/300, Training Loss: 0.6437, Validation Loss: 0.6398, Training Accuracy: 0.6413, Validation Accuracy: 0.6443\n",
      "Epoch 300/300, Training Loss: 0.6294, Validation Loss: 0.6398, Training Accuracy: 0.6514, Validation Accuracy: 0.6443\n",
      "Test Loss: 0.6487, Test Accuracy: 0.6467\n",
      "Test Accuracy: 0.6467\n",
      "Test Precision: 0.7045\n",
      "Test Recall: 0.4366\n",
      "Test F1 Score: 0.5391\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_batchnorm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN_batchnorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)  # Add BatchNorm\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(self.bn1(x))  # Apply BatchNorm\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(self.bn2(x))  # Apply BatchNorm\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(self.bn3(x))  # Apply BatchNorm\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Replace the model with the one using batch normalization\n",
    "model = NN_batchnorm(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.5495, Validation Loss: 0.5683, Training Accuracy: 0.7274, Validation Accuracy: 0.7248\n",
      "Epoch 20/300, Training Loss: 0.5479, Validation Loss: 0.5617, Training Accuracy: 0.7188, Validation Accuracy: 0.6980\n",
      "Epoch 30/300, Training Loss: 0.5358, Validation Loss: 0.5651, Training Accuracy: 0.7518, Validation Accuracy: 0.6846\n",
      "Epoch 40/300, Training Loss: 0.5059, Validation Loss: 0.5610, Training Accuracy: 0.7647, Validation Accuracy: 0.7248\n",
      "Epoch 50/300, Training Loss: 0.5022, Validation Loss: 0.5559, Training Accuracy: 0.7647, Validation Accuracy: 0.6980\n",
      "Epoch 60/300, Training Loss: 0.5173, Validation Loss: 0.5486, Training Accuracy: 0.7733, Validation Accuracy: 0.7114\n",
      "Epoch 70/300, Training Loss: 0.5032, Validation Loss: 0.5427, Training Accuracy: 0.7561, Validation Accuracy: 0.7315\n",
      "Epoch 80/300, Training Loss: 0.4988, Validation Loss: 0.5575, Training Accuracy: 0.7676, Validation Accuracy: 0.6980\n",
      "Epoch 90/300, Training Loss: 0.5011, Validation Loss: 0.5596, Training Accuracy: 0.7661, Validation Accuracy: 0.6913\n",
      "Epoch 100/300, Training Loss: 0.4790, Validation Loss: 0.5445, Training Accuracy: 0.7791, Validation Accuracy: 0.7248\n",
      "Epoch 110/300, Training Loss: 0.4771, Validation Loss: 0.5478, Training Accuracy: 0.7891, Validation Accuracy: 0.7047\n",
      "Epoch 120/300, Training Loss: 0.4766, Validation Loss: 0.5703, Training Accuracy: 0.7719, Validation Accuracy: 0.6980\n",
      "Epoch 130/300, Training Loss: 0.4754, Validation Loss: 0.5562, Training Accuracy: 0.7877, Validation Accuracy: 0.7181\n",
      "Epoch 140/300, Training Loss: 0.4604, Validation Loss: 0.5505, Training Accuracy: 0.7948, Validation Accuracy: 0.7181\n",
      "Epoch 150/300, Training Loss: 0.4639, Validation Loss: 0.5793, Training Accuracy: 0.7905, Validation Accuracy: 0.6913\n",
      "Epoch 160/300, Training Loss: 0.4721, Validation Loss: 0.5450, Training Accuracy: 0.7848, Validation Accuracy: 0.7248\n",
      "Epoch 170/300, Training Loss: 0.4658, Validation Loss: 0.5486, Training Accuracy: 0.7834, Validation Accuracy: 0.7114\n",
      "Epoch 180/300, Training Loss: 0.4615, Validation Loss: 0.5761, Training Accuracy: 0.7834, Validation Accuracy: 0.6980\n",
      "Epoch 190/300, Training Loss: 0.4370, Validation Loss: 0.5511, Training Accuracy: 0.8077, Validation Accuracy: 0.7114\n",
      "Epoch 200/300, Training Loss: 0.4845, Validation Loss: 0.5625, Training Accuracy: 0.7676, Validation Accuracy: 0.7114\n",
      "Epoch 210/300, Training Loss: 0.4464, Validation Loss: 0.5460, Training Accuracy: 0.8092, Validation Accuracy: 0.7248\n",
      "Epoch 220/300, Training Loss: 0.4461, Validation Loss: 0.5444, Training Accuracy: 0.8006, Validation Accuracy: 0.7181\n",
      "Epoch 230/300, Training Loss: 0.4537, Validation Loss: 0.5425, Training Accuracy: 0.8063, Validation Accuracy: 0.7248\n",
      "Epoch 240/300, Training Loss: 0.4040, Validation Loss: 0.5638, Training Accuracy: 0.8235, Validation Accuracy: 0.6913\n",
      "Epoch 250/300, Training Loss: 0.4446, Validation Loss: 0.5794, Training Accuracy: 0.8106, Validation Accuracy: 0.6980\n",
      "Epoch 260/300, Training Loss: 0.4366, Validation Loss: 0.5915, Training Accuracy: 0.7905, Validation Accuracy: 0.7047\n",
      "Epoch 270/300, Training Loss: 0.4421, Validation Loss: 0.5616, Training Accuracy: 0.8063, Validation Accuracy: 0.6913\n",
      "Epoch 280/300, Training Loss: 0.4328, Validation Loss: 0.5862, Training Accuracy: 0.7977, Validation Accuracy: 0.6779\n",
      "Epoch 290/300, Training Loss: 0.4488, Validation Loss: 0.5702, Training Accuracy: 0.7963, Validation Accuracy: 0.6846\n",
      "Epoch 300/300, Training Loss: 0.4576, Validation Loss: 0.5589, Training Accuracy: 0.8034, Validation Accuracy: 0.6913\n",
      "Test Loss: 0.4568, Test Accuracy: 0.8133\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.8133\n",
      "Test Precision: 0.7722\n",
      "Test Recall: 0.8592\n",
      "Test F1 Score: 0.8133\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchnorm(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "num_epochs = 300\n",
    "batch_size = 128\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Learning Rate: {learning}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Training Loss: 0.6935, Validation Loss: 0.6822, Training Accuracy: 0.5280, Validation Accuracy: 0.6242\n",
      "Epoch 20/300, Training Loss: 0.6812, Validation Loss: 0.6746, Training Accuracy: 0.5409, Validation Accuracy: 0.6711\n",
      "Epoch 30/300, Training Loss: 0.6682, Validation Loss: 0.6724, Training Accuracy: 0.6184, Validation Accuracy: 0.5772\n",
      "Epoch 40/300, Training Loss: 0.6719, Validation Loss: 0.6553, Training Accuracy: 0.5868, Validation Accuracy: 0.6779\n",
      "Epoch 50/300, Training Loss: 0.6604, Validation Loss: 0.6596, Training Accuracy: 0.6298, Validation Accuracy: 0.6309\n",
      "Epoch 60/300, Training Loss: 0.6464, Validation Loss: 0.6472, Training Accuracy: 0.6255, Validation Accuracy: 0.6577\n",
      "Epoch 70/300, Training Loss: 0.6434, Validation Loss: 0.6222, Training Accuracy: 0.6499, Validation Accuracy: 0.6779\n",
      "Epoch 80/300, Training Loss: 0.6501, Validation Loss: 0.6324, Training Accuracy: 0.6313, Validation Accuracy: 0.6577\n",
      "Epoch 90/300, Training Loss: 0.6439, Validation Loss: 0.6278, Training Accuracy: 0.6528, Validation Accuracy: 0.6711\n",
      "Epoch 100/300, Training Loss: 0.6466, Validation Loss: 0.6254, Training Accuracy: 0.6643, Validation Accuracy: 0.6711\n",
      "Epoch 110/300, Training Loss: 0.6403, Validation Loss: 0.6240, Training Accuracy: 0.6758, Validation Accuracy: 0.6376\n",
      "Epoch 120/300, Training Loss: 0.6354, Validation Loss: 0.6160, Training Accuracy: 0.6212, Validation Accuracy: 0.6779\n",
      "Epoch 130/300, Training Loss: 0.6339, Validation Loss: 0.6050, Training Accuracy: 0.6542, Validation Accuracy: 0.7114\n",
      "Epoch 140/300, Training Loss: 0.6235, Validation Loss: 0.6092, Training Accuracy: 0.6772, Validation Accuracy: 0.7047\n",
      "Epoch 150/300, Training Loss: 0.6301, Validation Loss: 0.6185, Training Accuracy: 0.6413, Validation Accuracy: 0.6376\n",
      "Epoch 160/300, Training Loss: 0.6283, Validation Loss: 0.6198, Training Accuracy: 0.6528, Validation Accuracy: 0.6376\n",
      "Epoch 170/300, Training Loss: 0.6303, Validation Loss: 0.6094, Training Accuracy: 0.6499, Validation Accuracy: 0.6711\n",
      "Epoch 180/300, Training Loss: 0.6082, Validation Loss: 0.5985, Training Accuracy: 0.6829, Validation Accuracy: 0.7047\n",
      "Epoch 190/300, Training Loss: 0.6053, Validation Loss: 0.5965, Training Accuracy: 0.6772, Validation Accuracy: 0.7114\n",
      "Epoch 200/300, Training Loss: 0.5958, Validation Loss: 0.5954, Training Accuracy: 0.6801, Validation Accuracy: 0.6980\n",
      "Epoch 210/300, Training Loss: 0.6025, Validation Loss: 0.5912, Training Accuracy: 0.6829, Validation Accuracy: 0.7181\n",
      "Epoch 220/300, Training Loss: 0.6165, Validation Loss: 0.5933, Training Accuracy: 0.6700, Validation Accuracy: 0.6913\n",
      "Epoch 230/300, Training Loss: 0.5965, Validation Loss: 0.5844, Training Accuracy: 0.6915, Validation Accuracy: 0.7047\n",
      "Epoch 240/300, Training Loss: 0.6297, Validation Loss: 0.5877, Training Accuracy: 0.6700, Validation Accuracy: 0.6846\n",
      "Epoch 250/300, Training Loss: 0.6048, Validation Loss: 0.5811, Training Accuracy: 0.6887, Validation Accuracy: 0.7248\n",
      "Epoch 260/300, Training Loss: 0.6022, Validation Loss: 0.5833, Training Accuracy: 0.6643, Validation Accuracy: 0.7383\n",
      "Epoch 270/300, Training Loss: 0.5961, Validation Loss: 0.5786, Training Accuracy: 0.6901, Validation Accuracy: 0.7315\n",
      "Epoch 280/300, Training Loss: 0.5896, Validation Loss: 0.5749, Training Accuracy: 0.6987, Validation Accuracy: 0.7315\n",
      "Epoch 290/300, Training Loss: 0.5937, Validation Loss: 0.5761, Training Accuracy: 0.6944, Validation Accuracy: 0.7248\n",
      "Epoch 300/300, Training Loss: 0.5942, Validation Loss: 0.5674, Training Accuracy: 0.6944, Validation Accuracy: 0.7248\n",
      "Test Loss: 0.5992, Test Accuracy: 0.6867\n",
      "Learning Rate: 0.007\n",
      "Test Accuracy: 0.6867\n",
      "Test Precision: 0.6463\n",
      "Test Recall: 0.7465\n",
      "Test F1 Score: 0.6928\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NN_batchsize_lr(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning = 0.007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning)\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "accumulation_steps = 4  # Number of batches over which gradients will be accumulated\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, Y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    optimizer.zero_grad()  # Reset the gradients before accumulating\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        \n",
    "        # Scale loss for accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()  # Accumulate the gradients\n",
    "        \n",
    "        # Accumulate gradients and perform optimizer step every 'accumulation_steps' batches\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()  # Update the weights\n",
    "            optimizer.zero_grad()  # Reset the gradients for the next set of accumulation steps\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0) * accumulation_steps  # Adjust the loss accumulation\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct_train += (predictions.squeeze(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item() * val_inputs.size(0)\n",
    "            val_predictions = torch.round(torch.sigmoid(val_outputs))\n",
    "            correct_val += (val_predictions.squeeze(1) == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels.unsqueeze(1).float())\n",
    "        test_loss += loss.item() * test_inputs.size(0)\n",
    "        predictions = torch.round(torch.sigmoid(test_outputs))\n",
    "        y_true.extend(test_labels.tolist())\n",
    "        y_pred.extend(predictions.squeeze(1).tolist())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "test_losses = [test_loss] * num_epochs\n",
    "test_accuracies = [test_accuracy] * num_epochs\n",
    "\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(f\"Learning Rate: {learning}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
